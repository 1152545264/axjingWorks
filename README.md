<!--
 * @Author: axjing
 * @Date: 2019-06-03 16:37:21
 * @LastEditTime: 2020-02-29 09:24:40
 * @LastEditors: Please set LastEditors
 * @Description: In User Settings Edit
 * @FilePath: /c:\Users\axjin\Desktop\ANCODE\axjingWorks\README.md
 * @WeiXin: xiangxinweilaiAXJ
 * @ZhiHu: https://www.zhihu.com/people/3c073e64578df7441664e8e3a6d54c4b
 -->
 *说明：*README文档中还没有分目录整理，网页中有公式的地方会出现乱码，拉取到本地使用MakeDown编译会得到解决。近期忙于毕业及找工作，后续会更新解决，方便共同学习。
* ```axjingWorker```

  * 文件目录

    * addons 一些插件 
    * algorithm_note 算法基础，笔试高频题
    * scripts 文件预处理脚本
    * workspace 一些总结、心得
    * DeepLearning 手撸中......
    <!-- * ComputerVision 手撸中...... -->
    * MachineLearning 手撸中......

***
# ML --->MachineLearning
**监督学习总结**
**1. 适用问题**
监督学习可以认为是学习一个模型，使它能对给定的输入预测相应的输出。监督学习包括分类、标注、回归。本篇主要考虑前两者的学习方法。
分类问题是从实例的特征向量到类标记的预测问题；标注问题是从观测序列到标记序列(或状态序列)的预测问题。可以认为分类问题是标注问题的特殊情况。
分类问题中可能的预测结果是二类或多类；而标注问题中可能的预测结果是所有的标记序列，其数目是指数级的。
感知机、$k$近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直观、方法简单、实现容易等特点；
逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，往往分类准确率更高；
隐马尔可夫模型、条件随机场是主要的标注方法。通常条件随机场的标注准确率更事高。
**2. 模型**
分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射.它们可以写成条件概率分布$P(Y|X)$或决策函数$Y=f(X)$的形式。前者表示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型。
朴素贝叶斯法、隐马尔可夫模型是概率模型；感知机、$k$近邻法、支持向量机、提升方法是非概率模型；而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型。
直接学习条件概率分布$P(Y|X)$或决策函数$Y=f(X)$的方法为判别方法，对应的模型是判别模型：感知机、$k$近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法。
首先学习联合概率分布$P(X,Y)$，从而求得条件概率分布$P(Y|X)$的方法是生成方法，对应的模型是生成模型：朴素贝叶斯法、隐马尔可夫模型是生成方法。
决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。感知机、支持向量机、k近邻法的特征空间是欧氏空间(更一般地，是希尔伯特空间)。提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。
感知机模型是线性模型；而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型；$k$近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是非线性模型。
**3. 学习策略**
在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用合页损失函数、逻辑斯谛损失函数、指数损失函数，分别写为：
$$
[1-y f(x)]_{+}
$$
$$
\log[1+\exp (-y f(x))]
$$
$$
\exp (-y f(x))
$$
这3种损失函数都是0-1损失函数的上界，具有相似的形状
可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数(surrogateloas Punotion)表示分类的损失，定义经验风险或结构风险函数，实现二类分类学习任务。学习的策略是优化以下结构风险函数，
$$
\min _{f \in H} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)
$$
第1项为经验风险(经验损失)，第2项为正则化项，$L(y,f(x))$为损失函数，$J(f)$为模型的复杂度，$\lambda \geq 0$为系数。
支持向量机用$L_2$范数表示模型的复杂度。原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上$L_2$范数正则化项。提升方法没有显式的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果。
概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计。学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。对数似然损失可以写成：
$$-logP(y|x)$$
极大后验概率估计时，正则化项是先验概率的负对数。
决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。
逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)。
朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。
**4. 学习算法**
统计学习的问题有了具体的形式以后，就变成了最优化问题。
朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值，可以由概率计算公式直接计算。
感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等一般的无约束最优化问题的解法。
支持向量机学习，可以解凸二次规划的对偶问题。有序列最小最优化算法等方法。
决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。
提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。
EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。
支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题，全局最优解保证存在。而其他学习问题则不是凸优化问题。

**无监督学习终结：**
1.机器学习或统计学习一般包括监督学习、无监督学习、强化学习。
无监督学习是指从无标注数据中学习模型的机器学习问题。无标注数据是自然得到的数据，模型表示数据的类别、转换或概率无监督学习的本质是学习数据中的统计规律或潜在结构，主要包括聚类、降维、概率估计。
2.无监督学习可以用于对已有数据的分析，也可以用于对未来数据的预测。学习得到的模型有函数$z＝g(x)$，条件概率分布$P(z|x)$，或条件概率分布$P(x|z)$。
无监督学习的基本想法是对给定数据（矩阵数据）进行某种“压缩”，从而找到数据的潜在结构，假定损失最小的压缩得到的结果就是最本质的结构。可以考虑发掘数据的纵向结构，对应聚类。也可以考虑发掘数据的横向结构，对应降维。还可以同时考虑发掘数据的纵向与横向结构，对应概率模型估计。
3.聚类是将样本集合中相似的样本（实例）分配到相同的类，不相似的样本分配到不同的类。聚类分硬聚类和软聚类。聚类方法有层次聚类和$k$均值聚类。
4.降维是将样本集合中的样本（实例）从高维空间转换到低维空间。假设样本原本存在于低维空间，或近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系。降维有线性降维和非线性降维，降维方法有主成分分析。
5.概率模型估计假设训练数据由一个概率模型生成，同时利用训练数据学习概率模型的结构和参数。概率模型包括混合模型、率图模型等。概率图模型又包括有向图模型和无向图模型。
6.话题分析是文本分析的一种技术。给定一个文本集合，话题分析旨在发现文本集合中每个文本的话题，而话题由单词的集合表示。话题分析方法有潜在语义分析、概率潜在语义分析和潜在狄利克雷分配。
7.图分析的目的是发掘隐藏在图中的统计规律或潜在结构。链接分析是图分析的一种，主要是发现有向图中的重要结点，包括 **PageRank**算法。


1．MachineLearning是关于计算机基于数据构建概率统计模型并运用模型对数据进行分析与预测的一门学科。MachineLearning包括监督学习、非监督学习、半监督学习和强化学习。
2．MachineLearning方法三要素——模型、策略、算法，对理解统计学习方法起到提纲挈领的作用。
3．监督学习可以概括如下：从给定有限的训练数据出发， 假设数据是独立同分布的，而且假设模型属于某个假设空间，应用某一评价准则，从假设空间中选取一个最优的模型，使它对已给训练数据及未知测试数据在给定评价标准意义下有最准确的预测。
4．MachineLearning中，进行模型选择或者说提高学习的泛化能力是一个重要问题。如果只考虑减少训练误差，就可能产生过拟合现象。模型选择的方法有正则化与交叉验证。学习方法泛化能力的分析是MachineLearning理论研究的重要课题。
5．分类问题、标注问题和回归问题是监督学习的重要问题。如感知机、 k 近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归
与最大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型和条件随机场。这些方法是主要的分类、标注以及回归方法。它们又可以归类为生成方法与判别方法。
## 最小二乘法
高斯于1823年在误差$e_1,…,e_n$独立同分布的假定下,证明了最小二乘方法的一个最优性质: 在所有无偏的线性估计类中,最小二乘方法是其中方差最小的！
对于数据$(x_i, y_i)   (i=1, 2, 3...,m)$

拟合出函数$h(x)$

有误差，即残差：$r_i=h(x_i)-y_i$

此时$L2$范数(残差平方和)最小时，$h(x)$ 和 $y$ 相似度最高，更拟合

一般的$H(x)$为$n$次的多项式，$H(x)=w_0+w_1x+w_2x^2+...w_nx^n$

$w(w_0,w_1,w_2,...,w_n)$为参数

最小二乘法就是要找到一组 $w(w_0,w_1,w_2,...,w_n)$ ，使得$\sum_{i=1}^n(h(x_i)-y_i)^2$ (残差平方和) 最小

即，求 $min\sum_{i=1}^n(h(x_i)-y_i)^2$

## 感知机
**原理**
* 感知机是根据输入实例的特征向量$x$对其进行二分类的线性分类器：
$$
f(x)=\operatorname{sign}(w \cdot x+b)
$$
感知机模型对应于输入空间（特征空间）中的分离超平面$w \cdot x+b=0$
* 感知机的学习策略是极小化损失函数：
$$
\min_{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i} + b\right)
$$
损失函数对应于误分类点到分离超平面的总距离

* 感知机西也许实验法是基于梯度下降的对于损失函数的最优化算法，有原始形式和对偶形式。原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。随机选取一个误分类点使其梯度下降

* 当训练数据集线性可分时，感知机算法是收敛的。感知机在训练数据集上的误分类次数$k$满足不等式：
$$
k \leqslant\left(\frac{R}{\gamma}\right)^2
$$
在训练数据线性可分时，感知机学习存在无穷多个解，其解由于不同的初始值或不同的迭代顺序而有所不同

**二分类模型**
$f(x) = sign(w\cdot x + b)$

$\operatorname{sign}(x)=\left\{\begin{array}{ll}{+1,} & {x \geqslant 0} \\ {-1,} & {x<0}\end{array}\right.$

给定训练集：

$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$

定义感知机的损失函数 

$L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)$

---
**算法**
随机梯度下降(Stochastic Gradient Descent):
随机选取一个误分类点使其梯度下降；
$w = w + \eta y_{i}x_{i}$
$b = b + \eta y_{i}$
当实例点被误分类，即位于分类超平面的错误侧时，调整$w$，$b$的值，使超平面向此误分类点的一侧移动，直至误分类点被正确分类

## K-NN
**原理**
* k近邻算法是基本且简单的分类回归算法，基本流程为：对给定的训练实例点和输入实例点，首先确定输入实例点的k个最近邻训练实例点，然后利用这k个训练实例点的类的多数来预测输入实例点的类（投票）。
* k近邻模型对应与基于训练数据集对特征空间的一个划分。k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，其结果唯一确定。
* k近邻算法三要素：**距离度量、k值、分类决策规则**。常用的距离度量是欧式距离及更一般的$PL$距离。k值小时，k近邻模型更复杂；k值大时，模型更简单。k值得选择反应了对于近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k
* 常用的分类决策规则是多数表决，对应于经验风险最小化；
* k近邻的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对$k$维空间的一个划分，其每个节点对应于k维空间划分中的一个超矩形区域。利用kd树可以省去大部分的数据点搜索，从而减少搜索的计算量。

**距离度量**
设特征空间$x$是$n$维实数向量空间 ，$x_{i}, x_{j} \in \mathcal{X}$,$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}$,$x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}$
，则：$x_i$,$x_j$的$L_p$距离定义为:


$L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{i=1}^{n}\left|x_{i}^{(i)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}$

- $p= 1$  曼哈顿距离
- $p= 2$  欧氏距离

- $p= inf$   闵式距离minkowski_distance 

### kd树
**kd**树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。
**kd**树是二叉树，表示对$k$维空间的一个划分（partition）。构造**kd**树相当于不断地用垂直于坐标轴的超平面将$k$维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个$k$维超矩形区域。

构造**kd**树的方法如下：
构造根结点，使根结点对应于$k$维空间中包含所有实例点的超矩形区域；通过下面的递归方法，不断地对$k$维空间进行切分，生成子结点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域
（子结点）；这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。
通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数
（median）为切分点，这样得到的**kd**树是平衡的。注意，平衡的**kd**树搜索时的效率未必是最优的。

***构造平衡kd树算法***
输入：$k$维空间数据集$T＝\{x_1，x_2,…,x_N\}$，

其中$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(k)}\right)^{\mathrm{T}}$ ，$i＝1,2,…,N$；

输出：**kd**树。

（1）开始：构造根结点，根结点对应于包含$T$的$k$维空间的超矩形区域。
选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
由根结点生成深度为1的左、右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域， 右子结点对应于坐标$x^{(1)}$大于切分点的子区域。
将落在切分超平面上的实例点保存在根结点。
（2）重复：对深度为$j$的结点，选择$x^{(1)}$为切分的坐标轴，$l＝j(modk)+1$，以该结点的区域中所有实例的$x^{(1)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。
由该结点生成深度为$j+1$的左、右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应坐标$x^{(1)}$大于切分点的子区域。
将落在切分超平面上的实例点保存在该结点。
（3）直到两个子区域没有实例存在时停止。从而形成**kd**树的区域划分。

## 决策树
- 决策树模型是表示基于特征对实例进行分类的树形结构，决策树可以转换成一个**if...then...**规则的集合，也可以看做是定义在特征空间划分的类的条件概率分布；
- 决策树学习旨在构建一个与训练数据拟合很好、复杂度小的决策树。因为从可能的决策树中直接选取最优决策树是NP完全问题。现实中采取启发式方法学习次优的决策树；
决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用算法ID3、C4.5和CART;
- 特征选择的目的在于选取训练数据能够分类的特征。特征选取的关键是其准则，常用规则：
（1）样本集合$D$对特征$A$的信息增益（ID3）
$$g(D, A)=H(D)-H(D|A)$$
$$H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}$$
$$H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)$$
其中，$H(D)$是数据集$D$的熵，$H(D_i)$是数据集$D_i$的熵，$H(D|A)$是数据集$D$对特征$A$的条件熵。	$D_i$是$D$中特征$A$取第$i$个值的样本子集，$C_k$是$D$中属于第$k$类的样本子集。$n$是特征$A$取 值的个数，$K$是类的个数。

（2）样本集合$D$对特征$A$的信息增益比（C4.5）
$$g_{R}(D, A)=\frac{g(D, A)}{H(D)}$$
其中，$g(D,A)$是信息增益，$H(D)$是数据集$D$的熵。

（3）样本集合$D$的基尼指数（CART）
$$\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}$$
特征$A$条件下集合$D$的基尼指数：
 $$\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)$$

 - 决策树的生成。通常使用信息增益最大、信息增益比最大或者基尼指数最小作为特征选择准则。决策树的生成往往通过计算信息增益或其他指标，从根节点开始，递归产生决策树。这相当于用信息增益或者其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确的分类子集；

 - 决策树的剪枝：由于生产决策树存在过拟合问题，需要对它进行剪枝，用以简化学到的决策树，往往冲从已生成的树上剪掉一些叶节点或者叶节点以上的子树，并将其父节点或者根节点作为新的叶节点
## 随机深林

## 逻辑斯蒂回归（LR）
- 逻辑斯蒂回归是由以下条件概率分布表示的分类模型。逻辑斯蒂回归可以用二分类或者多分类:
$$P(Y=k | x)=\frac{\exp \left(w_{k} \cdot x\right)}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}, \quad k=1,2, \cdots, K-1$$

$$P(Y=K | x)=\frac{1}{1+\sum_{k=1}^{K-1} \exp \left(w_{k} \cdot x\right)}$$
这里，$x$为输入特征，$w$为特征的权值。
逻辑斯蒂回归模型源自逻辑斯蒂分布，其分布函数$F(x)$是$S$形函数。逻辑斯蒂回归模型是由输入的线性函数表示的输出和对数几率模型。

- 最大熵模型是由以下条件概率分布表示的分类模型。最大熵也可以用于二类或多分类：
$$P_{w}(y | x)=\frac{1}{Z_{w}(x)} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)$$
$$Z_{w}(x)=\sum_{y} \exp \left(\sum_{i=1}^{n} w_{i} f_{i}(x, y)\right)$$
其中，$Z_w(x)$是规范化因子，$f_i$为特征函数，$w_i$为特征的权值。
- 最大熵模型可以由最大熵原理推导得出。最大熵原理是概率模型学习或估计的一个准则。最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。

最大熵原理应用到分类模型的学习中，有以下约束最优化问题：
$$\min -H(P)=\sum_{x, y} \tilde{P}(x) P(y | x) \log P(y | x)$$

$$s.t.  \quad P\left(f_{i}\right)-\tilde{P}\left(f_{i}\right)=0, \quad i=1,2, \cdots, n$$
 
 $$\sum_{y} P(y | x)=1$$
求解此最优化问题的对偶问题得到最大熵模型。

- 逻辑斯谛回归模型与最大熵模型都属于对数线性模型。

- 逻辑斯谛回归模型及最大熵模型学习一般采用极大似然估计，或正则化的极大似然估计。逻辑斯谛回归模型及最大熵模型学习可以形式化为无约束最优化问题。求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法。

回归模型：$f(x) = \frac{1}{1+e^{-wx}}$
其中wx线性函数：$wx =w_0\cdot x_0 + w_1\cdot x_1 + w_2\cdot x_2 +...+w_n\cdot x_n,(x_0=1)$


## Boosting
1．提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些基本分类器线性组合，构成一个强分类器。代表性的提升方法是AdaBoost算法。
AdaBoost模型是弱分类器的线性组合：

$$f(x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)$$
2．AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值。

3．AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，这说明了它作为提升方法的有效性。

4．AdaBoost算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法。
每一步中极小化损失函数

$$\left(\beta_{m}, \gamma_{m}\right)=\arg \min _{\beta, \gamma} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta b\left(x_{i} ; \gamma\right)\right)$$

得 到 参 数$\beta_{m}, \gamma_{m}$。
5．提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中最有效的方法之一。

---

**Boost**

“装袋”（bagging）和“提升”（boost）是构建组合模型的两种最主要的方法，所谓的组合模型是由多个基本模型构成的模型，组合模型的预测效果往往比任意一个基本模型的效果都要好。

- 装袋：每个基本模型由从总体样本中随机抽样得到的不同数据集进行训练得到，通过重抽样得到不同训练数据集的过程称为装袋。

- 提升：每个基本模型训练时的数据集采用不同权重，针对上一个基本模型分类错误的样本增加权重，使得新的模型重点关注误分类样本

**AdaBoost**

AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。

算法的步骤如下：

1）给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。

2）针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。

3）计算模型$G_m$的误分率$e_m=\sum_{i=1}^Nw_iI(y_i\not= G_m(x_i))$

4）计算模型$G_m$的系数$\alpha_m=0.5\log[(1-e_m)/e_m]$

5）根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。

6）计算组合模型$f(x)=\sum_{m=1}^M\alpha_mG_m(x_i)$的误分率。

7）当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）

##  Expectation Maximization algorithm(EM算法)
1．EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法。含有隐变量的概率模型的数据表示为$\theta$ )。这里，$Y$是观测变量的数据，$Z$是隐变量的数据，$\theta$ 是模型参数。EM算法通过迭代求解观测数据的对数似然函数${L}(\theta)=\log {P}(\mathrm{Y} | \theta)$的极大化，实现极大似然估计。每次迭代包括两步：

$E$步，求期望，即求$logP\left(Z | Y, \theta\right)$ )关于$ P\left(Z | Y, \theta^{(i)}\right)$)的期望：

$$Q\left(\theta, \theta^{(i)}\right)=\sum_{Z} \log P(Y, Z | \theta) P\left(Z | Y, \theta^{(i)}\right)$$
称为$Q$函数，这里$\theta^{(i)}$是参数的现估计值；

$M$步，求极大，即极大化$Q$函数得到参数的新估计值：

$$\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)$$
 
在构建具体的EM算法时，重要的是定义$Q$函数。每次迭代中，EM算法通过极大化$Q$函数来增大对数似然函数${L}(\theta)$。

2．EM算法在每次迭代后均提高观测数据的似然函数值，即

$$P\left(Y | \theta^{(i+1)}\right) \geqslant P\left(Y | \theta^{(i)}\right)$$

在一般条件下EM算法是收敛的，但不能保证收敛到全局最优。

3．EM算法应用极其广泛，主要应用于含有隐变量的概率模型的学习。高斯混合模型的参数估计是EM算法的一个重要应用，下一章将要介绍的隐马尔可夫模型的非监督学习也是EM算法的一个重要应用。

4．EM算法还可以解释为$F$函数的极大-极大算法。EM算法有许多变形，如GEM算法。GEM算法的特点是每次迭代增加$F$函数值（并不一定是极大化$F$函数），从而增加似然函数值。
 >在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝下（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们运用出现的结果来判断这个事情本身的性质（参数），也就是似然。
 $$P(Y|\theta) = \prod[\pi p^{y_i}(1-p)^{1-y_i}+(1-\pi) q^{y_i}(1-q)^{1-y_i}]$$

**E step:**
$$\mu^{i+1}=\frac{\pi (p^i)^{y_i}(1-(p^i))^{1-y_i}}{\pi (p^i)^{y_i}(1-(p^i))^{1-y_i}+(1-\pi) (q^i)^{y_i}(1-(q^i))^{1-y_i}}$$

**M step:**
$$\pi^{i+1}=\frac{1}{n}\sum_{j=1}^n\mu^{i+1}_j$$

$$p^{i+1}=\frac{\sum_{j=1}^n\mu^{i+1}_jy_i}{\sum_{j=1}^n\mu^{i+1}_j}$$

$$q^{i+1}=\frac{\sum_{j=1}^n(1-\mu^{i+1}_jy_i)}{\sum_{j=1}^n(1-\mu^{i+1}_j)}$$

## SVM
1．支持向量机最简单的情况是线性可分支持向量机，或硬间隔支持向量机。构建它的条件是训练数据线性可分。其学习策略是最大间隔法。可以表示为凸二次规划问题，其原始最优化问题为

$$\min _{w, b} \frac{1}{2}\|w\|^{2}$$

$$s.t. \quad y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N$$

求得最优化问题的解为$w^*$，$b^*$，得到线性可分支持向量机，分离超平面是

$$w^{*} \cdot x+b^{*}=0$$

分类决策函数是

$$f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)$$

最大间隔法中，函数间隔与几何间隔是重要的概念。

线性可分支持向量机的最优解存在且唯一。位于间隔边界上的实例点为支持向量。最优分离超平面由支持向量完全决定。
二次规划问题的对偶问题是
$$\min \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}$$

$$s.t. \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0$$

$$\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N$$

通常，通过求解对偶问题学习线性可分支持向量机，即首先求解对偶问题的最优值
 
$a^*$，然后求最优值$w^*$和$b^*$，得出分离超平面和分类决策函数。

2．现实中训练数据是线性可分的情形较少，训练数据往往是近似线性可分的，这时使用线性支持向量机，或软间隔支持向量机。线性支持向量机是最基本的支持向量机。

对于噪声或例外，通过引入松弛变量$\xi_{\mathrm{i}}$，使其“可分”，得到线性支持向量机学习的凸二次规划问题，其原始最优化问题是

$$\min _{w, b, \xi} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}$$

$$s.t. \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N$$

$$\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N$$

求解原始最优化问题的解$w^*$和$b^*$，得到线性支持向量机，其分离超平面为

$$w^{*} \cdot x+b^{*}=0$$

分类决策函数为

$$f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)$$

线性可分支持向量机的解$w^*$唯一但$b^*$不唯一。对偶问题是

$$\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}$$

$$s.t. \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0$$

$$0 \leqslant \alpha_{i} \leqslant C, \quad i=1,2, \cdots, N$$

线性支持向量机的对偶学习算法，首先求解对偶问题得到最优解$\alpha^*$，然后求原始问题最优解$w^*$和$b^*$，得出分离超平面和分类决策函数。

对偶问题的解$\alpha^*$中满$\alpha_{i}^{*}>0$的实例点$x_i$称为支持向量。支持向量可在间隔边界上，也可在间隔边界与分离超平面之间，或者在分离超平面误分一侧。最优分离超平面由支持向量完全决定。

线性支持向量机学习等价于最小化二阶范数正则化的合页函数

$$\sum_{i=1}^{N}\left[1-y_{i}\left(w \cdot x_{i}+b\right)\right]_{+}+\lambda\|w\|^{2}$$

3．非线性支持向量机

对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个高维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。由于在线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例与实例之间的内积，所以不需要显式地指定非线性变换，而是用核函数来替换当中的内积。核函数表示，通过一个非线性转换后的两个实例间的内积。具体地，$K(x,z)$是一个核函数，或正定核，意味着存在一个从输入空间x到特征空间的映射$\mathcal{X} \rightarrow \mathcal{H}$，对任意$\mathcal{X}$，有

$$K(x, z)=\phi(x) \cdot \phi(z)$$

对称函数$K(x,z)$为正定核的充要条件如下：对任意$$\mathrm{x}_{\mathrm{i}} \in \mathcal{X}, \quad \mathrm{i}=1,2, \ldots, \mathrm{m}$$，任意正整数$m$，对称函数$K(x,z)$对应的Gram矩阵是半正定的。

所以，在线性支持向量机学习的对偶问题中，用核函数$K(x,z)$替代内积，求解得到的就是非线性支持向量机

$$f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} K\left(x, x_{i}\right)+b^{*}\right)$$

4．SMO算法

SMO算法是支持向量机学习的一种快速算法，其特点是不断地将原二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。这样通过启发式的方法得到原二次规划问题的最优解。因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是高效的。

---
分离超平面：$w^Tx+b=0$

点到直线距离：$r=\frac{|w^Tx+b|}{||w||_2}$

$||w||_2$为2-范数：$||w||_2=\sqrt[2]{\sum^m_{i=1}w_i^2}$

直线为超平面，样本可表示为：

$w^Tx+b\ \geq+1$

$w^Tx+b\ \leq+1$

---
**margin：**

**函数间隔**：$label(w^Tx+b)\ or\ y_i(w^Tx+b)$

**几何间隔**：$r=\frac{label(w^Tx+b)}{||w||_2}$，当数据被正确分类时，几何间隔就是点到超平面的距离

为了求几何间隔最大，SVM基本问题可以转化为求解:($\frac{r^*}{||w||}$为几何间隔，(${r^*}$为函数间隔)

$$\max\ \frac{r^*}{||w||}$$

$$(subject\ to)\ y_i({w^T}x_i+{b})\geq {r^*},\ i=1,2,..,m$$

分类点几何间隔最大，同时被正确分类。但这个方程并非凸函数求解，所以要先①将方程转化为凸函数，②用拉格朗日乘子法和KKT条件求解对偶问题。

①转化为凸函数：

先令${r^*}=1$，方便计算（参照衡量，不影响评价结果）

$$\max\ \frac{1}{||w||}$$

$$s.t.\ y_i({w^T}x_i+{b})\geq {1},\ i=1,2,..,m$$

再将$\max\ \frac{1}{||w||}$转化成$\min\ \frac{1}{2}||w||^2$求解凸函数，1/2是为了求导之后方便计算。

$$\min\ \frac{1}{2}||w||^2$$

$$s.t.\ y_i(w^Tx_i+b)\geq 1,\ i=1,2,..,m$$

②用拉格朗日乘子法和KKT条件求解最优值：

$$\min\ \frac{1}{2}||w||^2$$

$$s.t.\ -y_i(w^Tx_i+b)+1\leq 0,\ i=1,2,..,m$$

整合成：

$$L(w, b, \alpha) = \frac{1}{2}||w||^2+\sum^m_{i=1}\alpha_i(-y_i(w^Tx_i+b)+1)$$

推导：$\min\ f(x)=\min \max\ L(w, b, \alpha)\geq \max \min\ L(w, b, \alpha)$

根据KKT条件：

$$\frac{\partial }{\partial w}L(w, b, \alpha)=w-\sum\alpha_iy_ix_i=0,\ w=\sum\alpha_iy_ix_i$$

$$\frac{\partial }{\partial b}L(w, b, \alpha)=\sum\alpha_iy_i=0$$

代入$ L(w, b, \alpha)$

$\min\  L(w, b, \alpha)=\frac{1}{2}||w||^2+\sum^m_{i=1}\alpha_i(-y_i(w^Tx_i+b)+1)$

$\qquad\qquad\qquad=\frac{1}{2}w^Tw-\sum^m_{i=1}\alpha_iy_iw^Tx_i-b\sum^m_{i=1}\alpha_iy_i+\sum^m_{i=1}\alpha_i$

$\qquad\qquad\qquad=\frac{1}{2}w^T\sum\alpha_iy_ix_i-\sum^m_{i=1}\alpha_iy_iw^Tx_i+\sum^m_{i=1}\alpha_i$

$\qquad\qquad\qquad=\sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i=1}\alpha_iy_iw^Tx_i$

$\qquad\qquad\qquad=\sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i,j=1}\alpha_i\alpha_jy_iy_j(x_ix_j)$

再把max问题转成min问题：

$\max\ \sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i,j=1}\alpha_i\alpha_jy_iy_j(x_ix_j)=\min \frac{1}{2}\sum^m_{i,j=1}\alpha_i\alpha_jy_iy_j(x_ix_j)-\sum^m_{i=1}\alpha_i$

$s.t.\ \sum^m_{i=1}\alpha_iy_i=0,$

$ \alpha_i \geq 0,i=1,2,...,m$

以上为SVM对偶问题的对偶形式

**kernel**

在低维空间计算获得高维空间的计算结果，也就是说计算结果满足高维（满足高维，才能说明高维下线性可分）。

**soft margin & slack variable**

引入松弛变量$\xi\geq0$，对应数据点允许偏离的functional margin 的量。

目标函数：

$$\min\ \frac{1}{2}||w||^2+C\sum\xi_i\qquad s.t.\ y_i(w^Tx_i+b)\geq1-\xi_i$$ 

对偶问题：

$$\max\ \sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i,j=1}\alpha_i\alpha_jy_iy_j(x_ix_j)=\min \frac{1}{2}\sum^m_{i,j=1}\alpha_i\alpha_jy_iy_j(x_ix_j)-\sum^m_{i=1}\alpha_i$$

$$s.t.\ C\geq\alpha_i \geq 0,i=1,2,...,m\quad \sum^m_{i=1}\alpha_iy_i=0,$$


**Sequential Minimal Optimization**

首先定义特征到结果的输出函数：$u=w^Tx+b$.

因为$w=\sum\alpha_iy_ix_i$

有$u=\sum y_i\alpha_iK(x_i, x)-b$


$$\max \sum^m_{i=1}\alpha_i-\frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j<\phi(x_i)^T,\phi(x_j)>$$

$$s.t.\ \sum^m_{i=1}\alpha_iy_i=0,$$

$$ \alpha_i \geq 0,i=1,2,...,m$$

参考资料：
[1] :[Lagrange Multiplier and KKT](http://blog.csdn.net/xianlingmao/article/details/7919597)
[2] :[推导SVM](https://my.oschina.net/dfsj66011/blog/517766)
[3] :[机器学习算法实践-支持向量机(SVM)算法原理](http://pytlab.org/2017/08/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/)
[4] :[Python实现SVM](http://blog.csdn.net/wds2006sdo/article/details/53156589)

## 神经网络

## 朴素贝叶斯分类器
- 朴素贝叶斯是最典型的生成学习方法。生成方法由训练数据学习联合概率分布$P(X,Y)$，然后求得后验概率分布$P(X|Y)$。换言之，利用训练数据学习$P(X|Y)$e和$P(Y)$的估计得到联合概率分布：
$$P(X,Y)＝P(Y)P(X|Y)$$
概率估计方法可以是极大似然估计或贝叶斯估计
*极大似然估计???*

*贝叶斯估计???*

- 朴素贝叶斯法的基本假设是**条件独立**
$$\begin{aligned} P(X&=x | Y=c_{k} )=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned}$$
这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因此朴素贝叶斯法高效，且易于实现；缺点是分类性能可能较差。

- 朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测
$$P(Y | X)=\frac{P(X, Y)}{P(X)}=\frac{P(Y) P(X | Y)}{\sum_{Y} P(Y) P(X | Y)}$$
将输入$x$分到后验概率最大的类$y$。
$$y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right)$$
后验概率最大等于0-1损失函数时的期望风险最小化

**模型：** 高斯模型、多项式模型、伯努利模型
**高斯朴素贝叶斯：**
特征可能被假设为高斯：
概率密度函数：
$$P(x_i | y_k)=\frac{1}{\sqrt{2\pi\sigma^2_{yk}}}exp(-\frac{(x_i-\mu_{yk})^2}{2\sigma^2_{yk}})$$
数学期望(mean)：$\mu$
方差：$\sigma^2=\frac{\sum(X-\mu)^2}{N}$

### 贝叶斯决策论
### 极大似然估计
### 朴素贝叶斯分类器
### 贝叶斯网络



## 主成分分析

### PAC算法

## 概率图模型

### 隐马尔科夫模型
1．隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的序列的过程。

隐马尔可夫模型由初始状态概率向$\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。因此，隐马尔可夫模型可以写成$\lambda=(A, B, \pi)$。

隐马尔可夫模型是一个生成模型，表示状态序列和观测序列的联合分布，但是状态序列是隐藏的，不可观测的。

隐马尔可夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测序列预测其对应的标记序列。

2．概率计算问题。给定模型$\lambda=(A, B, \pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。前向-后向算法是通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。
 
3．学习问题。已知观测序列$O＝(o_1，o_2,…,o_T)$，估计模型$\lambda=(A, B, \pi)$参数，使得在该模型下观测序列概率$P(O|\lambda)$最大。即用极大似然估计的方法估计参数。Baum-Welch算法，也就是EM算法可以高效地对隐马尔可夫模型进行训练。它是一种非监督学习算法。

4．预测问题。已知模型$\lambda=(A, B, \pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I＝(i_1，i_2,…,i_T)$。维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列。


### 马尔科夫随机场

### 条件随机场
1．概率无向图模型是由无向图表示的联合概率分布。无向图上的结点之间的连接关系表示了联合分布的随机变量集合之间的条件独立性，即马尔可夫性。因此，概率无向图模型也称为马尔可夫随机场。

概率无向图模型或马尔可夫随机场的联合概率分布可以分解为无向图最大团上的正值函数的乘积的形式。

2．条件随机场是给定输入随机变量$X$条件下，输出随机变量$Y$的条件概率分布模型， 其形式为参数化的对数线性模型。条件随机场的最大特点是假设输出变量之间的联合概率分布构成概率无向图模型，即马尔可夫随机场。条件随机场是判别模型。

3．线性链条件随机场是定义在观测序列与标记序列上的条件随机场。线性链条件随机场一般表示为给定观测序列条件下的标记序列的条件概率分布，由参数化的对数线性模型表示。模型包含特征及相应的权值，特征是定义在线性链的边与结点上的。线性链条件随机场的数学表达式是
$$
P(y | x)=\frac{1}{Z(x)} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
$$

其中，
 $$
Z(x)=\sum_{y} \exp \left(\sum_{i, k} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{i, l} \mu_{l} s_{l}\left(y_{i}, x, i\right)\right)
$$

4．线性链条件随机场的概率计算通常利用前向-后向算法。

5．条件随机场的学习方法通常是极大似然估计方法或正则化的极大似然估计，即在给定训练数据下，通过极大化训练数据的对数似然函数以估计模型参数。具体的算法有改进的迭代尺度算法、梯度下降法、拟牛顿法等。

6．线性链条件随机场的一个重要应用是标注。维特比算法是给定观测序列求条件概率最大的标记序列的方法。



***

# DL --->DeepLearning

## 基于梯度的学习

## 代价函数

## 输出单元

## logistic sigmoid与双曲线函数

## 万能近似性质

## 反向传播

### 微积分中的链式法则

### 反向传播算法


## 深度学习中的正则化

### 参数范数惩罚

#### $L^2$参数正则化

#### $L^1$正则化

### Dropout

## 模型中的优化

### 梯度下降

### 动量

### Nesterov动量

## 自适应学习率算法

### AdaGrad

### RMSProp

### Adam

## 二阶近似方法

### 牛顿法

### 共轭梯度

### BFGS

## 卷积网络

### 卷积运算

### 动机

### 池化

### 基本卷积函数的变体

### 结构化输出

### 数据类型

### 高效的卷积算法

## 蒙特卡洛方法

## 对数似然梯度

## 随机最大似然和对比散度

## 深度生成模型--玻尔兹曼机

```广泛使用的网络模型```\
## SqueezeNet:

《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size》是希望降低网路的复杂度，即压缩模型、减少模型的大小，同时达到public网络的识别精度。但并不能提高网络的识别精度和速度。主要内容包括两点：（1）压缩层：利用1 x 1的卷积核，把N张输入特征图，压缩到M(N>M)张特征图，再输入下一层网络；（2）fire module网络单元。
二、网络设计思想
1、尽量用1x1的卷积核替代3x3的卷积核
尽可能使用1x1卷积核为主，因为1x1卷积核比3x3卷积核参数少了9倍。
2、引入Squeeze layer,尽量减少每一层的输入特征图数量
如对于3x3卷积层，参数的个数是(number of input channels) x (number of fiters) x (3x3).所以对于3x3的卷积核参数的减少策略有两种：filters个数（输出特征图个数），另一个方法是减少input channels个数。为此文献中引入了squeeze layers，利用1x1的卷积层，把本来是N张特征图的输入降M(M<N)张特征图，再做为输入。标准的一个卷积层：
filters shape = (3, 3, 64, 128)
3、延迟下采样操作
在Alexnet中，第一层卷积层stride=4，直接下采样了四倍，在一般的CNN中，一般的卷积层、池化层都会有下采样（stride>1）,甚至在前面几层网络的下采样比例会比较大，这样会导致最后几层的神经元的激活映射区域减少。为了提高精度可以设计下采样层延迟，延迟到后面几层进行下采样（这也是为什么本篇论文不能提高速度的原因，下采样越快后面的计算量就会大大减少）
三、fire module
根据前面的一些策略方案，文献设计了一个称之为fire module的单元，一个深度网络就是由n多个fire module串联在一起的网络，fire module如下图所示：

              图
结构解说：假设网络的输入时N张特征图
（1）首先通过1x1卷积层，把N张特征图FN压缩成M张特征图FM(M<N):
FM = fM(FN)
其中f表示卷积层的操作
（2）激活函数Relu层映射：FRELU = max(0, FM)
(3)以relu层的输出结果FRELU，分别构架1x1的卷积层，3x3的卷积层，然后把他们的输出特征图相连在一起：
Fout = [f1x1(FRELU), f3x3(FRELU)]


## U-Net:





## ShuffleNet:





## MobileNet:





## AlexNet:





## VGG; ResNet; RNN; LSTM; FCN; R-CNN; Fast R-CNN; Faster R-CNN;（见图像分割）





## Inception:















## SSD:








## R-FCN:



## FPN:






## SPP:








## YOLOv1:






## YOLOv2:







## YOLOv3:





## GAN:
GAN全名叫Generative Adversarial Networks，即生成对抗网络，是一种典型的无监督学习方法。在GAN出现之前，一般是用AE（AutoEncoder）的方法来做图像生成的，但是得到的图像比较模糊，效果始终都不理想。直到2014年，Goodfellow大神在NIPS2014会议上首次提出了GAN，使得GAN第一次进入了人们的眼帘并大放异彩，到目前为止GAN的变种已经超过400种，并且CVPR2018收录的论文中有三分之一的主题和GAN有关，可见GAN仍然是当今一大热门研究方向。

GAN的应用场景非常广泛，主要有以下几个方面：
1. 图像、音频生成。比如训练数据的生成。
2. 图像翻译。从真实场景的图像到漫画风格的图像、风景画与油画间的风格互换等等。
3. 图像修复。比如图像去噪、去除图像中的马赛克（嘿嘿…）。
4. 图像超分辨率重建。卫星、遥感以及医学图像中用的比较多，大大提升后续的处理精度。

<left><font face="黑体" color=green size=6>GAN原理简述</font></left>

GAN的原理表现为对抗哲学，举个例子：警察和小偷的故事，二者满足两个对抗条件：
1.小偷不停的更新偷盗技术以避免被抓。
2.警察不停的发现新的方法与工具来抓小偷。
小偷想要不被抓就要去学习国外的先进偷盗技术，而警察想要抓到小偷就要尽可能的去掌握小偷的偷盗习性。两者在博弈的过程中不断的总结经验、吸取教训，从而都得到稳步的提升，这就是对抗哲学的精髓所在。要注意这个过程一定是一个交替的过程，也就是说两者是交替提升的。想象一下，如果一开始警察就很强大，把所有小偷全部抓光了，那么在没有了小偷之后警察也不会再去学习新的知识了，侦查能力就得不到提升。反之亦然，如果小偷刚开始就很强大，警察根本抓不到小偷，那么小偷也没有动力学习新的偷盗技术了，小偷的偷盗能力也得不到提升，这就好比在训练神经网络时出现了梯度消失一样。所以一定是一个动态博弈的过程，这也是GAN最显著的特性之一。

在讲完了警察与小偷的故事之后，我们引入今天的主人公——GAN。




***
# CV基础 --->ComputerVision

## 灰度直方图

## 点运算&代数运算&几何运算

## 线性系统

### 卷积

### 多尺度表示

### 图像金字塔

## 傅里叶变换

## 滤波设计

## 采样数据处理

## 离散图像变换

## 小波变换

## 图像分割
```
1 目标分割(Target Segmentation)，任务是把目标对应的部分分割出来。
2 目标检测(Target Detection),检测到图片当中的目标的具体位置 
3 目标识别(Target Recognition),即是在所有的给定数据中，分类出哪一些sample是目标，哪一些不是。这个仅仅做一下分类任务。yes or no
4 目标追踪(Target Tracking),这个任务很重要的第一点是目标定位（Target Locating）

而且这个任务设计到的数据一般具有时间序列（Temporal Data）。常见的情况是首先Target被Identify以后，算法或者系统需要在接下来时序的数据中，快速并高效地对给定目标进行再定位。任务需要区别类似目标，需要避免不要的重复计算，充分利用好时序相关性（Temporal Correlation），并且需要对一些简单的变化Robust，必须旋转，遮盖，缩小放大，Motion Blur之类的线性或者非线性变化。
```

图像分割是计算机视觉研究中的一个经典难题，已经成为图像理解领域关注的一个热点，图像分割是图像分析的第一步，是计算机视觉的基础，是图像理解的重要组成部分，同时也是图像处理中最困难的问题之一。所谓图像分割是指根据灰度、彩色、空间纹理、几何形状等特征把图像划分成若干个互不相交的区域，使得这些特征在同一区域内表现出一致性或相似性，而在不同区域间表现出明显的不同。简单的说就是在一副图像中，把目标从背景中分离出来。对于灰度图像来说，区域内部的像素一般具有灰度相似性，而在区域的边界上一般具有灰度不连续性。关于图像分割技术，由于问题本身的重要性和困难性，从20世纪70年代起图像分割问题就吸引了很多研究人员为之付出了巨大的努力。虽然到目前为止，还不存在一个通用的完美的图像分割的方法，但是对于图像分割的一般性规律则基本上已经达成的共识，已经产生了相当多的研究成果和方法。

<center><font face="黑体" color=green size=6>传统分割方法</font></center>

利用数字图像处理、拓扑学、数学等方面的知识来进行图像分割的方法

### 阈值分割

阈值法的基本思想是基于图像的**灰度特征**来计算*一个或多个灰度阈值*，并将图像中每个像素的灰度值与阈值作比较，最后将像素根据比较结果分到合适的类别中。因此，该方法最为关键的一步就是按照某个准则函数来求解最佳灰度阈值。

使用范围：阈值法特别适用于**目标和背景占据不同灰度级范围**的图像。

图像若只有目标和背景两大类，那么只需要选取一个阈值进行分割，此方法成为单阈值分割；但是如果图像中有多个目标需要提取，单一阈值的分割就会出现作物，在这种情况下就需要选取多个阈值将每个目标分隔开，这种分割方法相应的成为多阈值分割。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144703.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图1 对数字的一种阈值分割方法。</font></center>
阀值分割方法的优缺点：
* 计算简单，效率较高；
* 只考虑像素点灰度值本身的特征，一般不考虑空间特征，因此对噪声比较敏感，鲁棒性不高。

由此可知，阈值分割方法的关键就在于阈值的选择。若将智能遗传算法应用在阀值筛选上，选取能最优分割图像的阀值，这可能是基于阀值分割的图像分割法的发展趋势。


### 基于梯度的图像分割


### 边缘检测和连接
基于边缘检测的图像分割算法试图通过检测包含不同区域的边缘来解决分割问题。它可以说是人们最先想到也是研究最多的方法之一。通常不同区域的边界上像素的灰度值变化比较剧烈，如果将图片从空间域通过傅里叶变换到频率域，边缘就对应着高频部分，这是一种非常简单的边缘检测算法。

边缘检测技术通常可以按照处理的技术分为串行边缘检测和并行边缘检测。串行边缘检测是要想确定当前像素点是否属于检测边缘上的一点，取决于先前像素的验证结果。并行边缘检测是一个像素点是否属于检测边缘高尚的一点取决于当前正在检测的像素点以及与该像素点的一些临近像素点。

最简单的边缘检测方法是并行微分算子法，它利用相邻区域的像素值不连续的性质，采用一阶或者二阶导数来检测边缘点。近年来还提出了基于曲面拟合的方法、基于边界曲线拟合的方法、基于反应-扩散方程的方法、串行边界查找、基于变形模型的方法。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144728.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

边缘检测的优缺点：

（1）边缘定位准确；
（2）速度快；
（3）不能保证边缘的连续性和封闭性；
（4）在高细节区域存在大量的碎边缘，难以形成一个大区域，但是又不宜将高细节区域分成小碎片；

由于上述的（3）（4）两个难点，边缘检测只能产生边缘点，而非完整意义上的图像分割过程。这也就是说，在边缘点信息获取到之后还需要后续的处理或者其他相关算法相结合才能完成分割任务。

在以后的研究当中，用于提取初始边缘点的自适应阈值选取、用于图像的层次分割的更大区域的选取以及如何确认重要边缘以去除假边缘将变得非常重要。


### 基于区域的图像分割算法

基于区域的分割方法是以直接寻找区域为基础的分割技术，基于区域提取方法有两种基本形式：一种是区域生长，从单个像素出发，逐步合并以形成所需要的分割区域；另一种是从全局出发，逐步切割至所需的分割区域。

#### 区域增长
区域生长是从一组代表不同生长区域的种子像素开始，接下来将种子像素邻域里符合条件的像素合并到种子像素所代表的生长区域中，并将新添加的像素作为新的种子像素继续合并过程，直到找不到符合条件的新像素为止，该方法的关键是选择合适的初始种子像素以及合理的生长准则。

区域生长算法需要解决的三个问题：
  1 选择或确定一组能正确代表所需区域的种子像素；
  2 确定在生长过程中能将相邻像素包括进来的准则；
  3 指定让生长过程停止的条件或规则。

#### 区域分裂合并

区域生长是从某个或者某些像素点出发，最终得到整个区域，进而实现目标的提取。而分裂合并可以说是区域生长的逆过程，从整幅图像出发，不断的分裂得到各个子区域，然后再把前景区域合并，得到需要分割的前景目标，进而实现目标的提取。其实如果理解了上面的区域生长算法这个区域分裂合并算法就比较好理解啦。

四叉树分解法就是一种典型的区域分裂合并法，基本算法如下：
1 对于任一区域，如果$H(R_i)=False$就将其分裂成不重叠的四等分；
2 对相邻的两个区域$R_i$和$R_j$，它们也可以大小不同（即不在同一层），如果条件$H(R_iUR_j)=True$满足，就将它们合并起来；
3 如果进一步的分裂或合并都不可能，则结束。

其中R代表整个正方形图像区域，P代表逻辑词。
区域分裂合并算法优缺点：
（1）对复杂图像分割效果好；
（2）算法复杂，计算量大；
（3）分裂有可能破怪区域的边界。

在实际应用当中通常将区域生长算法和区域分裂合并算法结合使用，该类算法对某些复杂物体定义的复杂场景的分割或者对某些自然景物的分割等类似先验知识不足的图像分割效果较为理想。

### 二值图像处理

### 分割图像的结构化

### 基于聚类的分割方法

#### 分水岭算法

分水岭算法是一个非常好理解的算法，它根据分水岭的构成来考虑图像的分割，现实中我们可以想象成有山和湖的景象，那么一定是如下图的，水绕山山围水的景象。
分水岭分割方法，是一种基于拓扑理论的数学形态学的分割方法，其基本思想是把图像看作是测地学上的拓扑地貌，图像中每一点像素的灰度值表示该点的海拔高度，每一个局部极小值及其影响区域称为集水盆，而集水盆的边界则形成分水岭。分水岭的概念和形成可以通过模拟浸入过程来说明。在每一个局部极小值表面，刺穿一个小孔，然后把整个模型慢慢浸入水中，随着浸入的加深，每一个局部极小值的影响域慢慢向外扩展，在两个集水盆汇合处构筑大坝，即形成分水岭。
分水岭对微弱边缘具有良好的响应，图像中的噪声、物体表面细微的灰度变化都有可能产生过度分割的现象，但是这也同时能够保证得到封闭连续边缘。同时，分水岭算法得到的封闭的集水盆也为分析图像的区域特征提供了可能。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144716.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图2</font></center>

#### K-means算法分割

### 基于小波分析和小波变换的图像分割算法

小波变换是近年来得到的广泛应用的数学工具，也是现在数字图像处理必学部分，它在时间域和频率域上都有量高的局部化性质，能将时域和频域统一于一体来研究信号。而且小波变换具有多尺度特性，能够在不同尺度上对信号进行分析，因此在图像分割方面的得到了应用，

二进小波变换具有检测二元函数的局部突变能力，因此可作为图像边缘检测工具。图像的边缘出现在图像局部灰度不连续处，对应于二进小波变换的模极大值点。通过检测小波变换模极大值点可以确定图像的边缘小波变换位于各个尺度上，而每个尺度上的小波变换都能提供一定的边缘信息，因此可进行多尺度边缘检测来得到比较理想的图像边缘。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144742.png" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

上图左图是传统的阈值分割方法，右边的图像就是利用小波变换的图像分割。可以看出右图分割得到的边缘更加准确和清晰

另外，将小波和其他方法结合起来处理图像分割的问题也得到了广泛研究，比如一种局部自适应阈值法就是将Hilbert图像扫描和小波相结合，从而获得了连续光滑的阈值曲线。

### 基于遗传算法的图像分割

遗传算法（Genetic Algorithms，简称GA）是1973年由美国教授Holland提出的，是一种借鉴生物界自然选择和自然遗传机制的随机化搜索算法。是仿生学在数学领域的应用。其基本思想是，模拟由一些基因串控制的生物群体的进化过程，把该过程的原理应用到搜索算法中，以提高寻优的速度和质量。此算法的搜索过程不直接作用在变量上，而是在参数集进行了编码的个体，这使得遗传算法可直接对结构对象（图像）进行操作。整个搜索过程是从一组解迭代到另一组解，采用同时处理群体中多个个体的方法，降低了陷入局部最优解的可能性，并易于并行化。搜索过程采用概率的变迁规则来指导搜索方向，而不采用确定性搜索规则，而且对搜索空间没有任何特殊要求（如连通性、凸性等），只利用适应性信息，不需要导数等其他辅助信息，适应范围广。

遗传算法擅长于全局搜索，但局部搜索能力不足，所以常把遗传算法和其他算法结合起来应用。将遗传算法运用到图像处理主要是考虑到遗传算法具有与问题领域无关且快速随机的搜索能力。其搜索从群体出发，具有潜在的并行性，可以进行多个个体的同时比较，能有效的加快图像处理的速度。但是遗传算法也有其缺点：搜索所使用的评价函数的设计、初始种群的选择有一定的依赖性等。要是能够结合一些启发算法进行改进且遗传算法的并行机制的潜力得到充分的利用，这是当前遗传算法在图像处理中的一个研究热点。

### 基于主动轮廓模型的分割算法

主动轮廓模型（active contours）是图像分割的一种重要方法，具有统一的开放式的描述形式，为图像分割技术的研究和创新提供了理想的框架。在实现主动轮廓模型时，可以灵活的选择约束力、初始轮廓和作用域等，以得到更佳的分割效果，所以主动轮廓模型方法受到越来越多的关注。

该方法是在给定图像中利用曲线演化来检测目标的一类方法，基于此可以得到精确的边缘信息。其基本思想是，先定义初始曲线C，然后根据图像数据得到能量函数，通过最小化能量函数来引发曲线变化，使其向目标边缘逐渐逼近，最终找到目标边缘。这种动态逼近方法所求得的边缘曲线具有封闭、光滑等优点。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144753.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

传统的主动轮廓模型大致分为参数主动轮廓模型和几何主动轮廓模型。参数主动轮廓模型将曲线或曲面的形变以参数化形式表达，Kass等人提出了经典的参数活动轮廓模型即“Snake”模型，其中Snake定义为能量极小化的样条曲线，它在来自曲线自身的内力和来自图像数据的外力的共同作用下移动到感兴趣的边缘，内力用于约束曲线形状，而外力则引导曲线到特征此边缘。参数主动轮廓模型的特点是将初始曲线置于目标区域附近，无需人为设定曲线的的演化是收缩或膨胀，其优点是能够与模型直接进行交互，且模型表达紧凑，实现速度快；其缺点是难以处理模型拓扑结构的变化。比如曲线的合并或分裂等。而使用水平集（level set）的几何活动轮廓方法恰好解决了这一问题。

<center><font face="黑体" color=green size=6>基于深度学习的分割算法</font></center>

### 基于特征编码（feature encoder based）

在特征提取领域中VGGnet和ResNet是两个非常有统治力的方法，接下来的一些篇幅会对这两个方法进行简短的介绍

#### VGGNet
由牛津大学计算机视觉组合和Google DeepMind公司研究员一起研发的深度卷积神经网络。它探索了卷积神经网络的深度和其性能之间的关系，通过反复的堆叠33的小型卷积核和22的最大池化层，成功的构建了16~19层深的卷积神经网络。VGGNet获得了ILSVRC 2014年比赛的亚军和定位项目的冠军，在top5上的错误率为7.5%。目前为止，VGGNet依然被用来提取图像的特征。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144850.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>
VGGNet的优缺点：
* 由于参数量主要集中在最后的三个FC当中，所以网络加深并不会带来参数爆炸的问题；
* 多个小核卷积层的感受野等同于一个大核卷积层（三个3x3等同于一个7x7）但是参数量远少于大核卷积层而且非线性操作也多于后者，使得其学习能力较强
* VGG由于层数多而且最后的三个全连接层参数众多，导致其占用了更多的内存（140M）

#### ResNet
随着深度学习的应用��各种深度学习模型随之出现，虽然在每年都会出现性能更好的新模型，但是对于前人工作的提升却不是那么明显，其中有重要问题就是深度学习网络在堆叠到一定深度的时候会出现梯度消失的现象，导致误差升高效果变差，后向传播时无法将梯度反馈到前面的网络层，使得前方的网络层的参数难以更新，训练效果变差。这个时候ResNet恰好站出来，成为深度学习发展历程中一个重要的转折点。

ResNet是由微软研究院的Kaiming He等四名华人提出，他们通过自己提出的ResNet Unit成功训练出来152层的神经网络并在ILSVRC2015比赛中斩获冠军。ResNet语义分割领域最受欢迎且最广泛运用的神经网络.ResNet的核心思想就是在网络中引入恒等映射，允许原始输入信息直接传到后面的层中，在学习过程中可以只学习上一个网络输出的残差（F(x)），因此ResNet又叫做残差网络。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705155354.png" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

使用到ResNet的分割模型：

Efficient Neural Network（ENet）：该网络类似于ResNet的bottleNeck方法；
ResNet-38：该网络在训练or测试阶段增加并移除了一些层，是一种浅层网络，它的结构是ResNet+FCN；
full-resolution residual network(FRRN)：FRRN网络具有和ResNet相同优越的训练特性，它由残差流和池化流两个处理流组成；
AdapNey：根据ResNet-50的网络进行改进，让原本的ResNet网络能够在更短的时间内学习到更多高分辨率的特征；
……

ResNet的优缺点：
1）引入了全新的网络结构（残差学习模块），形成了新的网络结构，可以使网络尽可能地加深；
2）使得前馈/反馈传播算法能够顺利进行，结构更加简单；
3）恒等映射地增加基本上不会降低网络的性能；
4）建设性地解决了网络训练的越深，误差升高，梯度消失越明显的问题；
5）由于ResNet搭建的层数众多，所以需要的训练时间也比平常网络要长。

### 基于区域选择（regional proposal based）
Regional proposal 在计算机视觉领域是一个非常常用的算法，尤其是在目标检测领域。其核心思想就是检测颜色空间和相似矩阵，根据这些来检测待检测的区域。然后根据检测结果可以进行分类预测。

在语义分割领域，基于区域选择的几个算法主要是由前人的有关于目标检测的工作渐渐延伸到语义分割的领域的，接下来小编将逐步介绍其个中关系。

#### Stage Ⅰ：R-CNN

伯克利大学的Girshick教授等人共同提出了首个在目标检测方向应用的深度学习模型：Region-based Convolutional Neural Network（R-CNN）。该网络模型如下图所示，其主要流程为：先使用selective search算法提取2000个候选框，然后通过卷积网络对候选框进行串行的特征提取，再根据提取的特征使用SVM对候选框进行分类预测，最后使用回归方法对区域框进行修正。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144921.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>
R-CNN的优缺点：

是首个开创性地将深度神经网络应用到目标检测的算法；
使用Bounding Box Regression对目标检测的框进行调整；
由于进行特征提取时是串行，处理耗时过长；
Selective search算法在提取每一个region时需要2s的时间，浪费大量时间

#### Stage Ⅱ：Fast R-CNN
由于R-CNN的效率太低，2015年由Ross等学者提出了它的改进版本：Fast R-CNN。其网络结构图如下图所示（从提取特征开始，略掉了region的选择）Fast R-CNN在传统的R-CNN模型上有所改进的地方是它是直接使用一个神经网络对整个图像进行特征提取，就省去了串行提取特征的时间；接着使用一个RoI Pooling Layer在全图的特征图上摘取每一个RoI对应的特征，再通过FC进行分类和包围框的修正。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144927.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

Fast R-CNN的优缺点

节省了串行提取特征的时间；
除了selective search以外的其它所有模块都可以合在一起训练；
最耗时间的selective search算法依然存在。

#### Stage Ⅲ：Faster R-CNN

2016年提出的Faster R-CNN可以说有了突破性的进展（虽然还是目标检测哈哈哈），因为它改变了它的前辈们最耗时最致命的部位：selective search算法。它将selective search算法替换成为RPN，使用RPN网络进行region的选取，将2s的时间降低到10ms，其网络结构如下图所示：

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144932.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

Faster R-CNN优缺点：

使用RPN替换了耗时的selective search算法，对整个网络结构有了突破性的优化；
Faster R-CNN中使用的RPN和selective search比起来虽然速度更快，但是精度和selective search相比稍有不及，如果更注重速度而不是精度的话完全可以只使用RPN；

#### Stage Ⅳ：Mask R-CNN

Mask R-CNN（终于到分割了！）是何恺明大神团队提出的一个基于Faster R-CNN模型的一种新型的分割模型，此论文斩获ICCV 2017的最佳论文，在Mask R-CNN的工作中，它主要完成了三件事情：目标检测，目标分类，像素级分割。
恺明大神是在Faster R-CNN的结构基础上加上了Mask预测分支，并且改良了ROI Pooling，提出了ROI Align。其网络结构真容就如下图所示啦：

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144938.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

Mask R-CNN的优缺点：

引入了预测用的Mask-Head，以像素到像素的方式来预测分割掩膜，并且效果很好；
用ROI Align替代了ROI Pooling，去除了RoI Pooling的粗量化，使得提取的特征与输入良好对齐；
分类框与预测掩膜共享评价函数，虽然大多数时间影响不大，但是有的时候会对分割结果有所干扰。

#### Mask Scoring R-CNN

最后要提出的是2019年CVPR的oral，来自华中科技大学的研究生黄钊金同学提出的

MS R-CNN，这篇文章的提出主要是对上文所说的Mask R-CNN的一点点缺点进行了修正。他的网络结构也是在Mask R-CNN的网络基础上做了一点小小的改进，添加了Mask-IoU。

黄同学在文章中提到：恺明大神的Mask R-CNN已经很好啦！但是有个小毛病，就是评价函数只对目标检测的候选框进行打分，而不是分割模板（就是上文提到的优缺点中最后一点），所以会出现分割模板效果很差但是打分很高的情况。所以黄同学增加了对模板进行打分的MaskIoU Head，并且最终的分割结果在COCO数据集上超越了恺明大神，下面就是MS R-CNN的网络结构啦~

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144946.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

MS R-CNN的优缺点：

优化了Mask R-CNN中的信息传播，提高了生成预测模板的质量；
未经大批量训练的情况下，就拿下了COCO 2017挑战赛实例分割任务冠军；
要说缺点的话。。应该就是整个网络有些庞大，一方面需要ResNet当作主干网络，另一方面需要其它各种Head共同承担各种任务。


### 基于RNN的图像分割算法

Recurrent neural networks（RNNs）除了在手写和语音识别上表现出色外，在解决计算机视觉的任务上也表现不俗，在本篇文章中我们就将要介绍RNN在2D图像处理上的一些应用，其中也包括介绍使用到它的结构或者思想的一些模型。

RNN是由Long-Short-Term Memory（LSTM）块组成的网络，RNN来自序列数据的长期学习的能力以及随着序列保存记忆的能力使其在许多计算机视觉的任务中游刃有余，其中也包括语义分割以及数据标注的任务。接下来的部分我们将介绍几个使用到RNN结构的用于分割的网络结构模型：

#### ReSeg模型
ReSeg可能不被许多人所熟知，在百度上搜索出的相关说明与解析也不多，但是这是一个很有效的语义分割方法。众所周知，FCN可谓是图像分割领域的开山作，而RegNet的作者则在自己的文章中大胆的提出了FCN的不足：没有考虑到局部或者全局的上下文依赖关系，而在语义分割中这种依赖关系是非常有用的。所以在ReSeg中作者使用RNN去检索上下文信息，以此作为分割的一部分依据。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705144954.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

该结构的核心就是Recurrent Layer，它由多个RNN组合在一起，捕获输入数据的局部和全局空间结构。

优缺点：
充分考虑了上下文信息关系；
使用了中值频率平衡，它通过类的中位数(在训练集上计算)和每个类的频率之间的比值来重新加权类的预测。这就增加了低频率类的分数，这是一个更有噪声的分割掩码的代价，因为被低估的类的概率被高估了，并且可能导致在输出分割掩码中错误分类的像素增加。

#### MDRNNs（Multi-Dimensional Recurrent Neural Networks）模型
传统的RNN在一维序列学习问题上有着很好的表现，比如演讲（speech）和在线手写识别。但是 在多为问题中应用却并不到位。MDRNNs在一定程度上将RNN拓展到多维空间领域，使之在图像处理、视频处理等领域上也能有所表现。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145010.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

该论文的基本思想是：将单个递归连接替换为多个递归连接，相应可以在一定程度上解决时间随数据样本的增加呈指数增长的问题。以下就是该论文提出的两个前向反馈和反向反馈的算法。

### 基于上采样、反卷积的分割方法

卷积神经网络在进行采样的时候会丢失部分细节信息，这样的目的是得到更具特征的价值。但是这个过程是不可逆的，有的时候会导致后面进行操作的时候图像的分辨率太低，出现细节丢失等问题。因此我们通过上采样在一定程度上可以不全一些丢失的信息，从而得到更加准确的分割边界。

接下来介绍几个非常著名的分割模型：
#### FCN(Fully Convolutional Network)
FCN！在图像分割领域已然成为一个业界标杆，大多数的分割方法多多少少都会利用到FCN或者其中的一部分，比如前面我们讲过的Mask R-CNN。
在FCN当中的反卷积-升采样结构中，图片会先进性上采样（扩大像素）；再进行卷积——通过学习获得权值。FCN的网络结构如下图所示：
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145020.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

优缺点：

FCN对图像进行了像素级的分类，从而解决了语义级别的图像分割问题；
FCN可以接受任意尺寸的输入图像，可以保留下原始输入图像中的空间信息；
得到的结果由于上采样的原因比较模糊和平滑，对图像中的细节不敏感；
对各个像素分别进行分类，没有充分考虑像素与像素的关系，缺乏空间一致性。

#### SetNet

SegNet是剑桥提出的旨在解决自动驾驶或者智能机器人的图像语义分割深度网络，SegNet基于FCN，与FCN的思路十分相似，只是其编码-解码器和FCN的稍有不同，其解码器中使用去池化对特征图进行上采样，并在分各种保持高频细节的完整性；而编码器不使用全连接层，因此是拥有较少参数的轻量级网络：
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145031.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

SetNet的优缺点：

保存了高频部分的完整性；
网络不笨重，参数少，较为轻便；
对于分类的边界位置置信度较低；
对于难以分辨的类别，例如人与自行车，两者如果有相互重叠，不确定性会增加。

以上两种网络结构就是基于反卷积/上采样的分割方法，当然其中最最最重要的就是FCN了，哪怕是后面大名鼎鼎的SegNet也是基于FCN架构的，而且FCN可谓是语义分割领域中开创级别的网络结构。

### 基于提高分辨率的分割方法

在这一个模块中我们主要给大家介绍一下基于提升特征分辨率的图像分割的方法。换一种说法其实可以说是恢复在深度卷积神经网络中下降的分辨率，从而获取更多的上下文信息。这一系列我将给大家介绍的是Google提出的DeepLab 。
DeepLab是结合了深度卷积神经网络和概率图模型的方法，应用在语义分割的任务上，目的是做逐像素分类，其先进性体现在DenseCRFs（概率图模型）和DCNN的结合。是将每个像素视为CRF节点，利用远程依赖关系并使用CRF推理直接优化DCNN的损失函数。
在图像分割领域，FCN的一个众所周知的操作就是平滑以后再填充，就是先进行卷积再进行pooling,这样在降低图像尺寸的同时增大感受野，但是在先减小图片尺寸（卷积）再增大尺寸（上采样）的过程中一定有一些信息损失掉了，所以这里就有可以提高的空间。
接下来我要介绍的是DeepLab网络的一大亮点：Dilated/Atrous Convolution，它使用的采样方式是带有空洞的采样。在VGG16中使用不同采样率的空洞卷积，可以明确控制网络的感受野。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145041.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图a对应3x3的1-dilated conv，它和普通的卷积操作是相同的；图b对应3x3的2-dilated conv，事迹卷积核的尺寸还是3x3（红点），但是空洞为1，其感受野能够达到7x7；图c对应3x3的4-dilated conv，其感受野已经达到了15x15.写到这里相信大家已经明白，在使用空洞卷积的情况下，加大了感受野，使每个卷积输出都包含了较大范围的信息。</font></center>

这样就解决了DCNN的几个关于分辨率的问题：
1）内部数据结构丢失；空间曾计划信息丢失；
2）小物体信息无法重建；

当然空洞卷积也存在一定的问题，它的问题主要体现在以下两方面：
1）网格效应
加入我们仅仅多次叠加dilation rate 2的 3x3 的卷积核则会出现以下问题

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145051.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

由此发现卷积核并不连续，也就是说并不是所有的像素都用来计算了，这样会丧失信息的连续性；


2）小物体信息处理不当

我们从空洞卷积的设计背景来看可以推测出它是设计来获取long-ranged information。然而空洞步频选取得大获取只有利于大物体得分割，而对于小物体的分割可能并没有好处。所以如何处理好不同大小物体之间的关系也是设计好空洞卷积网络的关键。

### 基于特征增强的分割方法

基于特征增强的分割方法包括：提取多尺度特征或者从一系列嵌套的区域中提取特征。在图像分割的深度网络中，CNN经常应用在图像的小方块上，通常称为以每个像素为中心的固定大小的卷积核，通过观察其周围的小区域来标记每个像素的分类。在图像分割领域，能够覆盖到更大部分的上下文信息的深度网络通常在分割的结果上更加出色，当然这也伴随着更高的计算代价。多尺度特征提取的方法就由此引进。
在这一模块中我先给大家介绍一个叫做SLIC，全称为simple linear iterative cluster的生成超像素的算法。
首先我们要明确一个概念：啥是超像素？其实这个比较容易理解，就像上面说的“小方块”一样，我们平常处理图像的最小单位就是像素了，这就是像素级（pixel-level）；而把像素级的图像划分成为区域级（district-level）的图像，把区域当成是最基本的处理单元，这就是超像素啦。
算法大致思想是这样的，将图像从RGB颜色空间转换到CIE-Lab颜色空间，对应每个像素的（L，a，b）颜色值和（x，y）坐标组成一个5维向量V[l, a, b, x, y],两个像素的相似性即可由它们的向量距离来度量，距离越大，相似性越小。
算法首先生成K个种子点，然后在每个种子点的周围空间里搜索距离该种子点最近的若干像素，将他们归为与该种子点一类，直到所有像素点都归类完毕。然后计算这K个超像素里所有像素点的平均向量值，重新得到K个聚类中心，然后再以这K个中心去搜索其周围与其最为相似的若干像素，所有像素都归类完后重新得到K个超像素，更新聚类中心，再次迭代，如此反复直到收敛。
有点像聚类的K-Means算法，最终会得到K个超像素。
Mostahabi等人提出的一种前向传播的分类方法叫做Zoom-Out就使用了SLIC的算法，它从多个不同的级别提取特征：局部级别：超像素本身；远距离级别：能够包好整个目标的区域；全局级别：整个场景。这样综合考虑多尺度的特征对于像素或者超像素的分类以及分割来说都是很有意义的。
接下来的部分我将给大家介绍另一种完整的分割网络：PSPNet：Pyramid Scene Parsing Network
论文提出在场景分割是，大多数的模型会使用FCN的架构，但是FCN在场景之间的关系和全局信息的处理能力存在问题，其典型问题有：1.上下文推断能力不强；2.标签之间的关系处理不好；3.模型可能会忽略小的东西。
本文提出了一个具有层次全局优先级，包含不同子区域时间的不同尺度的信息，称之为金字塔池化模块。
该模块融合了4种不同金字塔尺度的特征，第一行红色是最粗糙的特征–全局池化生成单个bin输出，后面三行是不同尺度的池化特征。为了保证全局特征的权重，如果金字塔共有N个级别，则在每个级别后使用1×1 1×11×1的卷积将对于级别通道降为原本的1/N。再通过双线性插值获得未池化前的大小，最终concat到一起。其结构如下图：

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145102.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

最终结果就是，在融合不同尺度的feature后，达到了语义和细节的融合，模型的性能表现提升很大，作者在很多数据集上都做过训练，最终结果是在MS-COCO数据集上预训练过的效果最好。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145110.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

为了捕捉多尺度特征，高层特征包含了更多的语义和更少的位置信息。结合多分辨率图像和多尺度特征描述符的优点，在不丢失分辨率的情况下提取图像中的全局和局部信息，这样就能在一定程度上提升网络的性能。

#### 使用CRF/MRF的方法

MRF全称是Marcov Random Field，马尔可夫随机场，其实说起来笔者在刚读硕士的时候有一次就有同学在汇报中提到了隐马尔可夫、马尔可夫链啥的，当时还啥都不懂，小白一枚（现在是准小白hiahia），觉得马尔可夫这个名字贼帅，后来才慢慢了解什么马尔科夫链呀，马尔可夫随机场，并且在接触到图像分割了以后就对马尔科夫随机场有了更多的了解。
MRF其实是一种基于统计的图像分割算法，马尔可夫模型是指一组事件的集合，在这个集合中，事件逐个发生，并且下一刻事件的发生只由当前发生的事件决定，而与再之前的状态没有关系。而马尔可夫随机场，就是具有马尔可夫模型特性的随机场，就是场中任何区域都只与其临近区域相关，与其他地方的区域无关，那么这些区域里元素（图像中可以是像素）的集合就是一个马尔可夫随机场。
CRF的全称是Conditional Random Field，条件随机场其实是一种特殊的马尔可夫随机场，只不过是它是一种给定了一组输入随机变量X的条件下另一组输出随机变量Y的马尔可夫随机场，它的特点是埃及设输出随机变量构成马尔可夫随机场，可以看作是最大熵马尔可夫模型在标注问题上的推广。
在图像分割领域，运用CRF比较出名的一个模型就是全连接条件随机场（DenseCRF），接下来我们将花费一些篇幅来简单介绍一下。
CRF在运行中会有一个问题就是它只对相邻节点进行操作，这样会损失一些上下文信息，而全连接条件随机场是对所有节点进行操作，这样就能获取尽可能多的临近点信息，从而获得更加精准的分割结果。
在Fully connected CRF中，吉布斯能量可以写作：
\[{\rm{E}}(x) = \sum\limits_i {{\theta _i}({x_i}) + \sum\limits_{ij} {{\theta _{ij}}({x_i},{y_j})} } \]
其中k(m)为高斯核，写作：
\[{{\rm{k}}^m}({f_i},{f_j}) = {\omega _1}\exp ( - \frac{{{{\left\| {{p_i} + {p_j}} \right\|}^2}}}{{2\sigma _\alpha ^2}} - \frac{{{{\left\| {{I_i} + {I_j}} \right\|}^2}}}{{2\sigma _\beta ^2}}) + {\omega _2}\exp ( - \frac{{{{\left\| {{p_i} + {p_j}} \right\|}^2}}}{{2\sigma _\gamma ^2}})\]
该模型的一元势能包含了图像的形状，纹理，颜色和位置，二元势能使用了对比度敏感的的双核势能，CRF的二元势函数一般是描述像素点与像素点之间的关系，鼓励相似像素分配相同的标签，而相差较大的像素分配不同标签，而这个“距离”的定义与颜色值和实际相对距离有关，这样CRF能够使图像尽量在边界处分割。全连接CRF模型的不同就在于其二元势函数描述的是每一个像素与其他所有像素的关系，使用该模型在图像中的所有像素对上建立点对势能从而实现极大地细化和分割。
在分割结果上我们可以看看如下的结果图：
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190705145133.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图</font></center>

可以看到它在精细边缘的分割比平常的分割方法要出色得多，而且文章中使用了另一种优化算法，使得本来需要及其大量运算的全连接条件随机场也能在很短的时间里给出不错的分割结果。

至于其优缺点，我觉得可以总结为以下几方面：
在精细部位的分割非常优秀；
充分考虑了像素点或者图片区域之间的上下文关系；
在粗略的分割中可能会消耗不必要的算力；
可以用来恢复细致的局部结构，但是相应的需要较高的代价。

## 图像跟踪

### 基于卡尔曼滤波器的线性动态模型跟踪

## 物体测量

### 尺寸测量

### 形状分析

### 纹理分析

### 曲线和曲面拟合

## 局部图像特征
### HOG:



### SIFT:


## GIST:






## LBP:






## Harr:

## 霍夫变换

## 动态规划

## 隐马尔科夫模型

***

# 线性代数

## 线性相关

## 生成子空间

## 范数

## 特征分解

## 奇异值分解

***

# 概率与信息论

## 随机变量

## 概率分布

### 离散型变量和概率质量函数

### 连续型变量和概率密度函数

## 边缘概率

## 条件概率

## 条件概率的链式法则

## 独立性和条件独立性

## 期望、方差和协方差

## 常用概率分布
### Bernoulli分布

### Multinoulli分布

### 高斯分布

### 指数分布

### Laplace分布

### Dirac分布和经验分布

## 常用函数的性质

## 贝叶斯规则

## 连续变量的技术细节

## 信息论

## 结构化概率模型

## 梯度优化方法
## Jacobian矩阵

## Hessian矩阵

## 最小二乘法

## 最大似然估计


***
# DL在医学图像处理中的应用
0. 引言

医学图像处理的对象是各种不同成像机理的医学影像，临床广泛使用的医学成像种类主要有X-射线成像 （X-CT）、核磁共振成像（MRI）、核医学成像（NMI）和超声波成像（UI）四类。在目前的影像医疗诊断中，主要是通过观察一组二维切片图象去发现病变体，这往往需要借助医生的经验来判定。利用计算机图象处理技术对二维切片图象进行分析和处理，实现对人体器官、软组织和病变体的分割提取、三维重建和三维显示，可以辅助医生对病变体及其它感兴趣的区域进行定性甚至 定量的分析，从而大大提高医疗诊断的准确性和可靠性；在医疗教学、手术规划、手术仿真及各种医学研究中也能起重要的辅助作用[1,2]。目前，医学图像处理主要集中表现在病变检测、图像分割、图像配准及图像融合四个方面。

用深度学习方法进行数据分析呈现快速增长趋势，称为2013年的10项突破性技术之一。深度学习是人工神经网络的改进，由更多层组成，允许更高层次包含更多抽象信息来进行数据预测。迄今为止，它已成为计算机视觉领域中领先的机器学习工具，深度神经网络学习自动从原始数据（图像）获得的中级和高级抽象特征。最近的结果表明，从CNN中提取的信息在自然图像中的对目标识别和定位方面非常有效。世界各地的医学图像处理机构已经迅速进入该领域，并将CNN和其它深度学习方法应用于各种医学图像分析。

在医学成像中，疾病的准确诊断和评估取决于医学图像的采集和图像解释。近年来，图像采集已经得到了显着改善，设备以更快的速率和更高的分辨率采集数据。然而，图像解释过程，最近才开始受益于计算机技术。对医学图像的解释大多数都是由医生进行的，然而医学图像解释受到医生主观性、医生巨大差异认知和疲劳的限制。

用于图像处理的典型CNN架构由一系列卷积网络组成，其中包含有一系列数据缩减即池化层。与人脑中的低级视觉处理一样，卷积网络检测提取图像特征，例如可能表示直边的线或圆（例如器官检测）或圆圈（结肠息肉检测），然后是更高阶的特征，例如局部和全局形状和纹理特征提取[3]。CNN的输出通常是一个或多个概率或种类标签。

CNN是高度可并行化的算法。与单核的CPU处理相比，今天使用的图形处理单元（GPU）计算机芯片实现了大幅加速（大约40倍）。在医学图像处理中，GPU首先被引入用于分割和重建，然后用于机器学习。由于CNN的新变种的发展以及针对现代GPU优化的高效并行网络框架的出现，深度神经网络吸引了商业兴趣。从头开始训练深度CNN是一项挑战[4]。首先，CNN需要大量标记的训练数据，这一要求在专家注释昂贵且疾病稀缺的医学领域中可能难以满足。其次，训练深度CNN需要大量的计算和内存资源，否则训练过程将是非常耗时。第三，深度CNN训练过程中由于过度拟合和收敛问题而复杂化，这通常需要对网络的框架结构或学习参数进行重复调整，以确保所有层都以相当的速度学习[5]。鉴于这些困难，一些新的学习方案，称为“迁移学习”和“微调”，被证明可以解决上述问题从而越来越受欢迎。

1. 病变检测

计算机辅助检测（CAD）是医学图像分析的有待完善的领域，并且非常适合引入深度学习。在CAD 的标准方法中，一般通过监督方法或经典图像处理技术（如过滤和数学形态学）检测候选病变位置。**病变位置检测是分阶段的**，并且通常由大量手工制作的特征描述。将分类器用于特征向量映射到候选区来检测实际病变的概率。采用深度学习的直接方式是训练CNN操作一组以图像为中心的图像数据候选病变。Setio等在3D胸部CT扫描中检测肺结节，并在九个不同方向上提取以这些候选者为中心的2D贴片[6]，使用不同CNN的组合来对每个候选者进行分类，CAD系统结构如图1所示。根据检测结果显示，与先前公布的用于相同任务的经典CAD系统相比略有改进。罗斯等人应用CNN改进三种现有的CAD系统，用于检测CT成像中的结肠息肉，硬化性脊柱变形和淋巴结肿大[7]。他们还在三个正交方向上使用先前开发的候选检测器和2D贴片，以及多达100个随机旋转的视图。随机旋转的“2.5D”视图是从原始3D数据分解图像的方法。采用CNN对这些2.5D视图图像检测然后汇总，来提高检测的准确率。对于使用CNN的三个CAD系统，病变检测的准确率度提高了13-34％，而使用非深度学习分类器（例如支持向量机）几乎不可能实现这种程度的提升。早在1996年，Sahiner等人就已将CNN应用于医学图像处理。从乳房X线照片中提取肿块或正常组织的ROI。 CNN由输入层，两个隐藏层和输出层组成，并用于反向传播。在“GPU时代”以前，训练时间被描述为“计算密集型”，但没有给出任何时间。1993年，CNN应用于肺结节检测；1995年CNN用于检测乳腺摄影中的微钙化。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211707.jpg" width = "480" height = "300" /></div>
<font face="黑体" color=gray size=1>图1. CAD系统概述。（a）从立方体的九个对称平面中提取的二维斑块的示例。候选者位于贴片的中心，边界框为50 50 mm和64 64 px。（b）通过合并专门为固体，亚固体和大结节设计的探测器的输出来检测候选人。误报减少阶段是作为多个ConvNets的组合实现的。每个ConvNets流处理从特定视图中提取的2-D补丁。（c）融合每个ConvNet流输出的不同方法。 灰色和橙色框表示来自第一个完全连接的层和结节分类输出的连接神经元。 使用完全连接的层与softmax或固定组合器（产品规则）组合神经元。（a）使用体积对象的九个视图提取二维补丁。（b）拟议系统的示意图。（c）融合方法 </font>

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211731.jpg" width = "480" height = "300" /></div></center>
<center><font face="黑体" color=gray size=1>图2.结肠息肉的检测：不同息肉大小的FROC曲线，使用792测试CT结肠成像患者的随机视图ConvNet观察。 </font></center>

2. 图像分割

医学图像分割就是一个根据区域间的相似或不同把图像分割成若干区域的过程。目前，主要以各种细胞、组织与器官的图像作为处理的对象。传统的图像分割技术有基于区域的分割方法和基于边界的分割方法，前者依赖于图像的空间局部特征，如灰度、纹理及其它象素统计特性的均匀性等，后者主要是利用梯度信息确定目标的边界。结合特定的理论工具，图象分割技术有了更进一步的发展。比如基于三维可视化系统结合FastMarching算法和Watershed 变换的医学图象分割方法，能得到快速、准确的分割结果[8]。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211739.jpg" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图3.Watershed 变换的医学图象分割方法 </font></center>

近年来，随着其它新兴学科的发展，产生了一些全新的图像分割技术。如**基于统计学的方法、基于模糊理论的方法、基于神经网络的方法、基于小波分析的方法、基于模型的snake 模型(动态轮廓模型)、组合优化模型**等方法。虽然不断有新的分割方法被提出，但结果都不是很理想。目前研究的热点是一种基于知识的分割方法，即通过某种手段将一些先验的知识导入分割过程中，从而约束计算机的分割过程，使得分割结果控制在我们所能认识的范围内而不至于太离谱。比如在肝内部肿块与正常肝灰度值差别很大时，不至于将肿块与正常肝看成 2 个独立的组织。

医学图像分割方法的研究具有如下显著特点：现有任何一种单独的图像分割算法都难以对一般图像取得比较满意的结果，要更加注重多种分割算法的有效结合；由于人体解剖结构的复杂性和功能的系统性，虽然已有研究通过医学图像的自动分割区分出所需的器官、组织或找到病变区的方法，但目前现成的软件包一般无法完成全自动的分割，尚需要解剖学方面的人工干预[9]。在目前无法完全由计算机来完成图像分割任务的情况下，人机交互式分割方法逐渐成为研究重点；新的分割方法的研究主要以自动、精确、快速、自适应和鲁棒性等几个方向作为研究目标，经典分割技术与现代分割技术的综合利用(集成技术)是今后医学图像分割技术的发展方向[10,11]。

利用2891次心脏超声检查的数据集，Ghesu等结合深度学习和边缘空间学习进行医学图像检测和分割[12]。“大参数空间的有效探索”和在深度网络中实施稀疏性的方法相结合，提高了计算效率，并且与同一组发布的参考方法相比，平均分割误差减少了13.5％，八位患者的检测结果如图4所示。Brosch等人利用MRI图像上研究多发性硬化脑病变分割的问题。开发了一种3D深度卷积编码器网络，它结合了卷积和反卷积[13]，图5.增加网络深度对病变的分割性能的影响。卷积网络学习了更高级别的特征，并且反卷积网络预进行像素级别分割。将网络应用于两个公开的数据集和一个临床试验数据集，与5种公开方法进行了比较，展现了最好的方法。Pereira等人的研究中对MRI上的脑肿瘤分割进行了研究，使用更深层的架构，数据归一化和数据增强技巧[14]。将不同的CNN架构用于肿瘤，该方法分别对疑似肿瘤的图像增强和核心区域进行分割。在2013年的公共挑战数据集上获得了最高成绩。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/Snipaste_2019-07-04_22-12-51.png" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图4 示例图像显示了不同患者的检测结果从测试集。检测到的边界框以绿色显示，标准的框以黄色显示。原点位于每个框中心的线段定义相应的坐标系 </font></center>

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211757.jpg" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图5. 增加网络深度对病变的分割性能的影响。真阳性，假阴性和假阳性体素分别以绿色，黄色和红色突出显示。由于感受野的大小增加，具有和不具有捷径的7层CEN能够比3层CEN更好地分割大的病变。</font></center>

2018年德国医疗康复机构提出一种具有代表性的基于全卷积的前列腺图像分割方法。用CNN在前列腺的MRI图像上进行端到端训练，并可以一次完成整个分割。提出了一种新的目标函数，在训练期间根据Dice系数进行优化[15]。通过这种方式，可以处理前景和背景之间存在不平衡的情况，并且增加了随机应用的数据非线性变换和直方图匹配。实验评估中表明，该方法在公开数据集上取得了优秀的结果，但大大降低了处理时间。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211804.jpg" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图6 网络架构的示意图</font></center>

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211814.jpg" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图7 PROMISE 2012数据集分割结果。</font></center>

3. 图像配准

图象配准是图象融合的前提，是公认难度较大的图象处理技术，也是决定医学图象融合技术发展的关键技术。在临床诊断中，单一模态的图像往往不能提供医生所需要的足够信息，常需将多种模式或同一模式的多次成像通过配准融合来实现感兴趣区的信息互补。在一幅图像上同时表达来自多种成像源的信息，医生就能做出更加准确的诊断或制定出更加合适的治疗方法[16]。医学图像配准包括图像的定位和转换，即通过寻找一种空间变换使两幅图像对应点达到空间位置和解剖结构上的完全一致。图6简单说明了二维图像配准的概念。图(a)和图(b)是对应于同一人脑同一位置的两幅 MRI 图像，其中图(a)是质子密度加权成像，图(b)是纵向弛豫加权成像。这两幅图像有明显的不同，第一是方位上的差异，即图(a)相对于图(b)沿水平和垂直方向分别进行了平移；第二是两幅图像所表达的内容是不一致的，图(a)表达不同组织质子含量的差别，而图(b)则突出不同组织纵向弛豫的差别。图(c)给出了两幅图像之间像素点的对应映射关系，即(a)中的每一个点fx都被映射到(b)中唯一的一个点rx。如果这种映射是一一对应的，即一幅图像空间中的每一个点在另外一幅图像空间中都有对应点，或者至少在医疗诊断上感兴趣的那些点能够准确或近似准确的对应起来，我们就称之为配准[17,18]。图(d)给出了图(a)相对于图(b)的配准图像。从图(d)中可以看出，图(d)与(b)之间的的像素点的空间位置已经近似一致了。1993 年 Petra 等综述了二维图像的配准方法，并根据配准基准的特性,将图像配准的方法分为基于外部特征的图象配准(有框架) 和基于图象内部特征的图象配准(无框架) 两种方法。 后者由于其无创性和可回溯性, 已成为配准算法的研究中心。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211822.jpg" width = "480" height = "150" /></div>
<center><font face="黑体" color=gray size=1>图8 医学图像配准原理 </font></center>

2019年华中科技大学对基于PCANet的结构非刚性多模医学图像配准展开研究。提出了一种基于PCANet的结构表示方法用于多模态医学图像配准[19]。与人工设计的特征提取方法相比，PCANet可以通过多级线性和非线性变换自动从大量医学图像中学习内在特征。所提出的方法可以通过利用PCANet的各个层中提取的多级图像特征来为多模态图像提供有效的结构表示。对Atlas，BrainWeb和RIRE数据集的大量实验表明，与MIND，ESSD，WLD和NMI方法相比，所提出的方法可以提供更低的TRE值和更令人满意的结果

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211830.jpg" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图9 第一行分别是x和y方向变形的真实结果，第二行是PSR与x和y方向的真实情况的差异；第三行是MIND方法的变形和真实值之间的差异</font></center>

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211836.jpg" width = "480" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图10 PSR，MIND，ESSD，WLD和NMI方法的CT-MR图像配准。（a）参考PD图像；（b）浮动CT图像；（c）PSR方法；（d）MIND方法；（e）ESSD方法；（f）WLD方法；（g）NMI方法</font></center>

近年来，医学图像配准技术有了新的进展,在配准方法上应用了信息学的理论和方法，例如应用最大化的互信息量作为配准准则进行图像的配准，基于互信息的弹性形变模型也逐渐成为研究热点[20]。在配准对象方面从二维图像发展到三维多模医学图像的配准。一些新算法，如基于小波变换的算法、统计学参数绘图算法、遗传算法等，在医学图像上的应用也在不断扩展。向快速和准确方面改进算法，使用最优化策略改进图像配准以及对非刚性图像配准的研究是今后医学图像配准技 术的发展方向[21,22]。

4. 图像融合

图像融合的主要目的是通过对多幅图像间的冗余数据的处理来提高图像的可读性，对多幅图像间的互补信息的处理来提高图像的清晰度。多模态医学图像的融合把有价值的生理功能信息与精确的解剖结构结合在一起，可以为临床提供更加全面和准确的资料[23]。融合图像的创建分为图像数据的融合与融合图像的显示两部分来完成。目前，图像数据融合主要有以像素为基础的方法和以图像特征为基础的方法。前者是对图像进行逐点处理，把两幅图像对应像素点的灰度值进行加权求和、灰度取大或者灰度取小等操作，算法实现比较简单，不过实现效果和效率都相对较差，融合后图像会出现一定程度的模糊。后者要对图像进行特征提取、目标分割等处理，用到的算法原理复杂，但是实现效果却比较理想。融合图像的显示常用的有伪彩色显示法、断层显示法和三维显示法等。伪彩色显示一般以某个图像为基准,用灰度色阶显示，另一幅图像叠加在基准图像上，用彩色色阶显示。断层显示法常用于某些特定图像，可以将融合后的三维数据以横断面、冠状面和矢状面断层图像同步地显示，便于观察者进行诊断。三维显示法是将融合后数据以三维图像的形式显示，使观察者可更直观地观察病灶的空间解剖位置，这在外科手术设计和放疗计划制定中有重要意义。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211845.jpg" width = "200" height = "300" /></div>
<center><font face="黑体" color=gray size=1>图11 医学图像融合阶段的总结。 两阶段过程包括图像配准，然后是图像融合。</font></center>

在图像融合技术研究中，不断有新的方法出现，其中小波变换、 基于有限元分析的非线性配准以及人工智能技术在图像融合中的应用将是今后图像融合研究的热点与方向。随着三维重建显示技术的发展，三维图像融合技术的研究也越来越受到重视，三维图像的融合和信息表达，也将是图像融合研究的一个重点。

在计算机辅助图像处理的基础上，开发出综合利用图像处理方法， 结合人体常数和部分疾病的影像特征来帮助或模拟医生分析、诊断的图像分析系统成为一种必然趋势。目前已有一些采用人机交互定点、自动测量分析的图像分析软件，能定点或定项地完成一些测量和辅助诊断的工作，但远远没有达到智能分析和专家系统的水平；全自动识别标志点并测量分析以及医学图像信息与文本信息的融合， 是计算机辅助诊断技术今后的发展方向。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/20190704211851.jpg" width = "300" height = "380" /></div>
<center><font face="黑体" color=gray size=1>图12 多模态医学图像融合的例子。使用特定图像融合技术的模态1与模态2的组合可以使医学诊断和评估改进</font></center>

5. 预测与挑战

1）数据维度问题-2D与3D：在迄今为止的大多数工作中，是在2D图像中进行处理分析。人们常常质疑向3D过渡是否是迈向性能提高的重要一步。数据增强过程中存在若干变体，包括2.5D。例如，在Roth等人的研究中，以结肠息肉或淋巴结候选体中的体素为中心截取轴向图像，存在冠状和矢状图像。

2）学习方法 - 无监督与监督：当我们查看网络文献时，很明显大多数工作都集中在受监督的CNN上，以实现分类。这种网络对于许多应用是重要的，包括检测，分割和标记。尽管如此，一些工作仍集中于无监督方案，这些方案主要表现为图像编码。诸如玻尔兹曼机器（RBM）之类的无监督表示学习方法可能胜过滤波器，因为它们直接从训练数据中学习特征描述。RBM通过生成学习目标进行培训；这使网络成为可能从未标记的数据中学习，但不一定产生最适合分类的特征。Van Tulder等人进行了一项调查，结合卷积分类和RBM的生成和判别学习目标的优点，该机器学习了对描述训练数据和分类都很好的过滤器。结果表明，学习目标的组合完全胜过生成性学习。

3) 迁移学习和微调：在医学成像领域中获取与ImageNet一样全面注释的数据集仍然是一个挑战。当没有足够的数据时，有几种方法可以继续：1）迁移学习：从自然图像数据集或不同医学领域预训练的CNN模型（监督）用于新的医疗任务。在一个方案中，预先训练CNN应用于输入图像，然后从网络层提取输出。提取的输出被认为是特征并且用于训练单独的模式分类器。2）微调：当手头的任务确实存在中等大小的数据集时，较好的方案是使用预先训练的CNN作为网络的初始化，然后进行进一步的监督训练，其中几个（或全部）网络层，使用任务的新数据。

4）数据隐私受社会和技术问题的影响，需要从社会学和技术学的角度共同解决。在卫生部门讨论隐私时，会想到HIPAA（1996年健康保险流通与责任法案）。它为患者提供有关保护个人身份信息的法律权利，并为医疗保健提供者承担保护和限制其使用或披露的义务。在医疗保健数据不断增加的同时，研究人员面临如何加密患者信息以防止其被使用或披露的问题。同时带来，限制访问数据可能遗漏非常重要的信息。

6、结论
近几年来，与传统的机器学习算法相比，深度学习在日常生活自动化方面占据了中心位置，并取得了相当大的进步。基于优秀的性能，大多数研究人员认为在未来15年内，基于深度学习的应用程序将接管人类和大多数日常活动。但是，与其它现实世界的问题相比，医疗保健领域的深度学习尤其是医学图像的发展速度非常慢。到目前为止深度学习应用提供了积极的反馈，然而，由于医疗保健数据的敏感性和挑战，我们应该寻找更复杂的深度学习方法，以便有效地处理复杂的医疗数据。随着医疗技术和计算机科学的蓬勃发展，对医学图象处理提出的要求也越来越高。有效地提高医学图象处理技术的水平，与多学科理论的交叉融合，医务人员和理论技术人员之间的交流就显得越来越重要。医学图象处理技术作为提升现代医疗诊断水平的有力依据, 使实施风险低、创伤性小的手术方案成为可能，必将在医学信息研究领域发挥更大的作用。

参考文献
[1]林晓, 邱晓嘉. 图像分析技术在医学上的应用 [J] . 包头医学院学报, 2005, 21 (3) ： 311~ 314

[2]周贤善. 医学图像处理技术综述[J]. 福建电脑, 2009(1):34-34.

[3]Mcinerney T , Terzopoulos D . Deformable models in medical image analysis: a survey[J]. Medical Image Analysis, 1996, 1(2):91.

[4]Litjens G , Kooi T , Bejnordi B E , et al. A survey on deep learning in medical image analysis[J]. Medical Image Analysis, 2017, 42:60-88.

[5]Deserno T M , Heinz H , Maier-Hein K H , et al. Viewpoints on Medical Image Processing: From Science to Application[J]. Current Medical Imaging Reviews, 2013, 9(2):79-88.

[6]A. Setio et al., “Pulmonary nodule detection in CT images using multiview convolutional networks,” IEEE Trans. Med. Imag., vol. 35, no. 5,pp. 1160–1169, May 2016.

[7]H. Roth et al., “Improving computer-aided detection using convolutional neural networks and random view aggregation,” IEEE Trans.Med. Imag., vol. 35, no. 5, pp. 1170–1181, May 2016

[8]林瑶, 田捷. 医学图像分割方法综述[J]. 模式识别与人工智能, 2002, 15(2).

[9]Ghesu F C , Georgescu B , Mansi T , et al. An Artificial Agent for Anatomical Landmark Detection in Medical Images[C]// International Conference on Medical Image Computing & Computer-assisted Intervention. Springer, Cham, 2016.

[10]Pham D L , Xu C , Prince J L . Current methods in medical image segmentation.[J]. Annual Review of Biomedical Engineering, 2000, 2(2):315-337.

[11]Lehmann T M , Gonner C , Spitzer K . Survey: interpolation methods in medical image processing[J]. IEEE Transactions on Medical Imaging, 1999, 18(11):1049-1075.

[12]Cootes T F , Taylor C J . Statistical Models of Appearance for Medical Image Analysis and Computer Vision[J]. Proceedings of SPIE - The International Society for Optical Engineering, 2001, 4322(1).

[13] T. Brosch et al., “Deep 3D convolutional encoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation,” IEEE Trans. Med. Imag., vol. 35, no. 5,pp. 1229–1239, May 2016.

[14]Ghesu F C , Krubasik E , Georgescu B , et al. Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing[J]. IEEE Transactions on Medical Imaging, 2016, 35(5):1217-1228.

[15]Milletari F , Navab N , Ahmadi S A . V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation[J]. 2016.

[16] .周永新, 罗述谦. 一种人机交互式快速脑图象配准系统[J] . 北京生物医学工程, 2002; 21 (1) ：11~14

[17]杨虎, 马斌荣, 任海萍. 基于互信息的人脑图象配准研究[J] . 中国医学物理学杂志, 2001; 18 (2) ：69~73

[18]汪家旺，愈同福，姜晓彤，等.肺部孤立性结节定量研究[J].中国医学影 像技术,2003,19(9)：1218~1219

[19]Ishihara S , Ishihara K , Nagamachi M , et al. An analysis of Kansei structure on shoes using self-organizing neural networks[J]. International Journal of Industrial Ergonomics, 1997, 19(2):93-104.

[20]Maintz J B , Viergever M A . A Survey of Medical Image Registration[J]. Computer & Digital Engineering, 2009, 33(1):140-144.

[21]Hill D L G , Batchelor P G , Holden M , et al. Medical image registration[J]. Physics in Medicine & Biology, 2008, 31(4):1-45.

[22]Razzak M I , Naz S , Zaib A . Deep Learning for Medical Image Processing: Overview, Challenges and Future[J]. 2017.

[23]林晓, 邱晓嘉. 图像分析技术在医学上的应用 [J] . 包头医学院学报, 2005, 21 (3) ： 311~ 314



***

# Tensorflow学习：

**重要：**
使用图（graph）来表示计算任务；
在被称之为会话（Session）的上下文（Context）中执行图；
使用tensor 表示数据
通过变量（Variable）维护状态；
使用feed和fatch可以为任意的操作（arbitrary operation）赋值或者从其中获取数据.
## 计算图
1. Tensorflow是一个编程系统，使用图来表示计算任务。途中的节点被称之为op(opteration的缩写).一个op获得0个或者多个Tensor，执行计算任务，产生0个或者多个Tensor.每个Tensor是一个类型化的多维数组
2. TensorFlow程序常常被组织成一个**构建阶段**和一个**执行阶段**，在构建阶段op的执行步骤被描述成一个图。在执行阶段，使用会话执行图中的op.
## 构建图
* 构建图的第一步是创建源op(source op).源op不需要任何的输入，例如常量(Constant).源op的输出被传递给其他的op做运算。

* python库中，op构造器的返回值代表被构造出来的op的输出， 这些返回值可以传递给其他的op构造器作为输入。

```python

import tensorflow as tf

# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点# 加到默认图中.## 构造器的返回值代表该常量 op 的返回值.
matrix1 = tf.constant([[3., 3.]])

# 创建另外一个常量 op, 产生一个 2x1 矩阵.
matrix2 = tf.constant([[2.],[2.]])

# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.# 返回值 'product' 代表矩阵乘法的结果.
product = tf.matmul(matrix1, matrix2)
```
默认途现在有三个节点，两个constant() op, 和一个matmul() op. 为了真正进行矩阵相乘运算，并得到矩阵的乘法的结果，你必须在会话里启动这个图

## 在一个会话中启动图
构造阶段完成后，才能启动图。启动图的第一步是创建一个Session对象，如果无任何创建参数，会话构造器将启动默认图.
```python

# 启动默认图.
sess = tf.Session()

# 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. # 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回# 矩阵乘法 op 的输出.## 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.# # 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.## 返回值 'result' 是一个 numpy `ndarray` 对象.
result = sess.run(product)
print result
# ==> [[ 12.]]

# 任务完成, 关闭会话.
sess.close()
```

Session 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 "with" 代码块 来自动完成关闭动作.
```python
with tf.Session() as sess:
    result = sess.run([product])
    print result
```
在实现上，tensorflow将图像定义转换成分布式执行的操作，以充分利用可用的计算资源

## Tensor
TensorFlow程序使用Tensor数据结构来代表所有数据，计算图中，操作间传递的数据都是tensor.可以把TensorFlow 的tensor看做一个n维的数组或者列表。一个tensor包含一个静态的rank,和一个shape

TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.

* rank: 在TensorFlow系统中，张量的位数被描述为rank，但是张量的rank和矩阵的rank不同，并不是同一个概念.张量的rank(有时候是关于顺序、度数、n维)是张量维数的一个数量描述
如
```python
t = [[1,2,3], [4,5,6,], [7,8,9]]
```

你可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句t[i, j]来访问其中的任何元素.而对于三阶张量你可以用't[i, j, k]'来访问其中的任何元素.
* shape
Tensorflow文档中使用三种记号来方便的描绘张量的维度：rank,shape以及维数。下表表示他们之间的关系

|rank|shape|dim|
|---|---|---|
|0|[]|0-D|
|1|[D0]|1-D|
|2|[D0,D1]|1-D|
|3|[D0,D1,D]|3-D|


## 变量
Variables 变量维护了图的执行过程中的状态信息。例如：
```python

# 创建一个变量, 初始化为标量 0.
state = tf.Variable(0, name="counter")

# 创建一个 op, 其作用是使 state 增加 1

one = tf.constant(1)
new_value = tf.add(state, one)
update = tf.assign(state, new_value)

# 启动图后, 变量必须先经过`初始化` (init) op 初始化,# 首先必须增加一个`初始化` op 到图中.
init_op = tf.initialize_all_variables()

# 启动图, 运行 opwith tf.Session() as sess:
  # 运行 'init' op
  sess.run(init_op)
  # 打印 'state' 的初始值
  print sess.run(state)
  # 运行 op, 更新 'state', 并打印 'state'
  for _ in range(3):
    sess.run(update)
    print sess.run(state)

输出:0# 1# 2# 3
```
## Fetch
为了取回操作的输出内容，可以在使用Session对象run()调用执行图时，传入一些tensor,这些tensor会帮助你取回结果。在之前的例子里，我们只取回了单个节点state,但是你也可以取回多个tensor:
```python

input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermed = tf.add(input2, input3)
mul = tf.mul(input1, intermed)

with tf.Session():
  result = sess.run([mul, intermed])
  print result

# 输出:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]
```

## Feed
* 计算图中引入了tensor,以常量或变量的形式存储。Tensorflow还提供了feed机制，改机制可以临时替代图中的任意操作中的tensor，可以对任意操作提供补丁，直接插入一个tensor.
* feed使用一个tensor值临时替换一个操作的输出结果。你可以提供feed数据作为run()的调用参数。feed只在调用它的方法内有效，方法结束，feed就会消失。最常见的用例是将某些特殊的操作指定为"feed"操作，标记的方法是使用tf.placeholder()为这些操作创建占位符

```python

input1 = tf.placeholder(tf.types.float32)
input2 = tf.placeholder(tf.types.float32)
output = tf.mul(input1, input2)

with tf.Session() as sess:
  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})

# 输出:# [array([ 14.], dtype=float32)]
```
***
# 面试经验及技巧
## 算法及数据结构
### 数组
* 数组是一种基本的数据结构，用于**按顺序**存储元素集合。但元素可以随机存取，因为数组中的每个元素都可以通过数组*索引*来识别。
* **动态数组**它仍然是一个随机存取的列表数据结构，但大小是可变的。例如，在 C++ 中的 vector，以及在 Java 中的 ArrayList。

### Hash table(散列表)


### 快速排序算法
快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要Ο(n log n)次比较。在最坏状况下则需要Ο(_n_2)次比较，但这种状况并不常见。事实上，快速排序通常明显比其他Ο(n log n) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。
快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。\
**算法步骤：**
1 从数列中挑出一个元素，称为 “基准”（pivot），
2 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。
3 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。
递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会退出，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/sort_quick_anim.gif" width = "480" height = "300" /></div>


### 堆排序算法
堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：**即子结点的键值或索引总是小于（或者大于）它的父节点。**
堆排序的平均时间复杂度为Ο(n_log_n) 
**算法步骤:**
1 创建一个堆H[0...n-1]
2 把堆首（最大值）和堆尾互换
3 把堆的尺寸缩小1，并调用shift_down(0),目的是把新的数组顶端数据调整到相应位置
4 重复步骤2，直到堆的尺寸为1

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/heap_sort.gif" width = "480" height = "300" /></div>

### 归并排序
归并排序（Merge sort，台湾译作：合并排序）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。
**算法步骤：**
1. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列
2. 设定两个指针，最初位置分别为两个已经排序序列的起始位置
3. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置
4. 重复步骤3直到某一指针达到序列尾
5. 将另一序列剩下的所有元素直接复制到合并序列尾

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/merge_sort.gif" width = "480" height = "300" /></div>

### 二分查找算法
二分查找算法是一种在有序数组中查找某一特定元素的搜索算法。搜素过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜素过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。折半搜索每次把搜索区域减少一半，时间复杂度为Ο(log_n_) 。

### 线性查找算法（BFPRT）
BFPRT算法解决的问题十分经典，即从某n个元素的序列中选出第k大（第k小）的元素，通过巧妙的分析，BFPRT可以保证在最坏情况下仍为线性时间复杂度。该算法的思想与快速排序思想相似，当然，为使得算法在最坏情况下，依然能达到o(n)的时间复杂度，五位算法作者做了精妙的处理。
**算法步骤：**

1. 将n个元素每5个一组，分成n/5(上界)组。
2. 取出每一组的中位数，任意排序方法，比如插入排序。
3. 递归的调用selection算法查找上一步中所有中位数的中位数，设为x，偶数个中位数的情况下设定为选取中间小的一个。
4. 用x来分割数组，设小于等于x的个数为k，大于x的个数即为n-k。
5. 若i==k，返回x；若i<k，在小于x的元素中递归查找第i小的元素；若i>k，在大于x的元素中递归查找第i-k小的元素。

终止条件：n=1时，返回的即是i小元素。


### 图遍历
* 深度优先搜索（Depth-First-Search，DFS）
深度优先搜索（缩写DFS）有点类似广度优先搜索，也是对一个连通图进行遍历的算法。它的思想是从一个顶点V0开始，沿着一条路一直走到底，如果发现不能到达目标解，那就返回到上一个节点，然后从另一条路开始走到底，这种尽量往深处走的概念即是深度优先的概念。
是搜索算法的一种。它沿着树的深度遍历树的节点，尽可能深的搜索树的分支。当节点v的所有边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止。DFS属于盲目搜索。
深度优先搜索是图论中的经典算法，利用深度优先搜索算法可以产生目标图的相应拓扑排序表，利用拓扑排序表可以方便的解决很多相关的图论问题，如最大路径问题等等。一般用堆数据结构来辅助实现DFS算法。
深度优先遍历图算法步骤：

1. 访问顶点v；
2. 依次从v的未被访问的邻接点出发，对图进行深度优先遍历；直至图中和v有路径相通的顶点都被访问；
3. 若此时图中尚有顶点未被访问，则从一个未被访问的顶点出发，重新进行深度优先遍历，直到图中所有顶点均被访问过为止。

上述描述可能比较抽象，举个实例：
DFS 在访问图中某一起始顶点 v 后，由 v 出发，访问它的任一邻接顶点 w1；再从 w1 出发，访问与 w1邻 接但还没有访问过的顶点 w2；然后再从 w2 出发，进行类似的访问，… 如此进行下去，直至到达所有的邻接顶点都被访问过的顶点 u 为止。
接着，退回一步，退到前一次刚访问过的顶点，看是否还有其它没有被访问的邻接顶点。如果有，则访问此顶点，之后再从此顶点出发，进行与前述类似的访问；如果没有，就再退回一步进行搜索。重复上述过程，直到连通图中所有顶点都被访问过为止。

* 广度优先搜索（Breadth-first search，BFS）
广度优先搜索一个图的时候是按照树的层次来搜索的，（层次遍历），队列来实现，形象的说，这里有点像辐射形状的搜索方式，从一个节点，向其旁边节点传递病毒，就这样一层一层的传递辐射下去，知道目标节点被辐射中了，此时就已经找到了从起点到终点的路径。
简单的说，BFS是从根节点开始，沿着树(图)的宽度遍历树(图)的节点。如果所有节点均被访问，则算法中止。BFS同样属于盲目搜索。一般用队列数据结构来辅助实现BFS算法。

**算法步骤：**
1. 首先将根节点放入队列中。
2. 从队列中取出第一个节点，并检验它是否为目标。
    * 如果找到目标，则结束搜寻并回传结果。
    * 否则将它所有尚未检验过的直接子节点加入队列中。
3. 若队列为空，表示整张图都检查过了——亦即图中没有欲搜寻的目标。结束搜寻并回传“找不到目标”。
4. 重复步骤2。

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/BFS.gif" width = "480" height = "380" /></div>


### Dijkstra算法
戴克斯特拉算法（Dijkstra’s algorithm）是由荷兰计算机科学家艾兹赫尔·戴克斯特拉提出。迪科斯彻算法使用了广度优先搜索解决非负权有向图的单源最短路径问题，算法最终得到一个最短路径树。该算法常用于路由算法或者作为其他图算法的一个子模块。
该算法的输入包含了一个有权重的有向图 G，以及G中的一个来源顶点 S。我们以 V 表示 G 中所有顶点的集合。每一个图中的边，都是两个顶点所形成的有序元素对。(u, v) 表示从顶点 u 到 v 有路径相连。我们以 E 表示G中所有边的集合，而边的权重则由权重函数 w: E → [0, ∞] 定义。因此，w(u, v) 就是从顶点 u 到顶点 v 的非负权重（weight）。边的权重可以想像成两个顶点之间的距离。任两点间路径的权重，就是该路径上所有边的权重总和。已知有 V 中有顶点 s 及 t，Dijkstra 算法可以找到 s 到 t的最低权重路径(例如，最短路径)。这个算法也可以在一个图中，找到从一个顶点 s 到任何其他顶点的最短路径。对于不含负权的有向图，Dijkstra算法是目前已知的最快的单源最短路径算法。
算法步骤：

初始时令 S={V0},T={其余顶点}，T中顶点对应的距离值

若存在<V0,Vi>，d(V0,Vi)为<V0,Vi>弧上的权值
若不存在<V0,Vi>，d(V0,Vi)为∞

从T中选取一个其距离值为最小的顶点W且不在S中，加入S
对其余T中顶点的距离值进行修改：若加进W作中间顶点，从V0到Vi的距离值缩短，则修改此距离值

重复上述步骤2、3，直到S中包含所有顶点，即W=Vi为止

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/Dijkstra.gif" width = "480" height = "380" /></div>

### 动态规划算法
动态规划（Dynamic programming）是一种在数学、计算机科学和经济学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 动态规划常常适用于有重叠子问题和最优子结构性质的问题，动态规划方法所耗时间往往远少于朴素解法。
动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个子问题解之时直接查表。 这种做法在重复子问题的数目关于输入的规模呈指数增长时特别有用。
关于动态规划最经典的问题当属背包问题。
**算法步骤：**

1 最优子结构性质。如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了重要线索。
2 子问题重叠性质。子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。


### 朴素贝叶斯分类算法
朴素贝叶斯分类算法是一种基于贝叶斯定理的简单概率分类算法。贝叶斯分类的基础是概率推理，就是在各种条件的存在不确定，仅知其出现概率的情况下，如何完成推理和决策任务。概率推理是与确定性推理相对应的。而朴素贝叶斯分类器是基于独立假设的，即假设样本每个特征与其他特征都不相关。
朴素贝叶斯分类器依靠精确的自然概率模型，在有监督学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法，换言之朴素贝叶斯模型能工作并没有用到贝叶斯概率或者任何贝叶斯模型。
尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够取得相当好的效果。

1. BP算法推导，以Sigmoid激活函数为例

2. 写出Leaky-ReLu的数学公式，相对于sigmoid函数的优点
sigmoid函数输入实数值并将其“挤压”到0到1范围内，适合输出为概率的情况。但其存在以下问题：
- Sigmoid函数饱和使梯度消失。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。
- Sigmoid函数的输出不是零中心的。因为如果输入神经元的数据总是正数，那么关于$w$的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降
- 幂运算比较耗时
而Leaky-ReLu函数在SGD算法的收敛速度比sigmoid和tanh快；梯度不会饱和，解决了梯度消失问题、计算复杂度低，不需要进行指数运算、适合用于后向传播
3. Adagrad的优缺点
优点：
前期g_t较小的时候， regularizer较大，能够放大梯度
后期g_t较大的时候，regularizer较小，能够约束梯度
适合处理稀疏梯度
缺点：
由公式可以看出，仍依赖于人工设置一个全局学习率
$n$设置过大的话，会使regularizer过于敏感，对梯度的调节太大
中后期，分母上梯度平方的累加将会越来越大，使gradient-->0，使得训练提前结束

4. 在图像处理中，Data augmentation有哪些方式
- Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；
- PCA  Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；
- Random Scale：尺度变换；
- Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；
- Horizontal/Vertical Flip：水平/垂直翻转；
- Shift：平移变换；
- Rotation/Reflection：旋转/仿射变换；
- Noise：高斯噪声、模糊处理；
- Label shuffle：类别不平衡数据的增广，

5. 为什么要进行数据归一化，有哪些方式
- 归一化：１）把数据变成(０，１)或者（-1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。
２）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
- 标准化：在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1，这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。
- 中心化：平均值为0，对标准差无要求
  归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。
  标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。
  无量纲：我的理解就是通过某种方法能去掉实际过程中的单位，从而简化计算。
2 为什么要归一化/标准化？
  归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。
（提升模型精度/提升收敛速度
3 数据预处理时
3.1 归一化
（1）Min-Max Normalization
   x' = (x - X_min) / (X_max - X_min)
（2）平均归一化
   x' = (x - μ) / (MaxValue - MinValue)
  （1）和（2）有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义。
（3）非线性归一化
  1）对数函数转换：y = log10(x)
  2）反余切函数转换：y = atan(x) * 2 / π
  （3）经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。
3.2 标准化
（1）Z-score规范化（标准差标准化 / 零均值标准化）
  x' = (x - μ)／σ
3.3 中心化
  x' = x - μ


编程题
1. 给你一个数列a[0], n[1],...,a[n-1]，n大于4，必定存在i, j, p, q, 使得$i<j<p<q$，且$n[j] - n[i] + n[q]-n[p]$值最大，求出这个最大值，并说明时间复杂度。
```C
#include <iostream>
#include<string.h>

using namespace std;

int calDiffmax(int *a,int n)
{
    int maxval=0;
    int sub=0;
    int k=0;
    for(int i=0;i<n;i++)
    {
        sub=a[i]-a[k];
        if(maxval<sub)
            maxval=sub;
        else if(sub<0)
        {
            k=i;//sub<0意味着a[i]<a[k],后续最大值会在a[i]<a[k]
        }
    }
    return maxval;
}

int diff2max(int *a,int n)
{
    int *left=new int[n];
    int *right=new int[n];
    memset(left,0,n);
    memset(right,0,n);
    //从左存储最大差值
    for(int i=1;i<n-2;i++)
    {
        left[i]=calDiffmax(a,i+1);
    }
    //从右存储最大差值
    for(int i=n-2;i>1;i--)
    {
        right[i]=calDiffmax(a+i,n-i);
    }
    /*
    for(int i=1;i<n;i++)
    {
        cout<<left[i]<<endl;
    }
    cout<<"*********************"<<endl;
    for(int i=1;i<n-1;i++)
    {
          cout<<right[i]<<endl;
    }
    */
 
    int max_val=0;
    int diff_sum=0;
    //寻找满足条件的最i<j左右最大差值

    for(int diff_l=1;diff_l<n-2;diff_l++)
    {
         for(int diff_r=diff_l+1;diff_r<n-1;diff_r++)
         {
             diff_sum=left[diff_l]+right[diff_r];
             if(diff_sum>max_val)
                max_val=diff_sum;
         }
    }
    return max_val;
}

int main()
{
    int a[10]={1,2,3,4,5,11,12,13,14,15};
    cout<<"max value a:"<<diff2max(a,10)<<endl;
    int b[10]={1,2,3,7,20,11,12,13,14,15};
    cout<<"max value b:"<<diff2max(b,10)<<endl;
    return 0;
}
```

2. 给你一个数组，求一个k值，使得前k个数的方差+后面n-k个数的方差最小，并说明时间复杂度。

```C

#include <iostream>
#include<stdio.h>
#include<string.h>


using namespace std;

int minVar(float *a,int n)
{
    float *left=new float[n];
    float *right=new float[n];
    float leftsum=0;
    float rightsum=0;
    float leftSquareSum=0;
    float rightSquareSum=0;
    memset(left,0,n);
    memset(right,0,n);
    for(int i=0;i<n;i++)
    {
        leftsum+=a[i];
        leftSquareSum+=a[i]*a[i];
        left[i]=leftSquareSum/(i+1)-(leftsum/(i+1))*(leftsum/(i+1));
    }
    /*
    for(int i=0;i<n;i++)
    {
        cout<<left[i]<<endl;
    }*/

    for(int i=n-1;i>=0;i--)
    {
        rightsum+=a[i];
        rightSquareSum+=a[i]*a[i];
        right[i]=rightSquareSum/(n-i)-(rightsum/(n-i))*(rightsum/(n-i));
    }
    /*
    for(int i=0;i<n;i++)
    {
        cout<<right[i]<<endl;
    }
    */
    for(int i=0;i<n-1;i++)
    {
        left[i]+=right[i+1];
    }

    /*
    cout<<endl;
    for(int i=0;i<n;i++)
    {
        cout<<left[i]<<endl;
    }
   */

    float minvar=left[0];
    int pos=0;
    for(int i=0;i<n;i++)
    {
        if(minvar>left[i])
        {
            minvar=left[i];
            pos=i;
        }
    }
    cout<<"minVar:"<<minvar<<endl;
    return (pos+1);
}

int main()
{
    float a[10]={1,2,3,4,5,11,12,13,14,15};
    cout<<"pos:"<<minVar(a,10)<<endl;
    return 0;
}

```

自我解答：
单选题：
1.B 2,A 3,C 4,B 5,C
多选题：
1.ABD 2.AC 3.AC 4.AB

### 卷积层、池化层、全连接层简单的比较
|  |卷积层|池化层|全连接层|
|--|:---|:---|:---|
|功能|提取特征|压缩特征图，提取主要特征|将学到的“分布式特征表示”映射到样本标记空间|
|操作|https://www.cnblogs.com/nsnow/p/4562308.html可看这个的动态图，可惜是二维的。对于三维数据比如RGB图像（3通道），卷积核的深度必须同输入的通道数，输出的通道数等于卷积核的个数。
卷积操作会改变输入特征图的通道数。|池化只是在二维数据上操作的，因此不改变输入的通道数。对于多通道的输入，这一点和卷积区别很大。|可以看做一种卷积操作，代码实现时先将输入转换为向量，然后送入卷积大小为向量长度的卷积层即可。|
|特性|权值共享：减少了参数的数量，并利用了图像目标的位置无关性。稀疏连接：输出的每个值只依赖于输入的部分值。| | 参数很多|
---
## 数学基础
### 微积分
1. SGD,Momentum,Adagard,Adam原理?
SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行跟新。
Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。
Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。
Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。

2. L1不可导的时候该怎么办
当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是沿着坐标轴的方向,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。
使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。

3. sigmoid函数特性
定义域为$(-\infty, +\infty)$
值域为$(-1,1)$
函数在定义域内为连续和光滑的函数
处处可导,导数为 $f'(x) = f(x)(1-f(x))$

### 统计学概率论
4. 切比雪夫不等式
${\rm{P}}(\left| {X - \mu } \right| \ge k\sigma ) \le \frac{1}{{{k^2}}}$, $k>0$, $\mu$为期望， $\sigma$为标准差

5. 最大似然估计和最大后验概率的区别?
最大似然估计提供了一种给定观察数据来评估模型参数的方法,而最大似然估计中的采样满足所有采样都是独立同分布的假设。最大后验概率是根据经验数据获难以观察量的点估计,与最大似然估计最大的不同是最大后验概率融入了要估计量的先验分布在其中,所以最大后验概率可以看做规则化的最大似然估计。

6. 什么是共轭先验分布?
假设$\theta$为总体分布中的参数,$\theta$的先验密度函数为$\pi(\theta)$,而抽样信息算得的后验密度函数与$\pi(\theta)$具有相同的函数形式,则称$\pi(\theta)$为$\theta$的共轭先验分布。

7. 概率和似然的区别?
概率是指在给定参数$\theta$的情况下,样本的随机向量X=x的可能性。而似然表示的是在给定样本X=x的情况下,参数$\theta$为真实值的可能性。一般情况,对随机变量的取值用概率表示。而在非贝叶斯统计的情况下,参数为一个实数而不是随机变量,一般用似然来表示。

8. 频率学派和贝叶斯学派的区别?
频率派认为抽样是无限的,在无限的抽样中,对于决策的规则可以很精确。贝叶斯派认为世界无时无刻不在改变,未知的变量和事件都有一定的概率,即后验概率是先验概率的修正。频率派认为模型参数是固定的,一个模型在无数次抽样后,参数是不变的。而贝叶斯学派认为数据才是固定的而参数并不是。频率派认为模型不存在先验而贝叶斯派认为模型存在先验。

9. 0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器?
0~1的均匀分布是均值为1/2，方差为0.转成均值为0，方差为1.概率论题目

10. Lasso的损失函数
\[{\rm{J}}(\theta ) = \frac{1}{2}{\left\| {y - Xw} \right\|^2} + \lambda \sum {\left| \theta  \right|} \]

11. Sfit特征提取和匹配的具体步骤？
生成高斯差分金字塔,尺度空间构建,空间极值点检测,稳定关键点的精确定位,稳定关键点方向信息分配,关键点描述,特征点匹配。

### 线性代数
12. 求$m*k$矩阵A和$n*k$矩阵B的欧几里得距离?
先得到矩阵$AB^T$,然后对矩阵$A$和矩阵分别求出其中每个向量的模平方,并扩展为两个$m*k$的矩阵$A'$和$B'$。最终求得新的矩阵$A'+B'-2AB^T$,并将此矩阵开平方得到A,B向量集的欧几里得距离。

13. 欧拉公式?
\[{e^{ix}} = \cos x + i\sin x\]

14. 矩阵正定性的判断,Hessian矩阵正定性在梯度下降中的应用?
若矩阵所有特征值均不小于0,则判定为半正定。若矩阵所有特征值均大于0,则判定为正定。在判断优化算法的可行性时Hessian矩阵的正定性起到了很大的作用,若Hessian正定,则函数的二阶偏导恒大于0,函数的变化率处于递增状态,在牛顿法等梯度下降的方法中,Hessian矩阵的正定性可以很容易的判断函数是否可收敛到局部或全局最优解。

15. 概率题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数
据红球和蓝球的个数，依据概率公式，求出平均抽取次数。

16. 讲一下PCA?
PCA是比较常见的线性降维方法,通过线性投影将高维数据映射到低维数据中,所期望的是在投影的维度上,新特征自身的方差尽量大,方差越大特征越有效,尽量使产生的新特征间的相关性越小。
PCA算法的具体操作为对所有的样本进行中心化操作,计算样本的协方差矩阵,然后对协方差矩阵做特征值分解,取最大的n个特征值对应的特征向量构造投影矩阵。

17. 拟牛顿法的原理
牛顿法的收敛速度快,迭代次数少,但是Hessian矩阵很稠密时,每次迭代的计算量很大,随着数据规模增大,Hessian矩阵也会变大,需要更多的存储空间以及计算量。拟牛顿法就是在牛顿法的基础上引入了Hessian矩阵的近似矩阵,避免了每次都计算Hessian矩阵的逆,在拟牛顿法中,用Hessian矩阵的逆矩阵来代替Hessian矩阵,虽然不能像牛顿法那样保证最优化的方向,但其逆矩阵始终是正定的,因此算法始终朝最优化的方向搜索。

18. 编辑距离
概念
编辑距离的作用主要是用来比较两个字符串的相似度的

编辑距离，又称Levenshtein距离（莱文斯坦距离也叫做Edit Distance），是指两个字串之间，由一个转成另一个所需的最少编辑操作次数，如果它们的距离越大，说明它们越是不同。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。

在概念中，我们可以看出一些重点那就是，编辑操作只有三种。插入，删除，替换这三种操作，我们有两个字符串，将其中一个字符串经过上面的这三种操作之后，得到两个完全相同的字符串付出的代价是什么就是我们要讨论和计算的。

例如：

我们有两个字符串：kitten 和 sitting:

现在我们要将kitten转换成sitting

我们可以做如下的一些操作；

kitten–>sitten 将K替换成S   sitten–> sittin 将e替换成i

sittin–> sitting 添加g

在这里我们设置每经过一次编辑，也就是变化（插入，删除，替换）我们花费的代价都是1。

例如：

如果str1=”ivan”，str2=”ivan”，那么经过计算后等于 0。没有经过转换。相似度=1-0/Math.Max(str1.length,str2.length)=1

如果str1=”ivan1”，str2=”ivan2”，那么经过计算后等于1。str1的”1”转换”2”，转换了一个字符，所以距离是1，相似度=1-1/Math.Max(str1.length,str2.length)=0.8

算法过程

1.str1或str2的长度为0返回另一个字符串的长度。 if(str1.length==0) return str2.length; if(str2.length==0) return str1.length;

2.初始化(n+1)*(m+1)的矩阵d，并让第一行和列的值从0开始增长。扫描两字符串（n*m级的），如果：str1[i] == str2[j]，用temp记录它，为0。否则temp记为1。然后在矩阵d[i,j]赋于d[i-1,j]+1 、d[i,j-1]+1、d[i-1,j-1]+temp三者的最小值。

3.扫描完后，返回矩阵的最后一个值d[n][m]即是它们的距离。

计算相似度公式：1-它们的距离/两个字符串长度的最大值。

我们用字符串“ivan1”和“ivan2”举例来看看矩阵中值的状况：

1、第一行和第一列的值从0开始增长

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/bianji1.png" width = "480" height = "380" /></div>
图一




首先我们先创建一个矩阵，或者说是我们的二维数列，假设有两个字符串，我们的字符串的长度分别是m和n，那么，我们矩阵的维度就应该是(m+1)*(n+1).

注意，我们先给数列的第一行第一列赋值，从0开始递增赋值。我们就得到了图一的这个样子

之后我们计算第一列，第二列，依次类推，算完整个矩阵。

我们的计算规则就是：

d[i,j]=min(d[i-1,j]+1 、d[i,j-1]+1、d[i-1,j-1]+temp) 这三个当中的最小值。

其中：str1[i] == str2[j]，用temp记录它，为0。否则temp记为1

我们用d[i-1,j]+1表示增加操作

d[i,j-1]+1 表示我们的删除操作

d[i-1,j-1]+temp表示我们的替换操作

2、举证元素的产生 Matrix[i - 1, j] + 1 ; Matrix[i, j - 1] + 1 ; Matrix[i - 1, j - 1] + t 三者当中的最小值
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/bianjijul2.png" width = "480" height = "380" /></div>

3.依次类推直到矩阵全部生成
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/biaijijuli3.png" width = "480" height = "380" /></div>
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/bianjijuli4.png" width = "480" height = "380" /></div>

这个就得到了我们的整个完整的矩阵。

---
## 机器学习算法
### 分类问题
1. 交叉熵公式？
交叉熵：设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：
\[{\rm{D(p||q) = }}\sum\limits_x {p(x)\log \frac{{p(x)}}{{q(x)}}}  = {E_{p(x)}}\log \frac{{p(x)}}{{q(x)}}\]
在一定程度上，相对熵可以度量两个随机变量的“距离”，且有D(p||q) ≠D(q||p)。另外，值得一提的是，D(p||q)是必然大于等于0的。

互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示：
\[{\rm{I(X,Y) = }}\sum\limits_{x,y} {p(x,y)\log \frac{{p(x,y)}}{{p(x)p(y)}}} \]

2.  LR公式？
逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层逻辑函数g(z)，即先把特征线性求和，然后使用函数g(z)作为假设函数来预测。g(z)可以将连续值映射到0 和1。g(z)为sigmoid function.
\[\begin{array}{l}
{\rm{g}}(z) = \frac{1}{{1 + {e^{ - z}}}}\\
{h_\theta }(x) = g({\theta ^T}x) = \frac{1}{{1 + {e^{ - {\theta ^T}x}}}}
\end{array}\]
3. 伯努利分布？
\[\begin{array}{l}
{\rm{P}}(y = 1|x;\theta ) = {h_\theta }(x)\\
{\rm{P}}(y = 0|x;\theta ) = 1 - {h_\theta }(x)
\end{array}\]
4. 极大似然函数？
对于训练数据集，特征数据x={x1, x2, … , xm}和对应的分类标签y={y1, y2, … , ym}，假设m个样本是相互独立的，那么，极大似然函数为：
\[\begin{array}{l}
L(\theta ) = p(\vec y|X;\theta )\\
 = \prod\limits_{i = 1}^m {p({y^{(i)}}|{x^{(i)}};\theta )} 
\end{array}\]

5. 逻辑回归怎么实现多分类？
方式一:修改逻辑回归的损失函数,使用softmax函数构造模型解决多分类问题,softmax分类模型会有相同于类别数的输出,输出的值为对于样本属于各个类别的概率,最后对于样本进行预测的类型为概率值最高的那个类别。
方式二:根据每个类别都建立一个二分类器,本类别的样本标签定义为0,其它分类样本标签定义为1,则有多少个类别就构造多少个逻辑回归分类器
若所有类别之间有明显的互斥则使用softmax分类器,若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。
6. SVM中什么时候用线性核什么时候用高斯核?
当数据的特征提取的较好,所包含的信息量足够大,很多问题是线性可分的那么可以采用线性核。若特征数较少,样本数适中,对于时间不敏感,遇到的问题是线性不可分的时候可以使用高斯核来达到更好的效果。
7. 什么是支持向量机,SVM与LR的区别?
支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔,最终可转化为凸二次规划问题求解。
LR是参数模型,SVM为非参数模型。LR采用的损失函数为logisticalloss,而SVM采用的是hingeloss。在学习分类器的时候,SVM只考虑与分类最相关的少数支持向量点。LR的模型相对简单,在进行大规模线性分类时比较方便。
8. 监督学习和无监督学习的区别？
输入的数据有标签则为监督学习,输入数据无标签为非监督学习。
9. 机器学习中的距离计算方法?
设空间中两个点为$(x_1, y_1)、(x_2, y_2)$
- 欧式距离：
\[\sqrt {{{{\rm{(}}{{\rm{x}}_1}{\rm{ - }}{{\rm{x}}_2}{\rm{)}}}^2}{\rm{ + (}}{{\rm{y}}_1}{\rm{ - }}{{\rm{y}}_2}{{\rm{)}}^2}} \]
- 曼哈顿距离：
\[{{\rm{x}}_1} - {x_2}\left| {{y_1} - {y_2}} \right|\]
- 余弦距离：
\[\cos  = \frac{{{x_1}*{x_2} + {y_1}*{y_2}}}{{\sqrt {x_1^2 + y_1^2} *\sqrt {x_2^2 + y_2^2} }}\]

- 契比雪夫距离：
\[{x_1} - {x_2}{y_1} - {y_2}\]

10. 朴素贝叶斯（naive Bayes）法的要求是？
贝叶斯定理、特征条件独立假设
解析：朴素贝叶斯属于生成式模型，学习输入和输出的联合概率分布。给定输入x，利用贝叶斯概率定理求出最大的后验概率作为输出y。

11. 训练集中类别不均衡，哪个参数最不准确？
准确度（Accuracy）
解析：举例，对于二分类问题来说，正负样例比相差较大为99:1，模型更容易被训练成预测较大占比的类别。因为模型只需要对每个样例按照0.99的概率预测正类，该模型就能达到99%的准确率。

12. SVM的作用，基本实现原理；
SVM可以用于解决二分类或者多分类问题，此处以二分类为例。SVM的目标是寻找一个最优化超平面在空间中分割两类数据，这个最优化超平面需要满足的条件是：离其最近的点到其的距离最大化，这些点被称为支持向量。
解析：建议练习推导SVM，从基本式的推导，到拉格朗日对偶问题。
13. SVM的硬间隔，软间隔表达式；
- 硬间隔：
\[\begin{array}{l}
\mathop {\min }\limits_{w,b} \frac{1}{2}{\left\| w \right\|^2}\\
s.t:{y^{(i)}}({w^T}{x^{(i)}} + b) >  = 1
\end{array}\]
- 软间隔：
\[\begin{array}{l}
\mathop {\min }\limits_{w,b} \frac{1}{2}{\left\| w \right\|^2} + C\sum\limits_{i = 1}^m {{\xi _i}} \\
s.t:{y^{(i)}}({w^T}{x^{(i)}} + b) >  = 1 - {\xi _i};{\xi _i} >  = 0
\end{array}\]

解析：不同点在于有无引入松弛变量

14. SVM使用对偶计算的目的是什么，如何推出来的，手写推导？
目的有两个：一是方便核函数的引入；二是原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与问题的变量个数有关。由于SVM的变量个数为支持向量的个数，相较于特征位数较少，因此转对偶问题。通过拉格朗日算子发使带约束的优化目标转为不带约束的优化函数，使得W和b的偏导数等于零，带入原来的式子，再通过转成对偶问题。



15. SVM的物理意义是什么？
构造一个最优化的超平面在空间中分割数据

16. 如果给你一些数据集，你会如何分类（我是分情况答的，从数据的大小，特征，是否有缺失，分情况分别答的）；
根据数据类型选择不同的模型，如Lr或者SVM，决策树。假如特征维数较多，可以选择SVM模型，如果样本数量较大可以选择LR模型，但是LR模型需要进行数据预处理；假如缺失值较多可以选择决策树。选定完模型后，相应的目标函数就确定了。还可以在考虑正负样例比比，通过上下集采样平衡正负样例比。
解析：需要了解多种分类模型的优缺点，以及如何构造分类模型的步骤

17. 如果数据有问题，怎么处理？
1.上下采样平衡正负样例比；
2.考虑缺失值；
3.数据归一化

18. 分层抽样的适用范围？
分层抽样利用事先掌握的信息,充分考虑了保持样本结构和总体结构的一致性,当总体由差异明显的几部分组成的时候,适合用分层抽样。

19. LR的损失函数？
\[J(\theta ) =  - \frac{1}{M}\sum\nolimits_{i = 1}^M {[{y_i}\log ({h_\theta }({x_i})) + (1 - {y_i})\log (1 - {h_\theta }({x_i}))]} \]
M为样本个数,${{h_\theta }({x_i})}$为模型对样本i的预测结果,$y_i$为样本i的真实标签

20. LR和线性回归的区别?
线性回归用来做预测,LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数,LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

21. 生成模型和判别模型基本形式，有哪些？
生成式：朴素贝叶斯、HMM、Gaussians、马尔科夫随机场
判别式：LR，SVM，神经网络，CRF，Boosting
详情：支持向量机

22. 核函数的种类和应用场景?
线性核、多项式核、高斯核。
特征维数高选择线性核
样本数量可观、特征少选择高斯核（非线性核）
样本数量非常多选择线性核（避免造成庞大的计算量）
详情：支持向量机

23. 分类算法列一下有多少种？应用场景
单一的分类方法主要包括：LR逻辑回归，SVM支持向量机，DT决策树、NB朴素贝叶斯、NN人工神经网络、K-近邻；集成学习算法：基于Bagging和Boosting算法思想，RF随机森林,GBDT，Adaboost,XGboost。

24. SVM核函数的选择?
当样本的特征很多且维数很高时可考虑用SVM的线性核函数。当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。

25. SVM的损失函数:
\[J(\theta ) = \frac{1}{2}{\left\| \theta  \right\|^2} + C\sum\limits_i {\max (0,1 - {y_i}({\theta ^T}{x_i} + b))} \]

26. 核函数的作用:
核函数隐含着一个从低维空间到高维空间的映射,这个映射可以把低维空间中线性不可分的两类点变成线性可分的。

27. SVM为什么使用对偶函数求解?
对偶将原始问题中的约束转为了对偶问题中的等式约束,而且更加方便了核函数的引入,同时也改变了问题的复杂度,在原始问题下,求解问题的复杂度只与样本的维度有关,在对偶问题下,只与样本的数量有关。

28.  ID3,C4.5和CART三种决策树的区别?
ID3决策树优先选择信息增益大的属性来对样本进行划分,但是这样的分裂节点方法有一个很大的缺点,当一个属性可取值数目较多时,可能在这个属性对应值下的样本只有一个或者很少个,此时它的信息增益将很高,ID3会认为这个属性很适合划分,但实际情况下叫多属性的取值会使模型的泛化能力较差,所以C4.5不采用信息增益作为划分依据,而是采用信息增益率作为划分依据。但是仍不能完全解决以上问题,而是有所改善,这个时候引入了CART树,它使用gini系数作为节点的分裂依据。

29. SVM和全部数据有关还是和局部数据有关?
SVM只和分类界限上的支持向量点有关,换而言之只和局部数据有关。

30. 为什么高斯核能够拟合无穷维度?
因为将泰勒展开式代入高斯核,将会得到一个无穷维度的映射。

31. 完整推导svm一遍，还有强化学习说一说，dqn的各种trick了解多少，以及都怎么实现

32. LR和SVM 区别?
1）LR是参数模型，SVM是非参数模型。2）从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3）SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4）逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。5）logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

33. 朴素贝叶斯基本原理和预测过程?
朴素贝叶斯分类和预测算法的原理
决策树和朴素贝叶斯是最常用的两种分类算法，本篇文章介绍朴素贝叶斯算法。贝叶斯定理是以英国数学家贝叶斯命名，用来解决两个条件概率之间的关系问题。简单的说就是在已知P(A|B)时如何获得P(B|A)的概率。朴素贝叶斯（Naive Bayes）假设特征P(A)在特定结果P(B)下是独立的。

1.概率基础：

在开始介绍贝叶斯之前，先简单介绍下概率的基础知识。概率是某一结果出现的可能性。例如，抛一枚匀质硬币，正面向上的可能性多大？概率值是一个0-1之间的数字，用来衡量一个事件发生可能性的大小。概率值越接近1，事件发生的可能性越大，概率值越接近0，事件越不可能发生。我们日常生活中听到最多的是天气预报中的降水概率。概率的表示方法叫维恩图。下面我们通过维恩图来说明贝叶斯公式中常见的几个概率。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/weien.png" width = "380" height = "280" /></div>

在维恩图中：
S：S是样本空间，是所有可能事件的总和。
P(A)：是样本空间S中A事件发生的概率，维恩图中绿色的部分。
P(B)：是样本空间S中B事件发生的概率，维恩图中蓝色的部分。
P(A∩B)：是样本空间S中A事件和B事件同时发生的概率，也就是A和B相交的区域。
P(A|B)：是条件概率，是B事件已经发生时A事件发生的概率。

对于条件概率，还有一种更清晰的表示方式叫概率树。下面的概率树表示了条件概率P(A|B)。与维恩图中的P(A∩B)相比，可以发现两者明显的区别。P(A∩B)是事件A和事件B同时发现的情况，因此是两者相交区域的概率。而事件概率P(A|B)是事件B发生时事件A发生的概率。这里有一个先决条件就是P(B)要首先发生。
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/gailvshu_P(AB).png" width = "380" height = "180" /></div>
因为条件概率P(A|B)是在事件B已经发生的情况下，事件A发生的概率，因此P(A|B)可以表示为事件A与B的交集与事件B的比率。

\[P(A|B) = \frac{{P(A \cap B)}}{{P(B)}}\]
\[P(A \cap B) = P(A|B)P(B)\]

2.贝叶斯公式：
贝叶斯算法通过已知的P(A|B)，P(A),和P(B)三个概率计算P(B|A)发生的概率。假设我们现在已知P(A|B)，P(A)和P(B)三个概率，如何计算P(B|A)呢？通过前面的概率树及P(A|B)的概率可知，P(B|A)的概率是在事件A发生的前提下事件B发生的概率，因此P(B|A)可以表示为事件B与事件A的交集与事件A的比率。
\[\begin{array}{l}
P({\rm{B}}|A) = \frac{{P(B \cap A)}}{{P(A)}}\\
P(B \cap A) = P(B|A)P(A)
\end{array}\]
到这一步，我们只需要证明P(A∩B)= P(B∩A)就可以证明在已知P(A|B)的情况下可以通过计算获得P(B|A)的概率，将概率树转化为下面的概率表，分别列出P(A|B),P(B|A),P(A),和P(B)的概率：
<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/beys2.png" width = "380" height = "280" /></div>
通过计算可以证明P(A|B)*P(B)和P(B|A)*P(A)最后求得的结果是概率表中的同一个区域的值，因此：

\[P(A \cap B) = P(B \cap A)\]

我们通过$P(A∩B)= P(B∩A)$证明了在已知P(A|B)，P(A),和P(B)三个概率的情况下可以计算出P(B|A)发生的概率。整个推导和计算过程可以说得通。但从统计学的角度来看，P(A|B)和P(B|A)两个条件概率之间存在怎样的关系呢？我们从贝叶斯推断里可以找到答案。
3.贝叶斯推断：
贝叶斯推断可以说明贝叶斯定理中两个条件概率之间的关系。换句话说就是我们为什么可以通过P(A|B)，P(A),和P(B)三个概率计算出P(B|A)发生的概率。

\[P(B|A) = \frac{{P(A|B)*P(B)}}{{P(A)}}\]

在贝叶斯推断中，每一种概率都有一个特定的名字：
 - P(B)是”先验概率”(Prior probability)。
* P(A)是”先验概率”(Prior probability)，也作标准化常量(normalized constant)。
- P(A|B)是已知B发生后A的条件概率，叫做似然函数(likelihood)。
- P(B|A)是已知A发生后B的条件概率，是我们要求的值，叫做后验概率。
- P(A|B)/P(A)是调整因子，也被称作标准似然度（standardised likelihood）。

贝叶斯推断中有几个关键的概念需要说明下：
- 第一个是先验概率，先验概率是指我们主观通过事件发生次数对概率的判断。
- 第二个是似然函数，似然函数是对某件事发生可能性的判断，与条件概率正好相反。通过事件已经发生的概率推算事件可能性的概率。
维基百科中对似然函数与概率的解释：
概率：是给定某一参数值，求某一结果的可能性。
例如，抛一枚匀质硬币，抛10次，6次正面向上的可能性多大？
似然函数：给定某一结果，求某一参数值的可能性。
例如，抛一枚硬币，抛10次，结果是6次正面向上，其是匀质的可能性多大？

- 第三个是调整因子：调整因子是似然函数与先验概率的比值，这个比值相当于一个权重，用来调整后验概率的值，使后验概率更接近真实概率。调整因子有三种情况，大于1，等于1和小于1。
调整因子P(A|B)/P(A)>1：说明事件可能发生的概率要大于事件已经发生次数的概率。
调整因子P(A|B)/P(A)=1：说明事件可能发生的概率与事件已经发生次数的概率相等。
调整因子P(A|B)/P(A)<1：说明事件可能发生的概率与事件小于已经发生次数的概率。
因此，贝叶斯推断可以理解为通过先验概率和调整因子来获得后验概率。其中调整因子是根据事件已经发生的概率推断事件可能发生的概率（通过硬币正面出现的次数来推断硬币均匀的可能性），并与已经发生的先验概率（硬币正面出现的概率）的比值。通过这个比值调整先验概率来获得后验概率。
后验概率＝先验概率ｘ调整因子

34. 请你说一说交叉熵，也可以再说一下其他的你了解的熵？
为了更好的理解，需要了解的概率必备知识有：
大写字母X表示随机变量，小写字母x表示随机变量X的某个具体的取值；
P(X)表示随机变量X的概率分布，P(X,Y)表示随机变量X、Y的联合概率分布，P(Y|X)表示已知随机变量X的情况下随机变量Y的条件概率分布；
p(X=x)表示随机变量X取某个具体值的概率，简记为p(x)；
p(X=x,Y=y) 表示联合概率，简记为p(x,y)，p(Y=y|X=x)表示条件概率，简记为p(y|x)，且有：p(x,y)=p(x)*p(y|x)。
- 熵：如果一个随机变量X的可能取值为X={x1,x2,…,xk}，其概率分布为P(X=xi)=pi（i= 1,2,...,n），则随机变量X的熵定义为：

\[H(X) =  - \sum\limits_x {p(x)\log p(x)} \]

联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。

条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。
且有此式子成立：H(Y|X)=H(X,Y)-H(X)，整个式子表示(X,Y)发生所包含的熵减去X单独发生包含的熵。至于怎么得来的请看推导

<div align=center><img src="https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/xaingduishang.png" width = "380" height = "280" /></div>
简单解释下上面的推导过程。整个式子共6行，其中
第二行推到第三行的依据是边缘分布p(x)等于联合分布p(x,y)的和；
第三行推到第四行的依据是把公因子logp(x)乘进去，然后把x,y写在一起；
第四行推到第五行的依据是：因为两个sigma都有p(x,y)，故提取公因子p(x,y)放到外边，然后把里边的-logp(x,y)-logp(x)）写成-log(p(x,y)/p(x)) ；
第五行推到第六行的依据是：p(x,y)=p(x)*p(y|x)，故p(x,y)/p(x)=p(y|x)。

- 相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设p(x)、q(x)是X中取值的两个概率分布，则p对q的相对熵是：
\[D(p\left\| q \right.) = \sum\limits_x {p(x)\log \frac{{p(x)}}{{q(x)}}} {\rm{ = }}{{\rm{E}}_{p(x)}}\log \frac{{p(x)}}{{q(x)}}\]

### 回归问题
35. L1和L2正则化的区别？
L1是模型各个参数的绝对值之和,L2为各个参数平方和的开方值。L1更趋向于产生少量的特征,其它特征为0,最优的参数值很大概率出现在坐标轴上,从而导致产生稀疏的权重矩阵,而L2会选择更多的矩阵,但是这些矩阵趋向于0。

36. Loss Function有哪些，怎么用？
平方损失（预测问题）、交叉熵（分类问题）、hinge损失（SVM支持向量机）、CART回归树的残差损失

37. 线性回归的表达式，损失函数；？
线性回归y=wx+b，w和x可能是多维。线性回归的损失函数为平方损失函数。
解析：一般会要求反向求导推导

38. 线性回归的损失函数？
\[\begin{array}{l}
J(\theta ) = \frac{1}{2}\sum\nolimits_{i = 1}^m {{{(h({x_i}) - {y_i})}^2}} \\
\frac{{\min }}{\theta }J(\theta )
\end{array}\]

39. 知道哪些传统机器学习模型？
常见的机器学习算法：
1）.回归算法：回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。 常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。

2）.基于实例的算法：基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map，SOM）。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

3）.决策树学习：决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree，CART），ID3 (Iterative Dichotomiser 3)，C4.5，Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest），多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。

4）.贝叶斯方法：贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators，AODE），以及Bayesian Belief Network（BBN）。

5）.基于核的算法：基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间，在这些高阶向量空间里，有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）， 径向基函数（Radial Basis Function，RBF)，以及线性判别分析（Linear Discriminate Analysis，LDA)等。

6）.聚类算法：聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization，EM）。

7）.降低维度算法：像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis，PCA），偏最小二乘回归（Partial Least Square Regression，PLS），Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）, 投影追踪（Projection Pursuit）等。

8）.关联规则学习：关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。

9）.集成算法：集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting，Bootstrapped Aggregation（Bagging），AdaBoost，堆叠泛化（Stacked Generalization，Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。

10）.人工神经网络：人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation），Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）。

### 聚类问题
40. 什么是DBSCAN？
DBSCAN是一种基于密度的空间聚类算法,它不需要定义簇的个数,而是将具有足够高密度的区域划分为簇,并在有噪声的数据中发现任意形状的簇,在此算法中将簇定义为密度相连的点的最大集合。

41. k-means算法流程?
从数据集中随机选择k个聚类样本作为初始的聚类中心,然后计算数据集中每个样本到这k个聚类中心的距离,并将此样本分到距离最小的聚类中心所对应的类中。将所有样本归类后,对于每个类别重新计算每个类别的聚类中心即每个类中所有样本的质心,重复以上操作直到聚类中心不变为止。

42. LDA的原理
LDA是一种基于有监督学习的降维方式,将数据集在低维度的空间进行投影,要使得投影后的同类别的数据点间的距离尽可能的靠近,而不同类别间的数据点的距离尽可能的远。

43. 介绍几种机器学习的算法，我就结合我的项目经理介绍了些RF, Kmeans等算法。
常见的机器学习算法：
1）. 回归算法：回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。 常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。

2）. 基于实例的算法：基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map，SOM）。深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

3）. 决策树学习：决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree，CART），ID3 (Iterative Dichotomiser 3)，C4.5，Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest），多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine，GBM）。

4）. 贝叶斯方法：贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators，AODE），以及Bayesian Belief Network（BBN）。

5）. 基于核的算法：基于核的算法中最著名的莫过于支持向量机（SVM）了。基于核的算法把输入数据映射到一个高阶的向量空间，在这些高阶向量空间里，有些分类或者回归问题能够更容易的解决。常见的基于核的算法包括：支持向量机（Support Vector Machine，SVM）， 径向基函数（Radial Basis Function，RBF)，以及线性判别分析（Linear Discriminate Analysis，LDA)等。

6）. 聚类算法：聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization，EM）。

7）. 降低维度算法：像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis，PCA），偏最小二乘回归（Partial Least Square Regression，PLS），Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）, 投影追踪（Projection Pursuit）等。

8）. 关联规则学习：关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。

9）. 集成算法：集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting，Bootstrapped Aggregation（Bagging），AdaBoost，堆叠泛化（Stacked Generalization，Blending），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。

10）. 人工神经网络：人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation），Hopfield网络，自组织映射（Self-Organizing Map, SOM）。学习矢量量化（Learning Vector Quantization， LVQ）。

RF：通过对训练数据样本以及属性进行有放回的抽样（针对某一个属性随机选择样本）这里有两种，一种是每次都是有放回的采样，有些样本是重复的，组成和原始数据集样本个数一样的数据集；另外一种是不放回的抽样，抽取出大约60%的训练信息。由此生成一颗CART树，剩下的样本信息作为袋外数据，用来当作验证集计算袋外误差测试模型；把抽取出的样本信息再放回到原数据集中，再重新抽取一组训练信息，再以此训练数据集生成一颗CART树。这样依次生成多颗CART树，多颗树组成森林，并且他们的生成都是通过随机采样的训练数据生成，因此叫随机森林。RF可以用于数据的回归，也可以用于数据的分类。回归时是由多颗树的预测结果求均值；分类是由多棵树的预测结果进行投票。正式由于它的随机性，RF有极强的防止过拟合的特性。由于他是由CART组成，因此它的训练数据不需要进行归一化，因为每课的建立过程都是通过选择一个能最好的对数据样本进行选择的属性来建立分叉，因此有以上好处的同时也带来了一个缺点，那就是忽略了属性与属性之间的关系。

K-meas：基本K-Means算法的思想很简单，事先确定常数K，常数K意味着最终的聚类类别数，首先随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，接着，重新计算每个类的质心(即为类中心)，重复这样的过程，知道质心不再改变，最终就确定了每个样本所属的类别以及每个类的质心。由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。

初始化常数K，随机选取初始点为质心

重复计算一下过程，直到质心不再改变

计算样本与每个质心之间的相似度，将样本归类到最相似的类中

重新计算质心

输出最终的质心以及每个类

44. KMeans讲讲，KMeans有什么缺点，K怎么确定?
在k-means算法中，用质心来表示cluster；且容易证明k-means算法收敛等同于所有质心不再发生变化。基本的k-means算法流程如下：
选取k个初始质心（作为初始cluster）；

repeat： 对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster； 重新计算k个cluser对应的质心；

until 质心不再发生变化

k-means存在缺点：

1）k-means是局部最优的，容易受到初始质心的影响；比如在下图中，因选择初始质心不恰当而造成次优的聚类结果。

2）同时，k值的选取也会直接影响聚类结果，最优聚类的k值应与样本数据本身的结构信息相吻合，而这种结构信息是很难去掌握，因此选取最优k值是非常困难的。

K值得确定：
法1：(轮廓系数)在实际应用中，由于Kmean一般作为数据预处理，或者用于辅助分聚类贴标签。所以k一般不会设置很大。可以通过枚举，令k从2到一个固定值如10，在每个k值上重复运行数次kmeans(避免局部最优解)，并计算当前k的平均轮廓系数，最后选取轮廓系数最大的值对应的k作为最终的集群数目。

45. Kmeans
基本K-Means算法的思想很简单，事先确定常数K，常数K意味着最终的聚类类别数，首先随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，接着，重新计算每个类的质心(即为类中心)，重复这样的过程，知道质心不再改变，最终就确定了每个样本所属的类别以及每个类的质心。由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。
初始化常数K，随机选取初始点为质心

重复计算一下过程，直到质心不再改变

计算样本与每个质心之间的相似度，将样本归类到最相似的类中

重新计算质心

输出最终的质心以及每个类

46. DBSCAN原理和算法伪代码，与kmeans，OPTICS区别?
DBSCAN聚类算法原理
DBSCAN（Density-Based Spatial Clustering of Applications with Noise）聚类算法，它是一种基于高密度连通区域的、基于密度的聚类算法，能够将具有足够高密度的区域划分为簇，并在具有噪声的数据中发现任意形状的簇。我们总结一下DBSCAN聚类算法原理的基本要点：

DBSCAN算法需要选择一种距离度量，对于待聚类的数据集中，任意两个点之间的距离，反映了点之间的密度，说明了点与点是否能够聚到同一类中。由于DBSCAN算法对高维数据定义密度很困难，所以对于二维空间中的点，可以使用欧几里德距离来进行度量。

DBSCAN算法需要用户输入2个参数：一个参数是半径（Eps），表示以给定点P为中心的圆形邻域的范围；另一个参数是以点P为中心的邻域内最少点的数量（MinPts）。如果满足：以点P为中心、半径为Eps的邻域内的点的个数不少于MinPts，则称点P为核心点。

DBSCAN聚类使用到一个k-距离的概念，k-距离是指：给定数据集P={p(i); i=0,1,…n}，对于任意点P(i)，计算点P(i)到集合D的子集S={p(1), p(2), …, p(i-1), p(i+1), …, p(n)}中所有点之间的距离，距离按照从小到大的顺序排序，假设排序后的距离集合为D={d(1), d(2), …, d(k-1), d(k), d(k+1), …,d(n)}，则d(k)就被称为k-距离。也就是说，k-距离是点p(i)到所有点（除了p(i)点）之间距离第k近的距离。对待聚类集合中每个点p(i)都计算k-距离，最后得到所有点的k-距离集合E={e(1), e(2), …, e(n)}。

根据经验计算半径Eps：根据得到的所有点的k-距离集合E，对集合E进行升序排序后得到k-距离集合E’，需要拟合一条排序后的E’集合中k-距离的变化曲线图，然后绘出曲线，通过观察，将急剧发生变化的位置所对应的k-距离的值，确定为半径Eps的值。

根据经验计算最少点的数量MinPts：确定MinPts的大小，实际上也是确定k-距离中k的值，DBSCAN算法取k=4，则MinPts=4。

另外，如果觉得经验值聚类的结果不满意，可以适当调整Eps和MinPts的值，经过多次迭代计算对比，选择最合适的参数值。可以看出，如果MinPts不变，Eps取得值过大，会导致大多数点都聚到同一个簇中，Eps过小，会导致已一个簇的分裂；如果Eps不变，MinPts的值取得过大，会导致同一个簇中点被标记为噪声点，MinPts过小，会导致发现大量的核心点。

我们需要知道的是，DBSCAN算法，需要输入2个参数，这两个参数的计算都来自经验知识。半径Eps的计算依赖于计算k-距离，DBSCAN取k=4，也就是设置MinPts=4，然后需要根据k-距离曲线，根据经验观察找到合适的半径Eps的值，下面的算法实现过程中，我们会详细说明。对于算法的实现，首先我们概要地描述一下实现的过程：

1）解析样本数据文件。2）计算每个点与其他所有点之间的欧几里德距离。3）计算每个点的k-距离值，并对所有点的k-距离集合进行升序排序，输出的排序后的k-距离值。4）将所有点的k-距离值，在Excel中用散点图显示k-距离变化趋势。5）根据散点图确定半径Eps的值。）根据给定MinPts=4，以及半径Eps的值，计算所有核心点，并建立核心点与到核心点距离小于半径Eps的点的映射。7）根据得到的核心点集合，以及半径Eps的值，计算能够连通的核心点，得到噪声点。8）将能够连通的每一组核心点，以及到核心点距离小于半径Eps的点，都放到一起，形成一个簇。9）选择不同的半径Eps，使用DBSCAN算法聚类得到的一组簇及其噪声点，使用散点图对比聚类效果。

算法伪代码：

算法描述：

算法：DBSCAN

输入：E——半径

MinPts——给定点在E邻域内成为核心对象的最小邻域点数。

D——集合。

输出：目标类簇集合

方法：Repeat

1）判断输入点是否为核心对象

2）找出核心对象的E邻域中的所有直接密度可达点。

Until 所有输入点都判断完毕

Repeat

针对所有核心对象的E邻域内所有直接密度可达点找到最大密度相连对象集合，中间涉及到一些密度可达对象的合并。Until 所有核心对象的E领域都遍历完毕

DBSCAN和Kmeans的区别：

1)K均值和DBSCAN都是将每个对象指派到单个簇的划分聚类算法，但是K均值一般聚类所有对象，而DBSCAN丢弃被它识别为噪声的对象。

2)K均值使用簇的基于原型的概念，而DBSCAN使用基于密度的概念。

3)K均值很难处理非球形的簇和不同大小的簇。DBSCAN可以处理不同大小或形状的簇，并且不太受噪声和离群点的影响。当簇具有很不相同的密度时，两种算法的性能都很差。

4)K均值只能用于具有明确定义的质心（比如均值或中位数）的数据。DBSCAN要求密度定义（基于传统的欧几里得密度概念）对于数据是有意义的。

5)K均值可以用于稀疏的高维数据，如文档数据。DBSCAN通常在这类数据上的性能很差，因为对于高维数据，传统的欧几里得密度定义不能很好处理它们。

6)K均值和DBSCAN的最初版本都是针对欧几里得数据设计的，但是它们都被扩展，以便处理其他类型的数据。

7)基本K均值算法等价于一种统计聚类方法（混合模型），假定所有的簇都来自球形高斯分布，具有不同的均值，但具有相同的协方差矩阵。DBSCAN不对数据的分布做任何假定。

8)K均值DBSCAN和都寻找使用所有属性的簇，即它们都不寻找可能只涉及某个属性子集的簇。

9)K均值可以发现不是明显分离的簇，即便簇有重叠也可以发现，但是DBSCAN会合并有重叠的簇。

10)K均值算法的时间复杂度是O(m)，而DBSCAN的时间复杂度是O(m^2)，除非用于诸如低维欧几里得数据这样的特殊情况。

11)DBSCAN多次运行产生相同的结果，而K均值通常使用随机初始化质心，不会产生相同的结果。

12)DBSCAN自动地确定簇个数，对于K均值，簇个数需要作为参数指定。然而，DBSCAN必须指定另外两个参数：Eps（邻域半径）和MinPts（最少点数）。

13)K均值聚类可以看作优化问题，即最小化每个点到最近质心的误差平方和，并且可以看作一种统计聚类（混合模型）的特例。DBSCAN不基于任何形式化模型。

DBSCAN与OPTICS的区别：

DBSCAN算法，有两个初始参数E（邻域半径）和minPts(E邻域最小点数)需要用户手动设置输入，并且聚类的类簇结果对这两个参数的取值非常敏感，不同的取值将产生不同的聚类结果，其实这也是大多数其他需要初始化参数聚类算法的弊端。

为了克服DBSCAN算法这一缺点，提出了OPTICS算法（Ordering Points to identify the clustering structure）。OPTICS并 不显示的产生结果类簇，而是为聚类分析生成一个增广的簇排序（比如，以可达距离为纵轴，样本点输出次序为横轴的坐标图），这个排序代表了各样本点基于密度 的聚类结构。它包含的信息等价于从一个广泛的参数设置所获得的基于密度的聚类，换句话说，从这个排序中可以得到基于任何参数E和minPts的DBSCAN算法的聚类结果。

OPTICS两个概念：

核心距离：对象p的核心距离是指是p成为核心对象的最小E’。如果p不是核心对象，那么p的核心距离没有任何意义。

可达距离：对象q到对象p的可达距离是指p的核心距离和p与q之间欧几里得距离之间的较大值。如果p不是核心对象，p和q之间的可达距离没有意义。

算法描述：OPTICS算法额外存储了每个对象的核心距离和可达距离。基于OPTICS产生的排序信息来提取类簇。

### 推荐系统算法
47. 请你说一说推荐算法，fm，lr，embedding
推荐算法：
基于人口学的推荐、基于内容的推荐、基于用户的协同过滤推荐、基于项目的协同过滤推荐、基于模型的协同过滤推荐、基于关联规则的推荐
FM：
\[y(x) = {w_0} + \sum\limits_{i = 1}^n {{w_i}{x_i}}  + \sum\limits_{i = 1}^n {\sum\limits_{j = i + 1}^n {({v_i},{v_j}){x_i}{x_j}} } \]

LR：

逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层逻辑函数g(z)，即先把特征线性求和，然后使用函数g(z)作为假设函数来预测。g(z)可以将连续值映射到0 和1。
Embedding：

Embedding在数学上表示一个maping：$f:X-->Y$，也就是一个function。其中该函数满足两个性质：1）injective （单射的）：就是我们所说的单射函数，每个Y只有唯一的X对应;2）structure-preserving（结构保存）：比如在X所属的空间上，那么映射后在Y所属空间上同理$y_1<=y_2$。

那么对于word embedding,就是找到一个映射(函数)将单词(word)映射到另外一个空间(其中这个映射具有injective和structure-preserving的特点),生成在一个新的空间上的表达，该表达就是word representation。

48. 协同过滤的itemCF，userCF区别适用场景?
Item CF 和 User CF两个方法都能很好的给出推荐，并可以达到不错的效果。但是他们之间还是有不同之处的，而且适用性也有区别。下面进行一下对比
计算复杂度：

Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。

适用场景：

在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。

相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。

49. 推荐系统的大概步骤，解决冷启动?
步骤：1）收集用户的所有信息。2）使用大数据计算平台对收集的信息进行处理，的到用户偏好数据。3）将偏好数据导入喜好类型计算算法中进行预算计算，的到预算结果。4）将推荐的结果导入数据库（redis、hbase）。5）发开一个推荐引擎，对外开放接口，输出推荐结果。
解决冷启动的方案：

1）提供非个性化的推荐

最简单的例子就是提供热门排行榜，可以给用户推荐热门排行榜，等到用户数据收集到一定的时候，再切换为个性化推荐。例如Netflix的研究也表明新用户在冷启动阶段确实是更倾向于热门排行榜的，老用户会更加需要长尾推荐

2）利用用户注册信息

用户的注册信息主要分为3种：（1）获取用户的注册信息；（2）根据用户的注册信息对用户分类；（3）给用户推荐他所属分类中用户喜欢的物品。

3）选择合适的物品启动用户的兴趣

用户在登录时对一些物品进行反馈，收集用户对这些物品的兴趣信息，然后给用户推荐那些和这些物品相似的物品。一般来说，能够用来启动用户兴趣的物品需要具有以下特点：

比较热门，如果要让用户对物品进行反馈，前提是用户得知道这是什么东西；

具有代表性和区分性，启动用户兴趣的物品不能是大众化或老少咸宜的，因为这样的物品对用户的兴趣没有区分性；

启动物品集合需要有多样性，在冷启动时，我们不知道用户的兴趣，而用户兴趣的可能性非常多，为了匹配多样的兴趣，我们需要提供具有很高覆盖率的启动物品集合，这些物品能覆盖几乎所有主流的用户兴趣

4）利用物品的内容信息

用来解决物品的冷启动问题，即如何将新加入的物品推荐给对它感兴趣的用户。物品冷启动问题在新闻网站等时效性很强的网站中非常重要，因为这些网站时时刻刻都有新物品加入，而且每个物品必须能够再第一时间展现给用户，否则经过一段时间后，物品的价值就大大降低了。

5）采用专家标注

很多系统在建立的时候，既没有用户的行为数据，也没有充足的物品内容信息来计算物品相似度。这种情况下，很多系统都利用专家进行标注。

6）利用用户在其他地方已经沉淀的数据进行冷启动

以QQ音乐举例：QQ音乐的猜你喜欢电台想要去猜测第一次使用QQ音乐的用户的口味偏好，一大优势是可以利用其它腾讯平台的数据，比如在QQ空间关注了谁，在腾讯微博关注了谁，更进一步，比如在腾讯视频刚刚看了一部动漫，那么如果QQ音乐推荐了这部动漫里的歌曲，用户会觉得很人性化。这就是利用用户在其它平台已有的数据。

再比如今日头条：它是在用户通过新浪微博等社交网站登录之后，获取用户的关注列表，并且爬取用户最近参与互动的feed（转发/评论等），对其进行语义分析，从而获取用户的偏好。

所以这种方法的前提是，引导用户通过社交网络账号登录，这样一方面可以降低注册成本提高转化率；另一方面可以获取用户的社交网络信息，解决冷启动问题。

7）利用用户的手机等兴趣偏好进行冷启动

Android手机开放的比较高，所以在安装自己的app时，就可以顺路了解下手机上还安装了什么其他的app。比如一个用户安装了美丽说、蘑菇街、辣妈帮、大姨妈等应用，就可以判定这是女性了，更进一步还可以判定是备孕还是少女。目前读取用户安装的应用这部分功能除了app应用商店之外，一些新闻类、视频类的应用也在做，对于解决冷启动问题有很好的帮助。

50. 用mapreduce实现10亿级以上数据的kmeans?
算法1.map(key,value)
输入：全局变量centers，偏移量key，样本value

输出：<key’,value>对，其中key’是最近中心的索引，value’是样本信息的字符串

从value构造样本的instance；
```
minDis=Double.MAX_VALUE；
Index=-1；
For i=0 to centers.length do
dis=ComputeDist(instance,centers[i]);
If dis<minDis{
minDis=dis;
index=i;
}
End For
```

把index作为key’；

把不同维度的values构造成value’；

输出<key’,value’>对；

End

注意这里的Step 2和Step 3初始化了辅助变量minDis和index；Step 4通过计算找出了与样本最近的中心点，函数ComputeDist(instance,centers[i])返回样本和中心点centers[i]的距离；Step 8输出了用来进行下一个过程（combiner）的中间数据。

Combine函数. 每个map任务完成之后，我们用combiner去合并同一个map任务的中间结果。因为中间结果是存储在结点的本地磁盘上，所以这个过程不会耗费网络传输的代价。在combine函数中，我们把属于相同簇的values求和。为了计算每个簇的对象的平均值，我们需要记录每个map的每个簇中样本的总数。Combine函数的伪代码见算法2.

算法2.combine(key,V)

输入：key为簇的索引，V为属于该簇的样本列表

输出：<key’,value’>对，key’为簇的索引，value’是由属于同一类的所有样本总和以及样本数所组成的字符串。

初始化一个数组，用来记录同一类的所有样本的每个维度的总和，样本是V中的元素；

初始化一个计数器num为0来记录属于同一类的样本总数；

While(V.hasNext()){

从V.next()构造样本实例instance；

把instance的不同维度值相加到数组

num++;

}

把key作为key’；

构造value’：不同维度的求和结果+num；

输出<key’,value’>对；

End

Reduce函数. Reduce函数的输入数据由每个结点的combine函数获得。如combine函数所描述，输入数据包括部分样本（同一类）的求和以及对应样本数。在reduce函数中，我们可以把同一类的所有样本求和并且计算出对应的样本数。因此，我们可以得到用于下一轮迭代的新中心。Reduce函数的伪代码见算法3。

算法3.Reduce(key,V)

输入：key为簇的索引，V为来自不同结点的部分总和的样本列表

输出：<key’,value’>对，key’为簇的索引，value’是代表新的聚类中心的字符串

初始化一个数组，用来记录同一类的所有样本的每个维度的总和，样本是V中的元素；

初始化一个计数器NUM为0来记录属于同一类的样本总数；

While(V.hasNext()){

从V.next()构造样本实例instance；

把instance的不同维度值相加到数组

NUM+=num;

}

数组的每个元素除以NUM来获得新的中心坐标；

把key作为key’；

构造value’为所有中心坐标的字符串；

输出<key’,value’>对；

End

### 模型融合
51. bagging和boosting的区别？
Bagging是从训练集中进行子抽样组成每个基模型所需要的子训练集,然后对所有基模型预测的结果进行综合操作产生最终的预测结果。
Boosting中基模型按次序进行训练,而基模型的训练集按照某种策略每次都进行一定的转化,最后以一定的方式将基分类器组合成一个强分类器。

Bagging的训练集是在原始集中有放回的选取,而Boosting每轮的训练集不变,只是训练集中的每个样本在分类器中的权重都会发生变化,此权值会根据上一轮的结果进行调整。

Bagging的各个预测函数可以并行生成,Boosting的各预测函数只能顺序生成。

Bagging中整体模型的期望近似于基模型的期望,所以整体模型的偏差相似于基模型的偏差,因此Bagging中的基模型为强模型(强模型拥有低偏差高方差)。

Boosting中的基模型为弱模型,若不是弱模型会导致整体模型的方差很大。
- 
Bagging和Boosting的区别：
1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：Bagging：使用均匀取样，每个样例的权重相等。Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：Bagging：所有预测函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：Bagging：各个预测函数可以并行生成。Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

52. XGBOOST和GDBT的区别
GDBT在函数空间中利用梯度下降法进行优化而XGB在函数空间中使用了牛顿法进行优化。即GDBT在优化中使用了一阶导数信息,而XGB对损失函数进行了二阶泰勒展开,用到了一阶和二阶倒数信息。XGB在损失函数中加入了正则项(树叶子节点个数,每个叶子节点上输出score的L2模平方和。对于缺失的样本,XGB可以自动学习出它的分裂方向。GDBT的节点分裂方式使用的是gini系数,XGB通过优化推导出分裂前后的增益来选择分裂节点。XGB在处理每个特征列时可以做到并行。

53. GDBT的原理,以及常用的调参参数？
先用一个初始值去学习一棵树,然后在叶子处得到预测值以及预测后的残差,之后的树则基于之前树的残差不断的拟合得到,从而训练出一系列的树作为模型。
n_estimators基学习器的最大迭代次数,learning_rate学习率，max_lead_nodes最大叶子节点数,max_depth树的最大深度,min_samples_leaf叶子节点上最少样本数。

54. stacking和blending的区别?？
Stacking和blending的区别在于数据的划分,blending用不相交的数据训练不同的基模型,并将其输出取加权平均。而stacking是将数据集划分为两个不相交的集合,在第一个集合的数据集中训练多个模型,在第二个数据集中测试这些模型,将预测结果作为输入,将正确的标签作为输出,再训练一个高层的模型

55. AdaBoost和GBDT的区别,AdaBoost和GBDT的区别

AdaBoost通过调整错分的数据点的权重来改进模型,而GBDT是从负梯度的方向去拟合改进模型。
AdaBoost改变了训练数据的权值,即样本的概率分布,减少上一轮被正确分类的样本权值,提高被错误分类的样本权值,而随机森林在训练每棵树的时候,随机挑选部分训练集进行训练。在对新数据进行预测时,AdaBoost中所有树加权投票进行预测,每棵树的权重和错误率有关,而随机森林对所有树的结果按照少数服从多数的原则进行预测。

56. boosting和bagging在不同情况下的选用？
Bagging与Boosting的区别：
1）取样方式（样本权重）：Bagging是均匀选取，样本的权重相等，Boosting根据错误率取样，错误率越大则权重越大。2）训练集的选择：Bagging随机选择训练集，训练集之间相互独立，Boosting的各轮训练集的选择与前面各轮的学习结果有关。3）预测函数：Bagging各个预测函数没有权重，可以并行生成，Boosting有权重，顺序生成。4）Bagging是减少variance，Boosting是减少bias。

Bagging 是 Bootstrap Aggregating的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的 variance. Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。

Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不不断进行行，误差会越来越小，所以模型的 bias 会不不断降低。这种算法无法并行。

57. gbdt推导和适用场景?
...
适用场景：GBDT几乎可用于所有回归问题（线性/非线性），GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。

58. rf和gbdt基分类器区别，里面的决策树分别长啥样，怎么剪枝
GBDT和RF都是集成方法中的经典模型，我们需要弄清楚下面几个问题：1）GBDT是采用boosing方法，RF采用的是baggging方法；2）bias和variance是解释模型泛化性能的，其实还有噪声。
然后，理解GBDT和RF执行原理，其中GBDT中的核心是通过用分类器（如CART、RF）拟合损失函数梯度，而损失函数的定义就决定了在子区域内各个步长，其中就是期望输出与分类器预测输出的查，即bias；而RF的核心就是自采样（样本随机）和属性随机（所有样本中随机选择K个子样本选择最优属性来划分），样本数相同下的不同训练集产生的各个分类器，即数据的扰动导致模型学习性能的变化，即variance。

Gradient boosting Decision Tree(GBDT)

GB算法中最典型的基学习器是决策树，尤其是CART，正如名字的含义，GBDT是GB和DT的结合。要注意的是这里的决策树是回归树，GBDT中的决策树是个弱模型，深度较小一般不会超过5，叶子节点的数量也不会超过10，对于生成的每棵决策树乘上比较小的缩减系数（学习率<0.1），有些GBDT的实现加入了随机抽样（subsample 0.5<=f <=0.8）提高模型的泛化能力。通过交叉验证的方法选择最优的参数。

Random Forest：

bagging （你懂得，原本叫Bootstrap aggregating），bagging 的关键是重复的对经过bootstrapped采样来的观测集子集进行拟合。然后求平均。。。一个bagged tree充分利用近2/3的样本集。。。所以就有了OOB预估(outof bag estimation)

GBDT和随机森林的相同点：

1）都是由多棵树组成；2）最终的结果都是由多棵树一起决定

GBDT和随机森林的不同点：

1）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成；

2）组成随机森林的树可以并行生成；而GBDT只能是串行生成；

3）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来；

4）随机森林对异常值不敏感，GBDT对异常值非常敏感；

5）随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成；

6）随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能。

RF决策树：

学习随机森林模型前，一定要先了解决策树模型。树越深，模型越复杂。

决策树模型的优点如下。1）容易理解和解释，树可以被可视化。2）不需要太多的数据预处理工作，即不需要进行数据归一化，创造哑变量等操作。3）隐含地创造了多个联合特征，并能够解决非线性问题。

GBDT决策树：

迭代决策树GBDT（Gradient Boosting Decision Tree）也被称为是MART（Multiple Additive Regression Tree)）或者是GBRT（Gradient Boosting Regression Tree），也是一种基于集成思想的决策树模型，但是它和Random Forest有着本质上的区别。不得不提的是，GBDT是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，更难能可贵的是，GBDT有着出众的准确率。

树的剪枝：

（1）前剪枝( Pre-Pruning)

通过提前停止树的构造来对决策树进行剪枝，一旦停止该节点下树的继续构造，该节点就成了叶节点。一般树的前剪枝原则有：a.节点达到完全纯度；b.树的深度达到用户所要的深度；c.节点中样本个数少于用户指定个数；d.不纯度指标下降的最大幅度小于用户指定的幅度。

（2）后剪枝( Post-Pruning)

首先构造完整的决策树，允许决策树过度拟合训练数据，然后对那些置信度不够的结点的子树用叶结点来替代。CART 采用Cost-Complexity Pruning（代价-复杂度剪枝法），代价(cost) ：主要指样本错分率；复杂度(complexity) ：主要指树t的叶节点数，(Breiman…)定义树t的代价复杂度(cost-complexity):信息熵H(X)，信息增益=H(D)-H(Y|X)，信息增益率=gain(x)/H(x)，Gini系数=1-sum（pk^2），基尼系数就是熵在x=1的地方一阶泰勒展开得到f(x)=1-x，所以gini=sum[x(1-x)]=1-sum(x^2)。

59. 随机森林和 GBDT 的区别

1）随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。4）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。5）随机森林对异常值不敏感；GBDT对异常值非常敏感。6）随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。7）随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。
60. xgboost的特征重要性计算

Xgboost根据结构分数的增益情况计算出来选择哪个特征作为分割点,而某个特征的重要性就是它在所有树中出现的次数之和。
61. xgboost的正则项表达式
参考回答：
 T为叶子节点的个数,w为叶子节点的分数
62. xgboost原理，怎么防过拟合
参考回答：
XGBoost是一个树集成模型，它使用的是K（树的总数为K）个树的每棵树对样本的预测值的和作为该样本在XGBoost系统中的预测，定义函数如下：


对于所给的数据集有n个样本，m个特征，定义为：



其中Xi表示第i个样本，yi表示第i个样本的类别标签。CART树的空间为F，如下：



其中q表示每棵树的结构映射每个样本到相应的叶节点的分数，即q表示树的模型，输入一个样本，根据模型将样本映射到叶节点输出预测的分数；Wq(x)表示树q的所有叶节点的分数组成集合；T是树q的叶节点数量。

所以，由（1）式可以看出，XGBoost的预测值为每棵树的预测值之和，即每棵树相应的叶节点的得分之和（Wi的和，Wi表示第i个叶节点的得分）。

我们的目标就是学习这样的K个树模型f(x).。为了学习模型f(x)，我们定义下面的目标函数：



其中，（2）式右边第一项为损失函数项，即训练误差，是一个可微的凸函数（比如用于回归的均方误差和用于分类的Logistic误差函数等），第二项为正则化项，即每棵树的复杂度之和，目的是控制模型的复杂度，防止过拟合。我们的目标是在L(φ)取得最小化时得出对应的模型f(x)。

由于XGBoost模型中的优化参数是模型f(x)，不是一个具体的值，所以不能用传统的优化方法在欧式空间中进行优化，而是采用additive training的方式去学习模型。每一次保留原来的模型不变，加入一个新的函数f到模型中，如下：


预测值在每一次迭代中加入一个新的函数f目的是使目标函数尽量最大地降低。

因为我们的目标是最小化L(φ)时得到模型f(x)，但是L(φ)中并没有参数f(x)，所以，我们将上图中的最后一式代入L(φ)中可得到如下式子：



对于平方误差（用于回归）来说（3）式转换成如下形式：



对于不是平方误差的情况下，一般会采用泰勒展开式来定义一个近似的目标函数，以方便我们的进一步计算。

根据如下的泰勒展开式，移除高阶无穷小项，得：





（3）式等价于下面的式子：



由于我们的目标是求L(φ)最小化时的模型f(x)（也是变量），当移除常数项时模型的最小值变化，但是取最小值的变量不变（比如：y=x^2+C，无论C去何值，x都在0处取最小值）。所以，为了简化计算，我们移除常数项，得到如下的目标函数：



定义 为叶节点j的实例，重写（4）式，将关于树模型的迭代转换为关于树的叶子节点的迭代，得到如下过程：


此时我们的目标是求每棵树的叶节点j的分数Wj，求出Wj后，将每棵树的Wj相加，即可得到最终的预测的分数。而要想得到最优的Wj的值，即最小化我们的目标函数，所以上式对Wj求偏导，并令偏导数为0，算出此时的W*j为：





将W*j代入原式得：



方程（5）可以用作得分(score)函数来测量树结构q的质量。该得分类似于评估决策树的不纯度得分，除了它是针对更广泛的目标函数得出的。

在xgboost调中，一般有两种方式用于控制过拟合：1）直接控制参数的复杂度：包括max_depth min_child_weight gamma；2）add randomness来使得对训练对噪声鲁棒。包括subsample colsample_bytree，或者也可以减小步长 eta，但是需要增加num_round，来平衡步长因子的减小。

63. xgboost，rf，lr优缺点场景。。。
参考回答：
Xgboost：
优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

适用场景：分类回归问题都可以。

Rf：

优点：1）表现性能好，与其他算法相比有着很大优势。2）随机森林能处理很高维度的数据（也就是很多特征的数据），并且不用做特征选择。3）在训练完之后，随机森林能给出哪些特征比较重要。4）训练速度快，容易做成并行化方法(训练时，树与树之间是相互独立的)。5）在训练过程中，能够检测到feature之间的影响。6）对于不平衡数据集来说，随机森林可以平衡误差。当存在分类不平衡的情况时，随机森林能提供平衡数据集误差的有效方法。7）如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。8）随机森林算法有很强的抗干扰能力（具体体现在6,7点）。所以当数据存在大量的数据缺失，用RF也是不错的。9）随机森林抗过拟合能力比较强（虽然理论上说随机森林不会产生过拟合现象，但是在现实中噪声是不能忽略的，增加树虽然能够减小过拟合，但没有办法完全消除过拟合，无论怎么增加树都不行，再说树的数目也不可能无限增加的）。10）随机森林能够解决分类与回归两种类型的问题，并在这两方面都有相当好的估计表现。（虽然RF能做回归问题，但通常都用RF来解决分类问题）。11）在创建随机森林时候，对generlization error(泛化误差)使用的是无偏估计模型，泛化能力强。

缺点：1）随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。2）对于许多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试。3）可能有很多相似的决策树，掩盖了真实的结果。4）对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。5）执行数据虽然比boosting等快（随机森林属于bagging），但比单只决策树慢多了。

适用场景：数据维度相对低（几十维），同时对准确性有较高要求时。因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

Lr：

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

64. xgboost特征并行化怎么做的
参考回答：
决策树的学习最耗时的一个步骤就是对特征值进行排序,在进行节点分裂时需要计算每个特征的增益,最终选增益大的特征做分裂,各个特征的增益计算就可开启多线程进行。而且可以采用并行化的近似直方图算法进行节点分裂。
65. xgboost和lightgbm的区别和适用场景

（1）xgboost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略，区别是xgboost对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了务必要的开销。 leaft-wise的做法是在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。
（2）lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法，histogram算法在内存和计算代价上都有不小优势。1）内存上优势：很明显，直方图算法的内存消耗为(#data* #features * 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 * #data * #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。2）计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)

（3）直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

（4）lightgbm支持直接输入categorical 的feature，在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。

（5）xgboost在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。

其适用场景根据实际项目和两种算法的优点进行选择。

66. HMM隐马尔可夫模型的参数估计方法是？
EM算法
解析：期望最大化（Expectation-Maximum,EM）算法

67. Bootstrap方法是什么？
从一个数据集中有放回的抽取N次，每次抽M个。
解析：Bagging算法基于bootstrap。面试时结合Bagging算法讲述会更好。

68. 问题：如何防止过拟合？

1.早停法；2.l1和l2正则化；3.神经网络的dropout；4.决策树剪枝；5.SVM的松弛变量；6.集成学习
解析：能够达到模型权重减小，模型简单的效果



















---
## 深度学习算法

---
## 编程语言工具和环境

---
## 自然语言处理

--- 
## 计算机基础



## 笔试
### 大疆DJI2019届秋季招聘笔试：
机器学习算法工程师2018年07月09日 链接：https://www.nowcoder.com/discuss/85562
本篇博客写的是关于大疆机器学习岗位的笔试题B卷（分两次考试：A卷和B卷），对其他岗位不具有参考性。今年的大疆笔试题题量大（单选，多选，填空，问答，编程），时间却很短（1个小时），可以说是时间短，题目多。好多人都没有做完，包括我。不说了，趁着刚做完，把题目做一个记录，题目来自于一些同学的拍照和自己的记忆。题目单选题
1. 关于Logistic回归和SVM，以下说法错误的是？
Logistic回归可用于预测事件发生概率的大小
Logistic回归的目标函数是最小化后验概率  <font face="黑体" color=green size=4>(1)</font>
SVM的目标的结构风险最小化
SVM可以有效避免模型过拟合
2. 假设三个稠密矩阵（Dense Matrix）A, B, C的尺寸分别为m*n, n*p和p*q，且$m<n<p<q$，计算速度最快的是？
(AB)C <font face="黑体" color=green size=4>(1)</font>
AC(B)
A(BC)
所有效率都相同
3. 以下有关特征数据归一化的说法错误的是：
特征数据归一化加速梯度下降优化的速度
特征数据归一化有可能提高模型的精度
线性归一化适用于特征数值分化比较大的情况  <font face="黑体" color=green size=4>(1)</font>
概率模型不需要做归一化处理
4. 假定你在神经网络中的隐藏层中使用激活函数X，在特定神经元给定任意输入，你会得到输出[-0.0001]，X可能是一下哪一个？
ReLU
tanh  <font face="黑体" color=green size=4>(1)</font>
sigmoid
其他都不是
5. 下列哪些项所描述的相关技术是对的？
AdaGrad和L-BFGS使用的都是一阶差分
AdaGrad和L-BFGS使用的都是二阶差分
Adagrad使用的是一阶差分，L-BFGS使用的是二阶差分 <font face="黑体" color=green size=4>(1)</font>
Adagrad使用的是二阶差分，L-BFGS使用的是一阶差分
多选题
1. 关于主成分分析PCA说法正确的是：
我们必须在使用PCA前规范化数据   <font face="黑体" color=green size=4>(1)</font>
我们应该选择使得模型有最大variance的主成分  <font face="黑体" color=green size=4>(1)</font>
我们应该选择使得模型有最小variance的主成分
我们可以使用PCA在低维度上做数据可视化   <font face="黑体" color=green size=4>(1)</font>
2. 以下描述错误的是？
SVM是这样一个分类器，他寻找具有最小边缘的超平面，因此它也经常被称为最小边缘分类器（minimal margin classifier）  <font face="黑体" color=green size=4>(1)</font>
在聚类分析中，簇内的相似性越大，簇间的差别越大，聚类的效果越好
在决策树中，随着树中节点变得太大，即使模型的训练误差还在继续减低，但是检验误差开始增大，这是出现了模型拟合不足的问题  <font face="黑体" color=green size=4>(1)</font>
聚类分析可以看做是一种非监督的分类
3. 假设目标遍历的类别非常不平衡，即主要类别占据了训练数据的99%，现在你的模型在训练集上表现为99%的准确度，那么下面说法正确的是：
准确度并不适合衡量不平衡类别问题  <font face="黑体" color=green size=4>(1)</font>
准确度适合衡量不平衡类别问题
精确度和召回率适合于衡量不平衡类别问题  <font face="黑体" color=green size=4>(1)</font>
精确度和召回率不适合衡量不平衡类别问题
4. 神经网络训练过程中的哪些现象表明可能出现了梯度爆炸？
模型梯度快速变大  从<font face="黑体" color=green size=4>(1)</font>
模型权重变为NaN值   从<font face="黑体" color=green size=4>(1)</font>
每个节点和层的误差梯度值持续超过1.0
损失函数持续减小
应该还有个第五题，不过没有资料了。

填空题
1. 输入图片大小是200*200，依次经过一层卷积（kernel size 5*5,padding 1,stride 2),pooling(kernel size 3*3，padding 0，stride 1),又经过一层卷积（kernel size 3*3，padding 1,stride 1)，输出特征图大小为( )?

2. 在训练集标签为[0,0,0,1,1,1,1,1], 求信息熵的大小。

简答题


***
# 计算机网络

1. OSI参考模型
^应用层：所有能产生网络流量的程序
^表示层：在传输之前是否进行加密或压缩处理二进制 (ASCII)
^会话层：查木马 netstat -n
^传输层：可靠传输、流量传输、不可靠传输
^网络层：负责选择最佳路径、规划IP地址
^数据链路层：帧的开始和结束、透明传输、差错校验
^物理层：接口标准、电器标准、如何在物理层链路上传输更快的速度
OSI参考模型对网络排错指导：
1. 物理层故障：查看链接状态、发送和接收数据包
2. 数据链路层故障：MAC地址冲突、ADSL欠费、网速没有办法协商
3. 网络层故障：没配网关、配置了错误的IP地址、子网掩码、配置错误的网关、路由器没有配置到达目标网络的路由
4. 应用层故障：应用程序配置错误
OSI参考模型和网络安全：
1. 物理层安全
2. 数据链路层安全： ASDL、账号密码、数据链路层安全、VLAN交换机端口绑定MAC地址
3. 网络层安全：在路由器上使用ACL控制数据包流量
4. 应用层安全：开发层应用程序没有漏洞

2. TCP/IP协议栈







***

# 学术研究方法

**导师放羊、实验室散养是非常常见的现象**，大家大可不必慌张。综合来说，导师的不坑爹指数排名如下：年轻有为、待你平等的小老板 > 年轻有为、剥削你的小老板 > tenure、偶尔管你的中年老板 > 能力一般的小老板 > 放羊的大老板 >> 任何年龄段、没能力还瞎指挥、限制你自由的老板。如果你的老板放羊，那属于比较坑爹型，但还没有到天都塌了的地步。这种情况下，你需要如下的自救：
你首先要意识到，几乎没有完全原创的工作，最起码任何论文都是有参考文献的吧？做科研灌水当然是不好的，但是一开始就好高骛远也同样是不对的。**科研的基本功需要扎实地训练，而这种训练需要你从<font face="黑体" color=green size=4>模仿开始</font>做起。一开始的时候idea的新颖程度低一些，工作量夯实一些，是完全可以理解的**，因为你需要这样的几次完整的科研周期的训练，才能成为一名合格的研究生。
首先选择一个你感兴趣/有前途/有钱途/有人能带你的大方向。优先阅读该方向里最近五年的survey（太老的可以不看）。鉴于计算机领域的发展速度太快，只阅读survey是远远不够的。你需要自行整理该方向相关的近三年的顶级会议（一般也就三四个），<font face="黑体" color=green size=4>以关键词搜索出所有的论文</font>，然后尽可能阅读一些你能懂的/和你想做的相关的/热门的论文，增加对该领域发展现状和顶级会议论文应该有的样子的初步了解。你最好能找到可以和你一起学习的同学/可以和你讨论的其他老板。idea的诞生是需要相互启发和相互质疑的。在完全没有任何科研经验的时候，最好要找到高年级的学长学姐或者其他小老板讨论，他们可以帮你确定一个小方向。这一点我深有体会，一个完全没有经验的新手是最需要有人可以带着入门的，事实上，这也是老板不放羊的最大的好处。你需要做的，就是在前辈的带领下，快速地从一个小方向切入进去，然后慢慢地自己开始发现新天地。最开始的时候阅读论文，最好能细致一点，把论文之间的引用关系理清楚，把近几年的发展脉络理清楚。我当年开始第一个工作的时候，就是把我论文需要引用的二十多篇论文的主要思想、方法都写了下来，把引用关系画成了一个DAG图。当你入门之后，你需要有快速阅读一篇文章并掌握其核心贡献点的能力，而不要再花费很多时间来标注。如果你不会设计实验/写论文，请模仿和你的工作最相关的论文。把他们的论文好好读几遍，从结构到段落到句子都可以模仿。我当年的第一篇论文，我的老板就说我写的不错，其实我也是吃透并模仿了好几篇参考文献而已。写论文的时候切记：逻辑第一！这种逻辑是贯穿全文的，段落层面的逻辑、句子层面的逻辑、甚至一句话里的逻辑，都是非常关键的。一篇好的论文要循循善诱，有理有据，让人读起来不要废太多脑子，就觉得你说的很有道理。这里面要着重注意各种句子层面的关系：转折、因果、递进等。一句话可以有无数种表达方式，你要做的就是在脑子里把各种方式过滤一遍，选择最流畅的那一种。在研究生学术生涯中，导师不是最关键的，最关键的是你的目标、决心和努力。一名合格的研究生，应该是全栈研究生，也就是阅读参考文献，想idea，修正idea，设计实验，跑实验，写论文，修改论文，做presentation，这一整条技术栈你都要可以独立进行。如果你缺失了任何一个环节，你都会受制于你的导师。这个问题下的回答已经非常多了，大道理你也都看过，也都明白，问题是，你是否真的做到了背水一战的决心和努力？