* ```axjingWorker```

  * 文件目录

    * addons 一些插件 
    * algorithm_note 算法基础，笔试高频题
    * scripts 文件预处理脚本
    * workspace 一些总结、心得
    * DeepLearning 手撸中......
    * ComputerVision 手撸中......
    * MachineLearning 手撸中......
***
# ML --->MachineLearning
## K-NN

## 决策树

## 随机深林

## Boosting

## SVM

## 神经网络

## 贝叶斯分类器
### 贝叶斯决策论

### 极大似然估计

### 朴素贝叶斯分类器

### 贝叶斯网络

### EM算法

## 主成分分析
### PAC算法

## 概率图模型
### 隐马尔科夫模型

### 马尔科夫随机场

### 条件随机场



***
# DL --->DeepLearning
## 基于梯度的学习
## 代价函数

## 输出单元

## logistic sigmoid与双曲线函数

## 万能近似性质

## 反向传播
### 微积分中的链式法则

### 反向传播算法


## 深度学习中的正则化
### 参数范数惩罚
#### $L^2$参数正则化

#### $L^1$正则化

### Dropout

## 模型中的优化
### 梯度下降

### 动量

### Nesterov动量

## 自适应学习率算法
### AdaGrad

### RMSProp

### Adam

## 二阶近似方法
### 牛顿法

### 共轭梯度

### BFGS

## 卷积网络
### 卷积运算

### 动机

### 池化

### 基本卷积函数的变体

### 结构化输出

### 数据类型

### 高效的卷积算法

## 蒙特卡洛方法

## 对数似然梯度

## 随机最大似然和对比散度

## 深度生成模型--玻尔兹曼机

## SqueezeNet:
```广泛使用的网络模型```\
《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size》是希望降低网路的复杂度，即压缩模型、减少模型的大小，同时达到public网络的识别精度。但并不能提高网络的识别精度和速度。主要内容包括两点：（1）压缩层：利用1 x 1的卷积核，把N张输入特征图，压缩到M(N>M)张特征图，再输入下一层网络；（2）fire module网络单元。
二、网络设计思想
1、尽量用1x1的卷积核替代3x3的卷积核
尽可能使用1x1卷积核为主，因为1x1卷积核比3x3卷积核参数少了9倍。
2、引入Squeeze layer,尽量减少每一层的输入特征图数量
如对于3x3卷积层，参数的个数是(number of input channels) x (number of fiters) x (3x3).所以对于3x3的卷积核参数的减少策略有两种：filters个数（输出特征图个数），另一个方法是减少input channels个数。为此文献中引入了squeeze layers，利用1x1的卷积层，把本来是N张特征图的输入降M(M<N)张特征图，再做为输入。标准的一个卷积层：
filters shape = (3, 3, 64, 128)
3、延迟下采样操作
在Alexnet中，第一层卷积层stride=4，直接下采样了四倍，在一般的CNN中，一般的卷积层、池化层都会有下采样（stride>1）,甚至在前面几层网络的下采样比例会比较大，这样会导致最后几层的神经元的激活映射区域减少。为了提高精度可以设计下采样层延迟，延迟到后面几层进行下采样（这也是为什么本篇论文不能提高速度的原因，下采样越快后面的计算量就会大大减少）
三、fire module
根据前面的一些策略方案，文献设计了一个称之为fire module的单元，一个深度网络就是由n多个fire module串联在一起的网络，fire module如下图所示：

              图
结构解说：假设网络的输入时N张特征图
（1）首先通过1x1卷积层，把N张特征图FN压缩成M张特征图FM(M<N):
FM = fM(FN)
其中f表示卷积层的操作
（2）激活函数Relu层映射：FRELU = max(0, FM)
(3)以relu层的输出结果FRELU，分别构架1x1的卷积层，3x3的卷积层，然后把他们的输出特征图相连在一起：
Fout = [f1x1(FRELU), f3x3(FRELU)]


## U-Net:





## ShuffleNet:





## MobileNet:





## AlexNet:





## VGG:




## Inception:



## ResNet:







## FCN:






## R-CNN:




## Fast R-CNN:





## SSD:








## R-FCN:



## FPN:






## SPP:








## YOLOv1:






## YOLOv2:







## YOLOv3:






## GAN:


## 循环和递归网络（RNN）
### RNN

### LSTM




***
# CV基础 --->ComputerVision
## 灰度直方图

## 点运算&代数运算&几何运算

## 线性系统
### 卷积

### 多尺度表示

### 图像金字塔

## 傅里叶变换

## 滤波设计

## 采样数据处理

## 离散图像变换

## 小波变换

## 图像分割
### 阈值分割

### 基于梯度的分割

### 边缘检测和连接

### 区域增长

### 二值图像处理

### 分割图像的结构化

### 基于聚类的分割方法

#### 分水岭算法

#### K-means算法分割


## 图像跟踪
### 基于卡尔曼滤波器的线性动态模型跟踪

## 物体测量
### 尺寸测量

### 形状分析

### 纹理分析

### 曲线和曲面拟合

## 局部图像特征
### HOG:



### SIFT:


## GIST:






## LBP:






## Harr:

## 霍夫变换

## 动态规划

## 隐马尔科夫模型
***
# 线性代数
## 线性相关

## 生成子空间

## 范数

## 特征分解

## 奇异值分解

***
# 概率与信息论
## 随机变量

## 概率分布
### 离散型变量和概率质量函数

### 连续型变量和概率密度函数

## 边缘概率

## 条件概率

## 条件概率的链式法则

## 独立性和条件独立性

## 期望、方差和协方差

## 常用概率分布
### Bernoulli分布

### Multinoulli分布

### 高斯分布

### 指数分布

### Laplace分布

### Dirac分布和经验分布

## 常用函数的性质

## 贝叶斯规则

## 连续变量的技术细节

## 信息论

## 结构化概率模型

## 梯度优化方法
## Jacobian矩阵

## Hessian矩阵

## 最小二乘法

## 最大似然估计


***
# 医学图像处理综述
## 引言
医学图像处理的对象是各种不同成像机理的医学影像，临床广泛使用的医学成像种类主要有X-射线成像 （X-CT）、核磁共振成像（MRI）、核医学成像（NMI）和超声波成像（UI）四类。在目前的影像医疗诊断中，主要是通过观察一组二维切片图象去发现病变体，这往往需要借助医生的经验来判定。利用计算机图象处理技术对二维切片图象进行分析和处理，实现对人体器官、软组织和病变体的分割提取、三维重建和三维显示，可以辅助医生对病变体及其它感兴趣的区域进行定性甚至 定量的分析，从而大大提高医疗诊断的准确性和可靠性；在医疗教学、手术规划、手术仿真及各种医学研究中也能起重要的辅助作用[1,2]。目前，医学图像处理主要集中表现在病变检测、图像分割、图像配准及图像融合四个方面。

用深度学习方法进行数据分析呈现快速增长趋势，称为2013年的10项突破性技术之一。深度学习是人工神经网络的改进，由更多层组成，允许更高层次包含更多抽象信息来进行数据预测。迄今为止，它已成为计算机视觉领域中领先的机器学习工具，深度神经网络学习自动从原始数据（图像）获得的中级和高级抽象特征。最近的结果表明，从CNN中提取的信息在自然图像中的对目标识别和定位方面非常有效。世界各地的医学图像处理机构已经迅速进入该领域，并将CNN和其它深度学习方法应用于各种医学图像分析。

在医学成像中，疾病的准确诊断和评估取决于医学图像的采集和图像解释。近年来，图像采集已经得到了显着改善，设备以更快的速率和更高的分辨率采集数据。然而，图像解释过程，最近才开始受益于计算机技术。对医学图像的解释大多数都是由医生进行的，然而医学图像解释受到医生主观性、医生巨大差异认知和疲劳的限制。

用于图像处理的典型CNN架构由一系列卷积网络组成，其中包含有一系列数据缩减即池化层。与人脑中的低级视觉处理一样，卷积网络检测提取图像特征，例如可能表示直边的线或圆（例如器官检测）或圆圈（结肠息肉检测），然后是更高阶的特征，例如局部和全局形状和纹理特征提取[3]。CNN的输出通常是一个或多个概率或种类标签。

CNN是高度可并行化的算法。与单核的CPU处理相比，今天使用的图形处理单元（GPU）计算机芯片实现了大幅加速（大约40倍）。在医学图像处理中，GPU首先被引入用于分割和重建，然后用于机器学习。由于CNN的新变种的发展以及针对现代GPU优化的高效并行网络框架的出现，深度神经网络吸引了商业兴趣。从头开始训练深度CNN是一项挑战[4]。首先，CNN需要大量标记的训练数据，这一要求在专家注释昂贵且疾病稀缺的医学领域中可能难以满足。其次，训练深度CNN需要大量的计算和内存资源，否则训练过程将是非常耗时。第三，深度CNN训练过程中由于过度拟合和收敛问题而复杂化，这通常需要对网络的框架结构或学习参数进行重复调整，以确保所有层都以相当的速度学习[5]。鉴于这些困难，一些新的学习方案，称为“迁移学习”和“微调”，被证明可以解决上述问题从而越来越受欢迎。

***
# Tensorflow学习：

**重要：**
使用图（graph）来表示计算任务；
在被称之为会话（Session）的上下文（Context）中执行图；
使用tensor 表示数据
通过变量（Variable）维护状态；
使用feed和fatch可以为任意的操作（arbitrary operation）赋值或者从其中获取数据.
## 计算图
1. Tensorflow是一个编程系统，使用图来表示计算任务。途中的节点被称之为op(opteration的缩写).一个op获得0个或者多个Tensor，执行计算任务，产生0个或者多个Tensor.每个Tensor是一个类型化的多维数组
2. TensorFlow程序常常被组织成一个**构建阶段**和一个**执行阶段**，在构建阶段op的执行步骤被描述成一个图。在执行阶段，使用会话执行图中的op.
## 构建图
* 构建图的第一步是创建源op(source op).源op不需要任何的输入，例如常量(Constant).源op的输出被传递给其他的op做运算。

* python库中，op构造器的返回值代表被构造出来的op的输出， 这些返回值可以传递给其他的op构造器作为输入。

```python

import tensorflow as tf

# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点# 加到默认图中.## 构造器的返回值代表该常量 op 的返回值.
matrix1 = tf.constant([[3., 3.]])

# 创建另外一个常量 op, 产生一个 2x1 矩阵.
matrix2 = tf.constant([[2.],[2.]])

# 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.# 返回值 'product' 代表矩阵乘法的结果.
product = tf.matmul(matrix1, matrix2)
```
默认途现在有三个节点，两个constant() op, 和一个matmul() op. 为了真正进行矩阵相乘运算，并得到矩阵的乘法的结果，你必须在会话里启动这个图

## 在一个会话中启动图
构造阶段完成后，才能启动图。启动图的第一步是创建一个Session对象，如果无任何创建参数，会话构造器将启动默认图.
```python

# 启动默认图.
sess = tf.Session()

# 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. # 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回# 矩阵乘法 op 的输出.## 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.# # 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.## 返回值 'result' 是一个 numpy `ndarray` 对象.
result = sess.run(product)
print result
# ==> [[ 12.]]

# 任务完成, 关闭会话.
sess.close()
```

Session 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 "with" 代码块 来自动完成关闭动作.
```python
with tf.Session() as sess:
    result = sess.run([product])
    print result
```
在实现上，tensorflow将图像定义转换成分布式执行的操作，以充分利用可用的计算资源

## Tensor
TensorFlow程序使用Tensor数据结构来代表所有数据，计算图中，操作间传递的数据都是tensor.可以把TensorFlow 的tensor看做一个n维的数组或者列表。一个tensor包含一个静态的rank,和一个shape

TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.

* rank: 在TensorFlow系统中，张量的位数被描述为rank，但是张量的rank和矩阵的rank不同，并不是同一个概念.张量的rank(有时候是关于顺序、度数、n维)是张量维数的一个数量描述
如
```python
t = [[1,2,3], [4,5,6,], [7,8,9]]
```

你可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句t[i, j]来访问其中的任何元素.而对于三阶张量你可以用't[i, j, k]'来访问其中的任何元素.
* shape
Tensorflow文档中使用三种记号来方便的描绘张量的维度：rank,shape以及维数。下表表示他们之间的关系

|rank|shape|dim|
|---|---|---|
|0|[]|0-D|
|1|[D0]|1-D|
|2|[D0,D1]|1-D|
|3|[D0,D1,D]|3-D|


## 变量
Variables 变量维护了图的执行过程中的状态信息。例如：
```python

# 创建一个变量, 初始化为标量 0.
state = tf.Variable(0, name="counter")

# 创建一个 op, 其作用是使 state 增加 1

one = tf.constant(1)
new_value = tf.add(state, one)
update = tf.assign(state, new_value)

# 启动图后, 变量必须先经过`初始化` (init) op 初始化,# 首先必须增加一个`初始化` op 到图中.
init_op = tf.initialize_all_variables()

# 启动图, 运行 opwith tf.Session() as sess:
  # 运行 'init' op
  sess.run(init_op)
  # 打印 'state' 的初始值
  print sess.run(state)
  # 运行 op, 更新 'state', 并打印 'state'
  for _ in range(3):
    sess.run(update)
    print sess.run(state)

# 输出:

# 0# 1# 2# 3
```
## Fetch
为了取回操作的输出内容，可以在使用Session对象run()调用执行图时，传入一些tensor,这些tensor会帮助你取回结果。在之前的例子里，我们只取回了单个节点state,但是你也可以取回多个tensor:
```python

input1 = tf.constant(3.0)
input2 = tf.constant(2.0)
input3 = tf.constant(5.0)
intermed = tf.add(input2, input3)
mul = tf.mul(input1, intermed)

with tf.Session():
  result = sess.run([mul, intermed])
  print result

# 输出:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]
```

## Feed
* 计算图中引入了tensor,以常量或变量的形式存储。Tensorflow还提供了feed机制，改机制可以临时替代图中的任意操作中的tensor，可以对任意操作提供补丁，直接插入一个tensor.
feed使用一个tensor值临时替换一个操作的输出结果。你可以提供feed数据作为run()的调用参数。feed只在调用它的方法内有效，方法结束，feed就会消失。最常见的用例是将某些特殊的操作指定为"feed"操作，标记的方法是使用tf.placeholder()为这些操作创建占位符
```python

input1 = tf.placeholder(tf.types.float32)
input2 = tf.placeholder(tf.types.float32)
output = tf.mul(input1, input2)

with tf.Session() as sess:
  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})

# 输出:# [array([ 14.], dtype=float32)]
```
***
# 面试经验及技巧
## 数据结构
### 数组
* 数组是一种基本的数据结构，用于**按顺序**存储元素集合。但元素可以随机存取，因为数组中的每个元素都可以通过数组*索引*来识别。
* **动态数组**它仍然是一个随机存取的列表数据结构，但大小是可变的。例如，在 C++ 中的 vector，以及在 Java 中的 ArrayList。

### Hash

## 算法
### 快速排序算法
快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要Ο(n log n)次比较。在最坏状况下则需要Ο(_n_2)次比较，但这种状况并不常见。事实上，快速排序通常明显比其他Ο(n log n) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来。
快速排序使用分治法（Divide and conquer）策略来把一个串行（list）分为两个子串行（sub-lists）。\
**算法步骤：**
1 从数列中挑出一个元素，称为 “基准”（pivot），
2 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。
3 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。
递归的最底部情形，是数列的大小是零或一，也就是永远都已经被排序好了。虽然一直递归下去，但是这个算法总会退出，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。
![sort quick](https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/sort_quick_anim.gif)

### 堆排序算法
堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：**即子结点的键值或索引总是小于（或者大于）它的父节点。**
堆排序的平均时间复杂度为Ο(n_log_n) 
**算法步骤:**
1 创建一个堆H[0...n-1]
2 把堆首（最大值）和堆尾互换
3 把堆的尺寸缩小1，并调用shift_down(0),目的是把新的数组顶端数据调整到相应位置
4 重复步骤2，直到堆的尺寸为1
![Heapsort](https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/heap_sort.gif)

### 归并排序
归并排序（Merge sort，台湾译作：合并排序）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。
**算法步骤：**
1. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列
2. 设定两个指针，最初位置分别为两个已经排序序列的起始位置
3. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置
4. 重复步骤3直到某一指针达到序列尾
5. 将另一序列剩下的所有元素直接复制到合并序列尾
![Merge sort](https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/merge_sort.gif)

### 二分查找算法
二分查找算法是一种在有序数组中查找某一特定元素的搜索算法。搜素过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜素过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。折半搜索每次把搜索区域减少一半，时间复杂度为Ο(log_n_) 。

### 线性查找算法（BFPRT）
BFPRT算法解决的问题十分经典，即从某n个元素的序列中选出第k大（第k小）的元素，通过巧妙的分析，BFPRT可以保证在最坏情况下仍为线性时间复杂度。该算法的思想与快速排序思想相似，当然，为使得算法在最坏情况下，依然能达到o(n)的时间复杂度，五位算法作者做了精妙的处理。
**算法步骤：**

1. 将n个元素每5个一组，分成n/5(上界)组。
2. 取出每一组的中位数，任意排序方法，比如插入排序。
3. 递归的调用selection算法查找上一步中所有中位数的中位数，设为x，偶数个中位数的情况下设定为选取中间小的一个。
4. 用x来分割数组，设小于等于x的个数为k，大于x的个数即为n-k。
5. 若i==k，返回x；若i<k，在小于x的元素中递归查找第i小的元素；若i>k，在大于x的元素中递归查找第i-k小的元素。

终止条件：n=1时，返回的即是i小元素。


### 图遍历
* 深度优先搜索（Depth-First-Search，DFS）
深度优先搜索（缩写DFS）有点类似广度优先搜索，也是对一个连通图进行遍历的算法。它的思想是从一个顶点V0开始，沿着一条路一直走到底，如果发现不能到达目标解，那就返回到上一个节点，然后从另一条路开始走到底，这种尽量往深处走的概念即是深度优先的概念。
是搜索算法的一种。它沿着树的深度遍历树的节点，尽可能深的搜索树的分支。当节点v的所有边都己被探寻过，搜索将回溯到发现节点v的那条边的起始节点。这一过程一直进行到已发现从源节点可达的所有节点为止。如果还存在未被发现的节点，则选择其中一个作为源节点并重复以上过程，整个进程反复进行直到所有节点都被访问为止。DFS属于盲目搜索。
深度优先搜索是图论中的经典算法，利用深度优先搜索算法可以产生目标图的相应拓扑排序表，利用拓扑排序表可以方便的解决很多相关的图论问题，如最大路径问题等等。一般用堆数据结构来辅助实现DFS算法。
深度优先遍历图算法步骤：

1. 访问顶点v；
2. 依次从v的未被访问的邻接点出发，对图进行深度优先遍历；直至图中和v有路径相通的顶点都被访问；
3. 若此时图中尚有顶点未被访问，则从一个未被访问的顶点出发，重新进行深度优先遍历，直到图中所有顶点均被访问过为止。

上述描述可能比较抽象，举个实例：
DFS 在访问图中某一起始顶点 v 后，由 v 出发，访问它的任一邻接顶点 w1；再从 w1 出发，访问与 w1邻 接但还没有访问过的顶点 w2；然后再从 w2 出发，进行类似的访问，… 如此进行下去，直至到达所有的邻接顶点都被访问过的顶点 u 为止。
接着，退回一步，退到前一次刚访问过的顶点，看是否还有其它没有被访问的邻接顶点。如果有，则访问此顶点，之后再从此顶点出发，进行与前述类似的访问；如果没有，就再退回一步进行搜索。重复上述过程，直到连通图中所有顶点都被访问过为止。

* 广度优先搜索（Breadth-first search，BFS）
广度优先搜索一个图的时候是按照树的层次来搜索的，（层次遍历），队列来实现，形象的说，这里有点像辐射形状的搜索方式，从一个节点，向其旁边节点传递病毒，就这样一层一层的传递辐射下去，知道目标节点被辐射中了，此时就已经找到了从起点到终点的路径。
简单的说，BFS是从根节点开始，沿着树(图)的宽度遍历树(图)的节点。如果所有节点均被访问，则算法中止。BFS同样属于盲目搜索。一般用队列数据结构来辅助实现BFS算法。

**算法步骤：**
1. 首先将根节点放入队列中。
2. 从队列中取出第一个节点，并检验它是否为目标。
    * 如果找到目标，则结束搜寻并回传结果。
    * 否则将它所有尚未检验过的直接子节点加入队列中。
3. 若队列为空，表示整张图都检查过了——亦即图中没有欲搜寻的目标。结束搜寻并回传“找不到目标”。
4. 重复步骤2。
![BFS](https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/BFS.gif)

### Dijkstra算法
戴克斯特拉算法（Dijkstra’s algorithm）是由荷兰计算机科学家艾兹赫尔·戴克斯特拉提出。迪科斯彻算法使用了广度优先搜索解决非负权有向图的单源最短路径问题，算法最终得到一个最短路径树。该算法常用于路由算法或者作为其他图算法的一个子模块。
该算法的输入包含了一个有权重的有向图 G，以及G中的一个来源顶点 S。我们以 V 表示 G 中所有顶点的集合。每一个图中的边，都是两个顶点所形成的有序元素对。(u, v) 表示从顶点 u 到 v 有路径相连。我们以 E 表示G中所有边的集合，而边的权重则由权重函数 w: E → [0, ∞] 定义。因此，w(u, v) 就是从顶点 u 到顶点 v 的非负权重（weight）。边的权重可以想像成两个顶点之间的距离。任两点间路径的权重，就是该路径上所有边的权重总和。已知有 V 中有顶点 s 及 t，Dijkstra 算法可以找到 s 到 t的最低权重路径(例如，最短路径)。这个算法也可以在一个图中，找到从一个顶点 s 到任何其他顶点的最短路径。对于不含负权的有向图，Dijkstra算法是目前已知的最快的单源最短路径算法。
算法步骤：

初始时令 S={V0},T={其余顶点}，T中顶点对应的距离值

若存在<V0,Vi>，d(V0,Vi)为<V0,Vi>弧上的权值
若不存在<V0,Vi>，d(V0,Vi)为∞

从T中选取一个其距离值为最小的顶点W且不在S中，加入S
对其余T中顶点的距离值进行修改：若加进W作中间顶点，从V0到Vi的距离值缩短，则修改此距离值

重复上述步骤2、3，直到S中包含所有顶点，即W=Vi为止
![Dijkstra’s algorithm](https://raw.githubusercontent.com/axjing/axjingWorks/master/Reference/Dijkstra.gif)

### 动态规划算法
动态规划（Dynamic programming）是一种在数学、计算机科学和经济学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 动态规划常常适用于有重叠子问题和最优子结构性质的问题，动态规划方法所耗时间往往远少于朴素解法。
动态规划背后的基本思想非常简单。大致上，若要解一个给定问题，我们需要解其不同部分（即子问题），再合并子问题的解以得出原问题的解。 通常许多子问题非常相似，为此动态规划法试图仅仅解决每个子问题一次，从而减少计算量： 一旦某个给定子问题的解已经算出，则将其记忆化存储，以便下次需要同一个子问题解之时直接查表。 这种做法在重复子问题的数目关于输入的规模呈指数增长时特别有用。
关于动态规划最经典的问题当属背包问题。
**算法步骤：**

1 最优子结构性质。如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。最优子结构性质为动态规划算法解决问题提供了重要线索。
2 子问题重叠性质。子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。

### 朴素贝叶斯分类算法
朴素贝叶斯分类算法是一种基于贝叶斯定理的简单概率分类算法。贝叶斯分类的基础是概率推理，就是在各种条件的存在不确定，仅知其出现概率的情况下，如何完成推理和决策任务。概率推理是与确定性推理相对应的。而朴素贝叶斯分类器是基于独立假设的，即假设样本每个特征与其他特征都不相关。
朴素贝叶斯分类器依靠精确的自然概率模型，在有监督学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法，换言之朴素贝叶斯模型能工作并没有用到贝叶斯概率或者任何贝叶斯模型。
尽管是带着这些朴素思想和过于简单化的假设，但朴素贝叶斯分类器在很多复杂的现实情形中仍能够取得相当好的效果。

